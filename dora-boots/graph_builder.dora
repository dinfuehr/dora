use std::HashMap;
use std::BitSet;

use package::bytecode::opcode as opc;
use package::compilation::CompilationInfo;
use package::graph;
use package::graph::{AllocationData, Block, Graph, Inst, InstExtraData, Op};
use package::graph::ty::Type;
use package::graph::{ClassInfo, FunctionInfo, CallKind, VirtualFunctionInfo, LambdaFunctionInfo, TraitObjectInfo};
use package::bytecode::data::{BytecodeFunction, BytecodeRegister, BytecodeType, ClassId, EnumId, FctId, FieldId};
use package::bytecode::data::{ConstPoolId, ConstPoolEntry, GlobalId, StructId, StructFieldId, TraitId};
use package::bytecode::instruction::BytecodeInstruction;
use package::bytecode::reader::BytecodeIterator;
use package::interface as iface;
use package::interface::config;
use package::regalloc::{EnumLayout, isReference, RecordLayout};
use package::regalloc;

pub fn createGraph(ci: CompilationInfo): Graph {
    let graph = Graph::new();

    // Create basic blocks for the bytecode.
    let blockMap = createBlocks(graph, ci.bc);

    // Create edges for the blocks.
    createEdges(graph, ci.bc, blockMap);

    // Fill basic blocks with instructions.
    let ssagen = SsaGen::new(ci, graph, ci.bc, blockMap, ci.typeParams);
    ssagen.run();

    graph
}

class SsaGen {
    ci: CompilationInfo,
    graph: Graph,
    bc: BytecodeFunction,
    typeParams: Array[BytecodeType],
    blockMap: BlockMap,
    currentBlock: Option[Block],
    offset: Int32,
    currentDef: Array[HashMap[Block, Inst]],
    blockTerminated: Bool,

    returnValueArgInst: Option[Inst],

    pushed_registers: Vec[BytecodeRegister],

    // a block is considered filled when all instructions are inserted
    filledBlocks: BitSet,

    // block is considered sealed when the set of predecessors is final
    sealedBlocks: BitSet,

    // tracks all incomplete phi instructions inserted into unsealed blocks
    incompletePhis: HashMap[Block, HashMap[BytecodeRegister, Inst]],
}

impl SsaGen {
    static fn new(ci: CompilationInfo, graph: Graph, bc: BytecodeFunction, blockMap: BlockMap, typeParams: Array[BytecodeType]): SsaGen {
        SsaGen(
            ci,
            graph,
            bc,
            typeParams,
            blockMap,
            None[Block],
            0i32,
            Array[HashMap[Block, Inst]]::new(),
            false,
            None[Inst],
            Vec[BytecodeRegister]::new(),
            BitSet::new(0),
            BitSet::new(0),
            HashMap[Block, HashMap[BytecodeRegister, Inst]]::new(),
        )
    }

    fn run() {
        self.prepare();
        self.setupArguments();

        for inst in BytecodeIterator::new(self.bc.code) {
            self.instructionStart(inst.start.toInt32());
            self.processInstruction(inst.op);
        }

        assert(self.blockTerminated);
    }

    fn prepare() {
        self.currentBlock = None;
        let blockCount = self.graph.blockCount();

        self.filledBlocks = BitSet::new(blockCount.toInt64());
        self.sealedBlocks = BitSet::new(blockCount.toInt64());

        let data = Vec[HashMap[Block, Inst]]::new();

        for i in std::range(0, self.bc.registers.size()) {
            data.push(HashMap[Block, Inst]::new());
        }

        self.currentDef = data.toArray();
    }

    fn setupArguments() {
        let mut argIdx = 0i32;
        let entryBlock = self.graph.getEntryBlock();
        let retTy = self.ci.returnType.specialize(self.typeParams);

        if retTy.isStruct() || retTy.isTuple() {
            let argInst = graph::createArgInst(argIdx, Type::Address);
            argIdx = argIdx + 1i32;
            entryBlock.appendInst(argInst);
            self.returnValueArgInst = Some[Inst](argInst);
        }

        let mut regIdx = 0i32;
        while regIdx < self.bc.arguments {
            let ty = self.regId(regIdx);
            let ty = self.graphTy(ty);

            if !ty.isUnit() {
                let argInst = graph::createArgInst(argIdx, ty);
                entryBlock.appendInst(argInst);
                self.writeVariable(BytecodeRegister(regIdx), entryBlock, argInst);
                argIdx = argIdx + 1i32;
            }

            regIdx = regIdx + 1i32;
        }
    }

    fn current(): Block {
        self.currentBlock.getOrPanic()
    }

    fn writeVariable(register: BytecodeRegister, block: Block, value: Inst) {
        assert(!self.reg(register).isUnit());
        assert(value.hasId());
        self.currentDef(register.value.toInt64()).insert(block, value);
    }

    fn readVariable(register: BytecodeRegister, block: Block): Inst {
        assert(!self.reg(register).isUnit());

        if self.currentDef(register.value.toInt64()).contains(block) {
            let inst = self.currentDef(register.value.toInt64())(block).getOrPanic();

            if inst.hasId() {
                return inst;
            }
        }

        self.readVariableRecursive(register, block)
    }

    fn readVariableRecursive(register: BytecodeRegister, block: Block): Inst {
        let ty = self.regGraphTy(register);

        let value: Inst = if !self.sealedBlocks.contains(block.id().toInt64()) {
            // While all blocks are created with predecessors and successors before
            // this pass in the BlockBuilder already, we still need to handle unsealed blocks.
            // E.g. Register is accessed in while header and updated in the while body.
            // In this case the while header is filled before the while body. If we wouldn't
            // handle unsealed blocks we wouldn't create a Phi instruction, since the
            // while body predecessor is still empty.
            let incomplete = graph::createPhiInst(ty);
            block.appendPhi(incomplete);

            if self.incompletePhis.contains(block) {
                self.incompletePhis(block).getOrPanic().insert(register, incomplete);
            } else {
                let map = HashMap[BytecodeRegister, Inst]::new();
                map.insert(register, incomplete);
                self.incompletePhis.insert(block, map);
            }

            incomplete
        } else if block.predecessors.size() == 1i64 {
            self.readVariable(register, block.predecessors.first().getOrPanic().source)
        } else {
            let phi = graph::createPhiInst(ty);
            block.appendPhi(phi);
            self.writeVariable(register, block, phi);
            self.addPhiOperands(register, phi)
        };

        self.writeVariable(register, block, value);
        value
    }

    fn addPhiOperands(register: BytecodeRegister, phi: Inst): Inst {
        for pred in phi.getBlock().predecessors {
            let inst = self.readVariable(register, pred.source);
            phi.addInput(inst);
        }
        phi.registerUses();
        self.tryRemoveTrivialPhi(phi)
    }

    fn tryRemoveTrivialPhi(phi: Inst): Inst {
        let mut same = None[Inst];

        for inp in phi.getInputs() {
            let op = inp.getValue();

            if (same.isSome() && same.getOrPanic() === op) || op === phi {
                continue;
            }

            if same.isSome() {
                return phi;
            }

            same = Some(op);
        }

        if same.isNone() {
            same = Some(graph::createUndefInst());
        }

        let users = phi.users();

        phi.replaceWith(same.getOrPanic());
        phi.remove();

        for i in std::range(0, users.size()) {
            let user = users(i);

            if user === phi {
                continue;
            }

            if user.isPhi() {
                self.tryRemoveTrivialPhi(user);
            }
        }

        same.getOrPanic()
    }

    fn markBlockTerminated() {
        self.blockTerminated = true;
    }

    fn instructionStart(offset: Int32) {
        self.offset = offset;

        let block = self.blockMap.blockAt(offset.toInt64());

        if block.isSome() {
            if self.currentBlock.isSome() {
                self.blockEndReached(block);
            } else {
                self.currentBlock = block;
            }
        }

        self.blockTerminated = false;
    }

    fn processInstruction(inst: BytecodeInstruction) {
       match inst {
            BytecodeInstruction::Add(dest, lhs, rhs) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitBin(dest, lhs, rhs, Op::Add);
                } else {
                    self.emitBin(dest, lhs, rhs, Op::CheckedAdd);
                }
            }
            BytecodeInstruction::Sub(dest, lhs, rhs) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitBin(dest, lhs, rhs, Op::Sub);
                } else {
                    self.emitBin(dest, lhs, rhs, Op::CheckedSub);
                }
            }
            BytecodeInstruction::Neg(dest, src) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitUn(dest, src, Op::Neg);
                } else {
                    self.emitUn(dest, src, Op::CheckedNeg);
                }
            }
            BytecodeInstruction::Mul(dest, lhs, rhs) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitBin(dest, lhs, rhs, Op::Mul);
                } else {
                    self.emitBin(dest, lhs, rhs, Op::CheckedMul);
                }
            }
            BytecodeInstruction::Div(dest, lhs, rhs) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitDivMod(dest, lhs, rhs, Op::Div);
                } else {
                    self.emitDivMod(dest, lhs, rhs, Op::CheckedDiv);
                }
            }
            BytecodeInstruction::Mod(dest, lhs, rhs) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitDivMod(dest, lhs, rhs, Op::Mod);
                } else {
                    self.emitDivMod(dest, lhs, rhs, Op::CheckedMod);
                }
            }
            BytecodeInstruction::And(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::And);
            }
            BytecodeInstruction::Or(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Or);
            }
            BytecodeInstruction::Xor(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Xor);
            }
            BytecodeInstruction::Not(dest, src) => {
                self.emitUn(dest, src, Op::Not);
            }
            BytecodeInstruction::Shl(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Shl);
            }
            BytecodeInstruction::Shr(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Shr);
            }
            BytecodeInstruction::Sar(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Sar);
            }
            BytecodeInstruction::Mov(dest, src) => {
                self.emitMov(dest, src);
            }
            BytecodeInstruction::LoadTupleElement(dest, src, idx)  => {
                self.emitLoadTupleElement(dest, src, idx);
            }
            BytecodeInstruction::LoadEnumElement(dest, src, idx) => {                
                self.emitLoadEnumElement(dest, src, idx);
            }
            BytecodeInstruction::LoadEnumVariant(dest, src, idx) => {
                self.emitLoadEnumVariant(dest, src, idx);
            }
            BytecodeInstruction::LoadStructField(dest, src, idx) => {
                self.emitLoadStructField(dest, src, idx);
            }
            BytecodeInstruction::LoadField(dest, obj, idx) => {
                self.emitLoadField(dest, obj, idx);
            }
            BytecodeInstruction::StoreField(src, obj, idx) => {
                self.emitStoreField(src, obj, idx);
            }
            BytecodeInstruction::LoadGlobal(dest, global_id) => {
                self.emitLoadGlobal(dest, global_id);
            }
            BytecodeInstruction::StoreGlobal(src, global_id) => {
                self.emitStoreGlobal(src, global_id);
            }
            BytecodeInstruction::PushRegister(src) => {
                self.pushed_registers.push(src);
            }
            BytecodeInstruction::ConstTrue(dest) => {
                let inst = graph::createBoolConst(true);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstFalse(dest) => {
                let inst = graph::createBoolConst(false);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstUInt8(dest, value) => {
                let inst = graph::createUInt8Const(value);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstChar(dest, idx) => {
                let value = self.bc.constPool(idx).toChar().getOrPanic();
                let inst = graph::createInt32Const(value.toInt32());
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstInt32(dest, idx) => {
                let value = self.bc.constPool(idx).toInt32().getOrPanic();
                let inst = graph::createInt32Const(value);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstInt64(dest, idx) => {
                let value = self.bc.constPool(idx).toInt64().getOrPanic();
                let inst = graph::createInt64Const(value);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstFloat32(dest, idx) => {
                let value = self.bc.constPool(idx).toFloat32().getOrPanic();
                let inst = graph::createFloat32Const(value);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstFloat64(dest, idx) => {
                let value = self.bc.constPool(idx).toFloat64().getOrPanic();
                let inst = graph::createFloat64Const(value);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstString(dest, idx) => {
                let value = self.bc.constPool(idx).toString().getOrPanic();
                let inst = graph::createStringConst(value);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::TestIdentity(dest, lhs, rhs) => {
                self.emitTestIdentity(dest, lhs, rhs);
            }
            BytecodeInstruction::TestEq(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Equal);
            }
            BytecodeInstruction::TestNe(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::NotEqual);
            }
            BytecodeInstruction::TestGt(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Greater);
            }
            BytecodeInstruction::TestGe(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::GreaterOrEqual);
            }
            BytecodeInstruction::TestLt(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Less);
            }
            BytecodeInstruction::TestLe(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::LessOrEqual);
            }
            BytecodeInstruction::JumpLoop(distance) => {
                let targetBlock = self.blockMap.blockAt((self.offset - distance).toInt64()).getOrPanic();
                let gotoInst = graph::createGotoInst(targetBlock);
                self.current().appendInst(gotoInst);
                self.markBlockTerminated();
            }
            BytecodeInstruction::LoopStart => {
                // nothing to do
            }
            BytecodeInstruction::Jump(distance) => {
                self.emitJump(distance);
            }
            BytecodeInstruction::JumpIfFalse(opnd, distance) => {
                self.emitConditionalJump(opnd, distance, false);
            }
            BytecodeInstruction::JumpIfTrue(opnd, distance) => {
                self.emitConditionalJump(opnd, distance, true);
            }
            BytecodeInstruction::InvokeDirect(dest, idx) => {
                self.emitInvokeDirect(dest, idx);
            }
            BytecodeInstruction::InvokeVirtual(dest, idx) => {
                self.emitInvokeVirtual(dest, idx, CallKind::Virtual);
            }
            BytecodeInstruction::InvokeStatic(dest, idx) => {
                self.emitInvokeStatic(dest, idx);
            }
            BytecodeInstruction::InvokeLambda(dest, idx) => {
                self.emitInvokeVirtual(dest, idx, CallKind::Lambda);
            }
            BytecodeInstruction::InvokeGenericStatic(dest, idx) => {
                self.emitInvokeGeneric(dest, idx, CallKind::Static);
            }
            BytecodeInstruction::InvokeGenericDirect(dest, idx) => {
                self.emitInvokeGeneric(dest, idx, CallKind::Direct);
            }
            BytecodeInstruction::NewObject(dest, idx) => {
                self.emitNewObject(dest, idx);
            }
            BytecodeInstruction::NewObjectInitialized(dest, idx) => {
                self.emitNewObject(dest, idx);
            }
            BytecodeInstruction::NewArray(dest, idx, length) => {
                self.emitNewArray(dest, idx, length);
            }
            BytecodeInstruction::NewTuple(dest, idx) => {
                self.emitNewTuple(dest, idx);
            }
            BytecodeInstruction::NewEnum(dest, idx) => {
                self.emitNewEnum(dest, idx);
            }
            BytecodeInstruction::NewStruct(dest, idx) => {
                self.emitNewStruct(dest, idx);
            }
            BytecodeInstruction::NewTraitObject(dest, idx, obj) => {
                self.emitNewTraitObject(dest, idx, obj);
            }
            BytecodeInstruction::NewLambda(dest, idx) => {
                self.emitNewLambda(dest, idx);
            }
            BytecodeInstruction::ArrayLength(dest, src) => {
                let srcInst = self.readVariable(src, self.current());
                let destInst = graph::createArrayLengthInst(srcInst);
                destInst.set_bytecode_position(self.offset);
                self.current().appendInst(destInst);
                self.writeVariable(dest, self.current(), destInst);
            }
            BytecodeInstruction::LoadArray(dest, arr, idx) => {
                self.emitLoadArray(dest, arr, idx);
            }
            BytecodeInstruction::StoreArray(src, arr, idx) => {
                self.emitStoreArray(src, arr, idx);
            }
            BytecodeInstruction::LoadTraitObjectValue(dest, src) => {
                self.emitLoadTraitObjectValue(dest, src);
            }
            BytecodeInstruction::Ret(opnd) => {
                self.emitRet(opnd);
            }
        }
    }

    fn blockEndReached(next: Option[Block]) {
        let block = self.current();

        if !self.blockTerminated {
            let gotoInst = graph::createGotoInst(next.getOrPanic());
            block.appendInst(gotoInst);
        }

        // We change the current block, that means all instructions
        // are inserted. The block is now filled.
        self.fillBlock(block);

        // We don't really know when to seal a block from the bytecode
        // Try to seal this block if all predecessors are filled.
        self.trySealBlock(block);

        // This block might have a back edge to a loop header. Since this
        // block is now filled, we might be able to seal another block.
        for succ in block.successors {
            self.trySealBlock(succ.target);
        }

        self.currentBlock = next;
    }

    fn fillBlock(block: Block) {
        assert(!self.filledBlocks.contains(block.id().toInt64()));
        self.filledBlocks.insert(block.id().toInt64());
    }

    fn trySealBlock(block: Block) {
        if self.sealedBlocks.contains(block.id().toInt64()) {
            return;
        }

        // all predecessors need to be filled
        for edge in block.predecessors {
            if !self.filledBlocks.contains(edge.source.id().toInt64()) {
                return;
            }
        }

        self.sealBlock(block);
    }

    fn sealBlock(block: Block) {
        assert(!self.sealedBlocks.contains(block.id().toInt64()));
        self.sealedBlocks.insert(block.id().toInt64());

        let map = self.incompletePhis(block);
        if map.isNone() { return; }

        for variableAndPhi in map.getOrPanic() {
            self.addPhiOperands(variableAndPhi.0, variableAndPhi.1);
        }
    }

    fn emitLoadField(dest: BytecodeRegister, obj: BytecodeRegister, idx: ConstPoolId) {
        let destType = self.reg(dest);

        let (cls_id, type_params, field_id) = match self.bc.constPool(idx) {
            ConstPoolEntry::Field(cls_id, type_params, field_id) => (cls_id, type_params, field_id),
            _ => unreachable[(ClassId, Array[BytecodeType], FieldId)](),
        };

        if !destType.isUnit() {
            let objInst = self.readVariable(obj, self.current());
            let type_params = type_params.specialize(self.typeParams);
            let offset = iface::getFieldOffset(cls_id, type_params, field_id);
            let value = self.loadField(destType, objInst, offset);
            self.writeVariable(dest, self.current(), value);
        }
    }

    fn emitStoreField(src: BytecodeRegister, obj: BytecodeRegister, idx: ConstPoolId) {
        let srcType = self.reg(src);

        let (cls_id, type_params, field_id) = match self.bc.constPool(idx) {
            ConstPoolEntry::Field(cls_id, type_params, field_id) => (cls_id, type_params, field_id),
            _ => unreachable[(ClassId, Array[BytecodeType], FieldId)](),
        };
        let type_params = type_params.specialize(self.typeParams);

        if !srcType.isUnit() {
            let srcInst = self.readVariable(src, self.current());
            let objInst = self.readVariable(obj, self.current());

            let offset = iface::getFieldOffset(cls_id, type_params, field_id);
            self.storeField(srcType, objInst, offset, srcInst, true);
        }
    }

    fn emitLoadStructField(dest: BytecodeRegister, src: BytecodeRegister, idx: ConstPoolId) {
        let destTy = self.reg(dest);

        let (struct_id, type_params, field_id) = match self.bc.constPool(idx) {
            ConstPoolEntry::StructField(struct_id, type_params, field_id) => (struct_id, type_params, field_id),
            _ => unreachable[(StructId, Array[BytecodeType], StructFieldId)](),
        };

        if !destTy.isUnit() {
            let srcInst = self.readVariable(src, self.current());
            let structLayout = self.computeStructLayout(struct_id, type_params);
            let field = structLayout.fields(field_id.value.toInt64());

            let inst = self.loadField(destTy, srcInst, field.offset);
            self.writeVariable(dest, self.current(), inst);
        }
    }

    fn emitLoadTupleElement(dest: BytecodeRegister, src: BytecodeRegister, idx: ConstPoolId) {
        let srcInst = self.readVariable(src, self.current());
        let destTy = self.reg(dest);

        let (tuple_ty, subtype_idx) = match self.bc.constPool(idx) {
            ConstPoolEntry::TupleElement(tuple_ty, subtype_idx) => (tuple_ty, subtype_idx),
            _ => unreachable[(BytecodeType, Int32)](),
        };

        assert(tuple_ty.isTuple());

        let subtypes = match tuple_ty {
            BytecodeType::Tuple(subtypes) => subtypes,
            _ => unreachable[Array[BytecodeType]](),
        };

        let tupleLayout = self.computeTupleLayout(subtypes);
        let offset = tupleLayout.fields(subtype_idx.toInt64()).offset;

        let inst = self.loadField(destTy, srcInst, offset);
        self.writeVariable(dest, self.current(), inst);
    }

    fn emitLoadEnumVariant(dest: BytecodeRegister, src: BytecodeRegister, idx: ConstPoolId) {
        let (enum_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Enum(enum_id, type_params) => (enum_id, type_params),
            _ => unreachable[(EnumId, Array[BytecodeType])](),
        };

        let layout = self.computeEnumLayout(enum_id, type_params);

        match layout {
            EnumLayout::Int32 => {
                let inst = self.readVariable(src, self.current());
                self.writeVariable(dest, self.current(), inst);
            }

            EnumLayout::PtrOrNull(layout) => {
                let src = self.readVariable(src, self.current());

                let zero = graph::createNullConst();
                self.current().appendInst(zero);

                let op = if layout.null_is_first {
                    Op::NotEqual
                } else {
                    Op::Equal
                };

                let result = graph::createBinaryInst(op, Type::Ptr, src, zero);
                self.current().appendInst(result);

                self.writeVariable(dest, self.current(), result);
            }

            EnumLayout::Tagged => {
                let src = self.readVariable(src, self.current());

                let inst = graph::createLoadInst(src, iface::OBJECT_HEADER_LENGTH, Type::Int32);
                inst.set_bytecode_position(self.offset);
                self.current().appendInst(inst);

                self.writeVariable(dest, self.current(), inst);
            }
        }
    }

    fn emitLoadEnumElement(dest: BytecodeRegister, src: BytecodeRegister, idx: ConstPoolId) {
        let (enum_id, type_params, variant_id, field_id) = match self.bc.constPool(idx) {
            ConstPoolEntry::EnumElement(enum_id, type_params, variant_id, field_id) => (enum_id, type_params, variant_id, field_id),
            _ => unreachable[(EnumId, Array[BytecodeType], Int32, Int32)](),
        };
        let layout = self.computeEnumLayout(enum_id, type_params);
        let type_params = type_params.specialize(self.typeParams);

        match layout {
            EnumLayout::Int32 => {
                unreachable[()]();
            }

            EnumLayout::PtrOrNull(layout) => {
                let src = self.readVariable(src, self.current());

                let null_idx = if layout.null_is_first { 0i32 } else { 1i32 };
                assert(variant_id != null_idx);
                assert(field_id == 0i32);

                self.writeVariable(dest, self.current(), src);
            }

            EnumLayout::Tagged => {
                let destTy = self.reg(dest);
                let src = self.readVariable(src, self.current());

                let offset = iface::getFieldOffsetForEnumVariant(enum_id, type_params, variant_id, field_id);
                let result = self.loadField(destTy, src, offset);

                self.writeVariable(dest, self.current(), result);
            }
        }
    }

    fn emitTestIdentity(dest: BytecodeRegister, lhs: BytecodeRegister, rhs: BytecodeRegister) {
        let registerType = self.reg(lhs);

        match registerType {
            BytecodeType::Ptr => {},
            _ => unreachable[()](),
        }

        let mut lhsInst = self.readVariable(lhs, self.current());
        let mut rhsInst = self.readVariable(rhs, self.current());

        let destInst = graph::createBinaryInst(Op::Equal, Type::Ptr, lhsInst, rhsInst);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitBin(dest: BytecodeRegister, lhs: BytecodeRegister, rhs: BytecodeRegister, op: Op) {
        let registerType = self.reg(lhs);

        let ty = match registerType {
            BytecodeType::UInt8 => Type::UInt8,
            BytecodeType::Int32 | BytecodeType::Char => Type::Int32,
            BytecodeType::Int64 => Type::Int64,
            BytecodeType::Float32 => Type::Float32,
            BytecodeType::Float64 => Type::Float64,
            BytecodeType::Enum(enum_id, type_params) => {
                assert(op == Op::Equal || op == Op::NotEqual);
                let layout = self.computeEnumLayout(enum_id, type_params);
                assert(layout.isInt32());
                Type::Int32
            }
            _ => unreachable[Type](),
        };

        let mut lhsInst = self.readVariable(lhs, self.current());
        let mut rhsInst = self.readVariable(rhs, self.current());

        if op.isCommutative() && lhsInst.isConst() {
            let tmp = lhsInst;
            lhsInst = rhsInst;
            rhsInst = tmp;
        }

        let destInst = graph::createBinaryInst(op, ty, lhsInst, rhsInst);
        destInst.set_bytecode_position(self.offset);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitUn(dest: BytecodeRegister, src: BytecodeRegister, op: Op) {
        let registerType = self.reg(dest);

        let ty = match registerType {
            BytecodeType::Bool => Type::Bool,
            BytecodeType::Int32 => Type::Int32,
            BytecodeType::Int64 => Type::Int64,
            BytecodeType::Float32 => Type::Float32,
            BytecodeType::Float64 => Type::Float64,
            _ => unreachable[Type](),
        };

        let srcInst = self.readVariable(src, self.current());
        let destInst = graph::createUnaryInst(op, ty, srcInst);
        destInst.set_bytecode_position(self.offset);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitDivMod(dest: BytecodeRegister, lhs: BytecodeRegister, rhs: BytecodeRegister, op: Op) {
        let registerType = self.reg(dest);

        let ty = match registerType {
            BytecodeType::Int32 => Type::Int32,
            BytecodeType::Int64 => Type::Int64,
            BytecodeType::Float32 => Type::Float32,
            BytecodeType::Float64 => Type::Float64,
            _ => unreachable[Type](),
        };
        
        let lhsInst = self.readVariable(lhs, self.current());
        let rhsInst = self.readVariable(rhs, self.current());

        if !registerType.isAnyFloat() {
            let divZeroCheck = graph::createDivZeroCheckInst(rhsInst);
            divZeroCheck.set_bytecode_position(self.offset);
            self.current().appendInst(divZeroCheck);
        }

        let destInst = graph::createBinaryInst(op, ty, lhsInst, rhsInst);
        destInst.set_bytecode_position(self.offset);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitMov(dest: BytecodeRegister, src: BytecodeRegister) {
        let ty = self.reg(src);

        if !ty.isUnit() {
            let srcInst = self.readVariable(src, self.current());
            self.writeVariable(dest, self.current(), srcInst);
        }
    }

    fn emitLoadGlobal(dest: BytecodeRegister, glob: GlobalId) {
        let destType = self.reg(dest);

        if iface::hasGlobalInitialValue(glob) {
            let ty = self.graphTy(destType);
            let inst = graph::createEnsureGlobalInitializedInst(glob, ty);
            self.current().appendInst(inst);
            inst.set_bytecode_position(self.offset);
        }

        let value = self.loadGlobal(destType, glob);
        self.writeVariable(dest, self.current(), value);
    }

    fn emitStoreGlobal(src: BytecodeRegister, glob: GlobalId) {
        let srcType = self.reg(src);

        let srcInst = self.readVariable(src, self.current());
        self.storeGlobal(srcType, glob, srcInst);

        if iface::hasGlobalInitialValue(glob) {
            let inst = graph::createMarkGlobalInitializedInst(glob);
            self.current().appendInst(inst);
        }
    }

    fn emitTest(dest: BytecodeRegister, lhs: BytecodeRegister, rhs: BytecodeRegister, op: Op) {
        let registerType = self.reg(lhs);

        let ty = match registerType {
            BytecodeType::Int32 => Type::Int32,
            BytecodeType::Int64 => Type::Int64,
            BytecodeType::Float32 => Type::Float32,
            BytecodeType::Float64 => Type::Float64,
            _ => unreachable[Type](),
        };

        let lhsInst = self.readVariable(lhs, self.current());
        let rhsInst = self.readVariable(rhs, self.current());
        let destInst = graph::createTestInst(op, ty, lhsInst, rhsInst);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitJump(offset: Int32) {
        let targetBlock = self.blockMap.blockAt((self.offset + offset).toInt64()).getOrPanic();
        let gotoInst = graph::createGotoInst(targetBlock);
        self.current().appendInst(gotoInst);
        self.markBlockTerminated();
    }

    fn emitConditionalJump(opnd: BytecodeRegister, distance: Int32, value: Bool) {
        let opndInst = self.readVariable(opnd, self.current());
        let targetBlock = self.blockMap.blockAt((self.offset + distance).toInt64()).getOrPanic();
        let fallthroughBlock = self.blockMap.nextBlockAt((self.offset+1i32).toInt64()).getOrPanic();

        let cond = if value {
            graph::createIfInst(opndInst, targetBlock, fallthroughBlock)
        } else {
            graph::createIfInst(opndInst, fallthroughBlock, targetBlock)
        };

        self.current().appendInst(cond);
        self.markBlockTerminated();
    }

    fn emitLoadArray(dest: BytecodeRegister, arr: BytecodeRegister, idx: BytecodeRegister) {
        let arrInst = self.readVariable(arr, self.current());
        let idxInst = self.readVariable(idx, self.current());

        let arrayLengthInst = graph::createArrayLengthInst(arrInst);
        self.current().appendInst(arrayLengthInst);

        let boundsCheckInst = graph::createBoundsCheckInst(idxInst, arrayLengthInst);
        boundsCheckInst.set_bytecode_position(self.offset);
        self.current().appendInst(boundsCheckInst);

        let destTy = self.reg(dest);

        if !destTy.isUnit() {
            let result = self.loadArray(destTy, arrInst, idxInst);
            self.writeVariable(dest, self.current(), result);
        }
    }

    fn emitStoreArray(src: BytecodeRegister, arr: BytecodeRegister, idx: BytecodeRegister) {
        let arrInst = self.readVariable(arr, self.current());
        let idxInst = self.readVariable(idx, self.current());

        let arrayLengthInst = graph::createArrayLengthInst(arrInst);
        self.current().appendInst(arrayLengthInst);

        let boundsCheckInst = graph::createBoundsCheckInst(idxInst, arrayLengthInst);
        boundsCheckInst.set_bytecode_position(self.offset);
        self.current().appendInst(boundsCheckInst);

        let srcType = self.reg(src);
        if !srcType.isUnit() {
            let srcInst = self.readVariable(src, self.current());
            self.storeArray(srcType, arrInst, idxInst, srcInst);
        }
    }

    fn emitLoadTraitObjectValue(dest: BytecodeRegister, src: BytecodeRegister) {
        let srcInst = self.readVariable(src, self.current());
        let destType = self.regGraphTy(dest);

        let offset = iface::OBJECT_HEADER_LENGTH;
        let value = self.loadField(BytecodeType::Ptr, srcInst, offset);

        self.writeVariable(dest, self.current(), value);
    }

    fn emitRet(opnd: BytecodeRegister) {
        let ty = self.reg(opnd);

        if ty.isUnit() {
            let inst = graph::createReturnVoidInst();
            self.current().appendInst(inst);
        } else if ty.isStruct() {
            let src = self.readVariable(opnd, self.current());
            let dest = self.returnValueArgInst.getOrPanic();
            self.copyStruct(ty, dest, 0i32, src, 0i32, false);
            let inst = graph::createReturnVoidInst();
            self.current().appendInst(inst);
        } else if ty.isTuple() {
            let src = self.readVariable(opnd, self.current());
            let dest = self.returnValueArgInst.getOrPanic();
            self.copyTuple(ty, dest, 0i32, src, 0i32, false);
            let inst = graph::createReturnVoidInst();
            self.current().appendInst(inst);
        } else {
            let value = self.readVariable(opnd, self.current());
            let ty = self.graphTy(ty);
            let inst = graph::createReturnInst(value, ty);
            self.current().appendInst(inst);
        }

        self.markBlockTerminated();
    }

    fn emitInvokeDirect(dest: BytecodeRegister, idx: ConstPoolId) {
        let (fct_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Fct(fct_id, type_params) => (fct_id, type_params),
            _ => unreachable[(FctId, Array[BytecodeType])](),
        };

        let intrinsic = iface::getIntrinsicForFunction(fct_id);

        if self.isIntrinsic(intrinsic) {
            let args = self.pushed_registers.toArray();
            self.pushed_registers.clear();
            self.emitIntrinsic(intrinsic, dest, args, idx);
        } else {
            self.emitCall(dest, CallKind::Direct, fct_id, type_params);
        }
    }

    fn emitInvokeStatic(dest: BytecodeRegister, idx: ConstPoolId) {
        let (fct_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Fct(fct_id, type_params) => (fct_id, type_params),
            _ => unreachable[(FctId, Array[BytecodeType])](),
        };

        let intrinsic = iface::getIntrinsicForFunction(fct_id);

        if self.isIntrinsic(intrinsic) {
            let args = self.pushed_registers.toArray();
            self.pushed_registers.clear();
            self.emitIntrinsic(intrinsic, dest, args, idx);
        } else {
            self.emitCall(dest, CallKind::Static, fct_id, type_params);
        }
    }

    fn emitCall(dest: BytecodeRegister, kind: CallKind, fct_id: FctId, type_params: Array[BytecodeType]) {
        let args = Vec[Inst]::new();
        args.reserve(self.pushed_registers.size());
        let destTy = self.reg(dest);

        let returnTy = self.prepareReturnValue(dest, destTy, args);

        for reg in self.pushed_registers {
            let ty = self.regGraphTy(reg);

            if !ty.isUnit() {
                let arg = self.readVariable(reg, self.current());
                args.push(arg);
            }
        }

        self.pushed_registers.clear();

        let info = FunctionInfo(fct_id, type_params);

        let inst = graph::createCallInst(info, kind, args, returnTy);
        inst.set_bytecode_position(self.offset);
        self.current().appendInst(inst);

        if !returnTy.isUnit() {
            self.writeVariable(dest, self.current(), inst);
        }
    }

    fn emitInvokeVirtual(dest: BytecodeRegister, idx: ConstPoolId, kind: CallKind) {
        let args = Vec[Inst]::new();
        args.reserve(self.pushed_registers.size());
        let destTy = self.reg(dest);

        let returnTy = self.prepareReturnValue(dest, destTy, args);
        let receiver_is_first = args.size() == 0;

        for reg in self.pushed_registers {
            let arg = self.readVariable(reg, self.current());
            args.push(arg);
        }

        self.pushed_registers.clear();

        let inst = match kind {
            CallKind::Virtual => {
                let (trait_object_ty, fct_id, type_params) = match self.bc.constPool(idx) {
                    ConstPoolEntry::TraitObjectMethod(trait_object_ty, fct_id, type_params) => (trait_object_ty, fct_id, type_params),
                    _ => unreachable[(BytecodeType, FctId, Array[BytecodeType])](),
                };

                let info = VirtualFunctionInfo(trait_object_ty, fct_id, type_params, receiver_is_first);
                graph::createVirtualCallInst(info, args, returnTy)
            }

            CallKind::Lambda => {
                let (params, return_type) = match self.bc.constPool(idx) {
                    ConstPoolEntry::Lambda(params, return_type) => (params, return_type),
                    _ => unreachable[(Array[BytecodeType], BytecodeType)](),
                };

                let info = LambdaFunctionInfo(params, return_type, receiver_is_first);
                graph::createLambdaCallInst(info, args, returnTy)
            }

            _ => unreachable[Inst](),
        };

        inst.set_bytecode_position(self.offset);
        self.current().appendInst(inst);

        if !returnTy.isUnit() {
            self.writeVariable(dest, self.current(), inst);
        }
    }

    fn prepareReturnValue(dest: BytecodeRegister, destTy: BytecodeType, args: Vec[Inst]): Type {
        if destTy.isStruct() || destTy.isTuple() {
            let layout = match destTy {
                BytecodeType::Struct(struct_id, type_params) => {
                    self.computeStructLayout(struct_id, type_params)
                },

                BytecodeType::Tuple(subtypes) => {
                    self.computeTupleLayout(subtypes)
                },

                _ => unreachable[RecordLayout](),
            };

            let allocInst = graph::createAllocateStackInst(layout);
            self.current().appendInst(allocInst);
            args.push(allocInst);

            if layout.refs.size() > 0 {
                let zero = graph::createNullConst();
                self.current().appendInst(zero);

                for ref in layout.refs {
                    let inst = graph::createStoreInst(allocInst, ref, zero, Type::Ptr);
                    self.current().appendInst(inst);
                }
            }

            self.writeVariable(dest, self.current(), allocInst);

            Type::Unit
        } else {
            self.graphTy(destTy)
        }
    }

    fn emitInvokeGeneric(dest: BytecodeRegister, idx: ConstPoolId, kind: CallKind) {
        let ty = self.regGraphTy(dest);

        let (type_param_idx, trait_fct_id, trait_type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Generic(type_param_idx, trait_fct_id, trait_type_params) => (type_param_idx, trait_fct_id, trait_type_params),
            _ => unreachable[(Int32, FctId, Array[BytecodeType])](),
        };

        let object_ty = self.typeParams(type_param_idx.toInt64());
        let callee_id = iface::findTraitImpl(trait_fct_id, trait_type_params, object_ty);

        let intrinsic = iface::getIntrinsicForFunction(callee_id);

        if self.isIntrinsic(intrinsic) {
            let args = self.pushed_registers.toArray();
            self.pushed_registers.clear();
            self.emitIntrinsic(intrinsic, dest, args, idx);
        } else {
            self.emitCall(dest, kind, callee_id, Array[BytecodeType]::new());
        }
    }

    fn isIntrinsic(intrinsic: Int32): Bool {
        intrinsic >= 0i32 &&
        intrinsic != opc::INTRINSIC_OPTION_GET_OR_PANIC
    }

    fn emitIntrinsic(intrinsic: Int32, dest: BytecodeRegister, args: Array[BytecodeRegister], idx: ConstPoolId) {
        if intrinsic == opc::INTRINSIC_INT64_ADD_UNCHECKED || intrinsic == opc::INTRINSIC_INT32_ADD_UNCHECKED {
            self.emitBin(dest, args(0), args(1), Op::Add);
        } else if intrinsic == opc::INTRINSIC_THREAD_CURRENT {
            self.emitIntrinsicThreadCurrent(dest, args);
        } else if intrinsic == opc::INTRINSIC_UNREACHABLE {
            self.emitIntrinsicUnreachable(dest, args, idx);
        } else if intrinsic == opc::INTRINSIC_ASSERT {
            self.emitIntrinsicAssert(dest, args);
        } else if intrinsic == opc::INTRINSIC_OPTION_IS_SOME || intrinsic == opc::INTRINSIC_OPTION_IS_NONE {
            self.emitIntrinsicOptionIsSomeAndIsNone(intrinsic, dest, args);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_TO_INT32 {
            self.emitIntrinsicConvert(dest, args, Type::Int32, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_TO_INT64 {
            self.emitIntrinsicConvert(dest, args, Type::Int64, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_TO_INT32 {
            self.emitIntrinsicConvert(dest, args, Type::Int32, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_TO_INT64 {
            self.emitIntrinsicConvert(dest, args, Type::Int64, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_INT32_TO_FLOAT32 {
            self.emitIntrinsicConvert(dest, args, Type::Float32, Type::Int32);
        } else if intrinsic == opc::INTRINSIC_INT32_TO_FLOAT64 {
            self.emitIntrinsicConvert(dest, args, Type::Float64, Type::Int32);
        } else if intrinsic == opc::INTRINSIC_INT32_TO_INT64 {
            self.emitIntrinsicConvert(dest, args, Type::Int64, Type::Int32);
        } else if intrinsic == opc::INTRINSIC_INT64_TO_FLOAT32 {
            self.emitIntrinsicConvert(dest, args, Type::Float32, Type::Int64);
        } else if intrinsic == opc::INTRINSIC_INT64_TO_FLOAT64 {
            self.emitIntrinsicConvert(dest, args, Type::Float64, Type::Int64);
        } else if intrinsic == opc::INTRINSIC_INT64_TO_INT32 {
            self.emitIntrinsicConvert(dest, args, Type::Int32, Type::Int64);
        } else if intrinsic == opc::INTRINSIC_PROMOTE_FLOAT32_TO_FLOAT64 {
            self.emitIntrinsicConvert(dest, args, Type::Float64, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_DEMOTE_FLOAT64_TO_FLOAT32 {
            self.emitIntrinsicConvert(dest, args, Type::Float32, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_ADD || intrinsic == opc::INTRINSIC_FLOAT32_ADD {
            self.emitBin(dest, args(0), args(1), Op::Add);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_SUB || intrinsic == opc::INTRINSIC_FLOAT32_SUB {
            self.emitBin(dest, args(0), args(1), Op::Sub);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_MUL || intrinsic == opc::INTRINSIC_FLOAT32_MUL {
            self.emitIntrinsicBin(dest, args, Op::Mul);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_DIV || intrinsic == opc::INTRINSIC_FLOAT32_DIV {
            self.emitIntrinsicBin(dest, args, Op::Div);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_NEG || intrinsic == opc::INTRINSIC_FLOAT32_NEG {
            self.emitIntrinsicUn(dest, args, Op::Neg);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_EQ || intrinsic == opc::INTRINSIC_FLOAT32_EQ {
            self.emitIntrinsicBin(dest, args, Op::Equal);
        } else if intrinsic == opc::INTRINSIC_INT64_ADD || intrinsic == opc::INTRINSIC_INT32_ADD {
            self.emitBin(dest, args(0), args(1), Op::CheckedAdd);
        } else if intrinsic == opc::INTRINSIC_INT64_SUB || intrinsic == opc::INTRINSIC_INT32_SUB {
            self.emitBin(dest, args(0), args(1), Op::CheckedSub);
        } else if intrinsic == opc::INTRINSIC_INT64_MUL || intrinsic == opc::INTRINSIC_INT32_MUL {
            self.emitIntrinsicBin(dest, args, Op::CheckedMul);
        } else if intrinsic == opc::INTRINSIC_INT64_DIV || intrinsic == opc::INTRINSIC_INT32_DIV {
            self.emitIntrinsicBin(dest, args, Op::CheckedDiv);
        } else if intrinsic == opc::INTRINSIC_INT64_MOD || intrinsic == opc::INTRINSIC_INT32_MOD {
            self.emitIntrinsicBin(dest, args, Op::CheckedMod);
        } else if intrinsic == opc::INTRINSIC_INT64_AND || intrinsic == opc::INTRINSIC_INT32_AND {
            self.emitIntrinsicBin(dest, args, Op::And);
        } else if intrinsic == opc::INTRINSIC_INT64_OR || intrinsic == opc::INTRINSIC_INT32_OR {
            self.emitIntrinsicBin(dest, args, Op::Or);
        } else if intrinsic == opc::INTRINSIC_INT64_XOR || intrinsic == opc::INTRINSIC_INT32_XOR {
            self.emitIntrinsicBin(dest, args, Op::Xor);
        } else if intrinsic == opc::INTRINSIC_INT64_SAR || intrinsic == opc::INTRINSIC_INT32_SAR {
            self.emitIntrinsicBin(dest, args, Op::Sar);
        } else if intrinsic == opc::INTRINSIC_INT64_SHL || intrinsic == opc::INTRINSIC_INT32_SHL {
            self.emitIntrinsicBin(dest, args, Op::Shl);
        } else if intrinsic == opc::INTRINSIC_INT64_SHR || intrinsic == opc::INTRINSIC_INT32_SHR {
            self.emitIntrinsicBin(dest, args, Op::Shr);
        } else if intrinsic == opc::INTRINSIC_INT64_NEG || intrinsic == opc::INTRINSIC_INT32_NEG {
            self.emitIntrinsicUn(dest, args, Op::CheckedNeg);
        } else if intrinsic == opc::INTRINSIC_INT64_EQ || intrinsic == opc::INTRINSIC_INT32_EQ {
            self.emitIntrinsicBin(dest, args, Op::Equal);
        } else if intrinsic == opc::INTRINSIC_BOOL_NOT || intrinsic == opc::INTRINSIC_INT64_NOT || intrinsic == opc::INTRINSIC_INT32_NOT {
            self.emitIntrinsicUn(dest, args, Op::Not);
        } else if intrinsic == opc::INTRINSIC_INT64_CMP {
            self.emitIntrinsicCompareOrdering(dest, args, Type::Int64);
        } else if intrinsic == opc::INTRINSIC_INT32_CMP || intrinsic == opc::INTRINSIC_CHAR_CMP {
            self.emitIntrinsicCompareOrdering(dest, args, Type::Int32);
        } else if intrinsic == opc::INTRINSIC_U_INT8_CMP {
            self.emitIntrinsicCompareOrdering(dest, args, Type::UInt8);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_CMP {
            self.emitIntrinsicCompareOrdering(dest, args, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_CMP {
            self.emitIntrinsicCompareOrdering(dest, args, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_REINTERPRET_FLOAT64_AS_INT64 {
            self.emitIntrinsicBitcast(dest, args, Type::Int64, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_REINTERPRET_FLOAT32_AS_INT32 {
            self.emitIntrinsicBitcast(dest, args, Type::Int32, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_ROUND_DOWN {
            self.emitIntrinsicRounding(dest, args, Op::RoundDown, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_ROUND_UP {
            self.emitIntrinsicRounding(dest, args, Op::RoundUp, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_ROUND_TO_ZERO {
            self.emitIntrinsicRounding(dest, args, Op::RoundToZero, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_ROUND_HALF_EVEN {
            self.emitIntrinsicRounding(dest, args, Op::RoundHalfEven, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_ROUND_DOWN {
            self.emitIntrinsicRounding(dest, args, Op::RoundDown, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_ROUND_UP {
            self.emitIntrinsicRounding(dest, args, Op::RoundUp, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_ROUND_TO_ZERO {
            self.emitIntrinsicRounding(dest, args, Op::RoundToZero, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_ROUND_HALF_EVEN {
            self.emitIntrinsicRounding(dest, args, Op::RoundHalfEven, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_SQRT {
            self.emitIntrinsicRounding(dest, args, Op::Sqrt, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_SQRT {
            self.emitIntrinsicRounding(dest, args, Op::Sqrt, Type::Float32);
        } else {
            let name = opc::intrinsicName(intrinsic);
            std::fatalError("unknown intrinsic ${name}");
        }
    }

    fn emitIntrinsicRounding(dest: BytecodeRegister, args: Array[BytecodeRegister], op: Op, ty: Type) {
        assert(args.size() == 1);
        let src = args(0);
        assert(ty == self.regGraphTy(dest));
        assert(ty == self.regGraphTy(src));
        let value = self.readVariable(src, self.current());
        let result = graph::createUnaryInst(op, ty, value);
        self.current().appendInst(result);

        self.writeVariable(dest, self.current(), result);
    }

    fn emitIntrinsicCompareOrdering(dest: BytecodeRegister, args: Array[BytecodeRegister], ty: Type) {
        assert(args.size() == 2);
        assert(ty == self.regGraphTy(args(0)));
        assert(ty == self.regGraphTy(args(1)));
        let lhs = self.readVariable(args(0), self.current());
        let rhs = self.readVariable(args(1), self.current());
        let result = graph::createCompareOrderingInst(ty, lhs, rhs);
        self.current().appendInst(result);
        self.writeVariable(dest, self.current(), result);
    }

    fn emitIntrinsicUn(dest: BytecodeRegister, args: Array[BytecodeRegister], op: Op) {
        assert(args.size() == 1);
        self.emitUn(dest, args(0), op);
    }

    fn emitIntrinsicBin(dest: BytecodeRegister, args: Array[BytecodeRegister], op: Op) {
        assert(args.size() == 2);
        self.emitBin(dest, args(0), args(1), op);
    }

    fn emitIntrinsicConvert(dest: BytecodeRegister, args: Array[BytecodeRegister], dest_ty: Type, src_ty: Type) {
        assert(args.size() == 1);
        let src = args(0);
        assert(dest_ty == self.regGraphTy(dest));
        assert(src_ty == self.regGraphTy(src));
        let value = self.readVariable(src, self.current());
        let result = graph::createConvertInst(dest_ty, value);
        self.current().appendInst(result);

        self.writeVariable(dest, self.current(), result);
    }

    fn emitIntrinsicBitcast(dest: BytecodeRegister, args: Array[BytecodeRegister], dest_ty: Type, src_ty: Type) {
        assert(args.size() == 1);
        let src = args(0);
        assert(dest_ty == self.regGraphTy(dest));
        assert(src_ty == self.regGraphTy(src));
        let value = self.readVariable(src, self.current());
        let result = graph::createBitcastInst(dest_ty, value);
        self.current().appendInst(result);

        self.writeVariable(dest, self.current(), result);
    }

    fn emitIntrinsicOptionIsSomeAndIsNone(intrinsic: Int32, dest: BytecodeRegister, args: Array[BytecodeRegister]) {
        assert(args.size() == 1);
        let ty = self.reg(args(0));
        let (enum_id, type_params) = match ty {
            BytecodeType::Enum(enum_id, type_params) => (enum_id, type_params),
            _ => unreachable[(EnumId, Array[BytecodeType])](),
        };
        let layout = self.computeEnumLayout(enum_id, type_params);
        let is_some = if intrinsic == opc::INTRINSIC_OPTION_IS_SOME {
            true
        } else {
            assert(intrinsic == opc::INTRINSIC_OPTION_IS_NONE);
            false
        };

        let enumData = iface::getEnumData(enum_id);
        let some_variant_id = if enumData.variants(0).fields.isEmpty() {
            1i32
        } else {
            0i32
        };

        let obj = self.readVariable(args(0), self.current());

        match layout {
            EnumLayout::Int32 => unreachable[()](),
            EnumLayout::PtrOrNull(_) => {
                let null = graph::createNullConst();
                self.current().appendInst(null);

                let op = if is_some {
                    Op::NotEqual
                } else {
                    Op::Equal
                };

                let result = graph::createBinaryInst(op, Type::Ptr, obj, null);
                self.current().appendInst(result);

                self.writeVariable(dest, self.current(), result);
            }
            EnumLayout::Tagged => {
                let variant_field = graph::createLoadInst(obj, iface::OBJECT_HEADER_LENGTH, Type::Int32);
                variant_field.set_bytecode_position(self.offset);
                self.current().appendInst(variant_field);

                let op = if is_some {
                    Op::Equal
                } else {
                    Op::NotEqual
                };

                let some_const = graph::createInt32Const(some_variant_id);
                self.current().appendInst(some_const);

                let result = graph::createBinaryInst(op, Type::Int32, variant_field, some_const);
                self.current().appendInst(result);

                self.writeVariable(dest, self.current(), result);
            }
        }
    }

    fn emitIntrinsicUnreachable(dest: BytecodeRegister, args: Array[BytecodeRegister], idx: ConstPoolId) {
        assert(args.isEmpty());
        let ty = self.regGraphTy(dest);

        let inst = graph::createUnreachableInst(ty);
        self.current().appendInst(inst);
        inst.set_bytecode_position(self.offset);

        if !ty.isUnit() {
            self.writeVariable(dest, self.current(), inst);
        }
    }

    fn emitIntrinsicAssert(dest: BytecodeRegister, args: Array[BytecodeRegister]) {
        assert(args.size() == 1);
        let condition = self.readVariable(args(0), self.current());

        let inst = graph::createAssertInst(condition);
        self.current().appendInst(inst);
        inst.set_bytecode_position(self.offset);
    }

    fn emitIntrinsicThreadCurrent(dest: BytecodeRegister, args: Array[BytecodeRegister]) {
        assert(args.isEmpty());
        let inst = graph::createThreadCurrentInst();
        self.current().appendInst(inst);
        self.writeVariable(dest, self.current(), inst);
    }

    fn emitNewObject(dest: BytecodeRegister, idx: ConstPoolId) {
        let (class_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Class(class_id, type_params) => (class_id, type_params),
            _ => unreachable[(ClassId, Array[BytecodeType])](),
        };

        let type_params = type_params.specialize(self.typeParams);
        let info = ClassInfo(class_id, type_params);
        let info = InstExtraData::ClassInfo(info);
        let objInst = graph::createNewObjectInst(info);
        objInst.set_bytecode_position(self.offset);
        self.current().appendInst(objInst);

        self.writeVariable(dest, self.current(), objInst);
        let mut field_id = 0i32;

        for reg in self.pushed_registers {
            let argTy = self.reg(reg);

            if !argTy.isUnit() {
                let arg = self.readVariable(reg, self.current());

                let offset = iface::getFieldOffset(class_id, type_params, FieldId(field_id));
                self.storeField(argTy, objInst, offset, arg, true);
            }

            field_id = field_id + 1i32;
        }

        self.pushed_registers.clear();
    }

    fn emitNewLambda(dest: BytecodeRegister, idx: ConstPoolId) {
        let (fct_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Fct(fct_id, type_params) => (fct_id, type_params),
            _ => unreachable[(FctId, Array[BytecodeType])](),
        };

        let type_params = type_params.specialize(self.typeParams);
        let info = FunctionInfo(fct_id, type_params);
        let info = InstExtraData::FunctionInfo(info);
        let objInst = graph::createNewObjectInst(info);
        objInst.set_bytecode_position(self.offset);
        self.current().appendInst(objInst);

        self.writeVariable(dest, self.current(), objInst);

        if !self.pushed_registers.isEmpty() {
            assert(self.pushed_registers.size() == 1);
            let reg = self.pushed_registers.first().getOrPanic();
            let arg = self.readVariable(reg, self.current());
            let argTy = self.reg(reg);

            let offset = iface::OBJECT_HEADER_LENGTH;
            self.storeField(argTy, objInst, offset, arg, true);
        }

        self.pushed_registers.clear();
    }

    fn emitNewTraitObject(dest: BytecodeRegister, idx: ConstPoolId, obj: BytecodeRegister) {
        let (trait_id, type_params, object_ty) = match self.bc.constPool(idx) {
            ConstPoolEntry::Trait(trait_id, type_params, object_ty) => (trait_id, type_params, object_ty),
            _ => unreachable[(TraitId, Array[BytecodeType], BytecodeType)](),
        };

        let info = TraitObjectInfo(trait_id, type_params, object_ty);
        let info = InstExtraData::TraitObjectInfo(info);
        let objInst = graph::createNewObjectInst(info);
        objInst.set_bytecode_position(self.offset);
        self.current().appendInst(objInst);

        self.writeVariable(dest, self.current(), objInst);

        let arg = self.readVariable(obj, self.current());
        let argTy = self.reg(obj);

        let offset = iface::OBJECT_HEADER_LENGTH;
        self.storeField(argTy, objInst, offset, arg, true);
    }

    fn emitNewStruct(dest: BytecodeRegister, idx: ConstPoolId) {
        let (struct_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Struct(struct_id, type_params) => (struct_id, type_params),
            _ => unreachable[(StructId, Array[BytecodeType])](),
        };
        let layout = self.computeStructLayout(struct_id, type_params);

        let allocInst = graph::createAllocateStackInst(layout);
        self.current().appendInst(allocInst);

        self.writeVariable(dest, self.current(), allocInst);
        let mut fieldId = 0i32;

        assert(layout.fields.size() == self.pushed_registers.size());

        for idx in std::range(0, layout.fields.size()) {
            let reg = self.pushed_registers(idx);
            let ty = self.reg(reg);

            if !ty.isUnit() {
                let field = layout.fields(idx);
                let arg = self.readVariable(reg, self.current());

                self.storeField(field.ty, allocInst, field.offset, arg, false);
            }

            fieldId = fieldId + 1i32;
        }

        self.pushed_registers.clear();
    }

    fn emitNewEnum(dest: BytecodeRegister, idx: ConstPoolId) {
        let (enum_id, type_params, variant_id) = match self.bc.constPool(idx) {
            ConstPoolEntry::EnumVariant(enum_id, type_params, variant_id) => (enum_id, type_params, variant_id),
            _ => unreachable[(EnumId, Array[BytecodeType], Int32)](),
        };
        let layout = self.computeEnumLayout(enum_id, type_params);
        let type_params = type_params.specialize(self.typeParams);

        match layout {
            EnumLayout::Int32 => {
                assert(self.pushed_registers.isEmpty());
                let inst = graph::createInt32Const(variant_id);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            }

            EnumLayout::PtrOrNull(_) => {
                if self.pushed_registers.isEmpty() {
                    let inst = graph::createNullConst();
                    self.current().appendInst(inst);
                    self.writeVariable(dest, self.current(), inst);
                } else {
                    assert(self.pushed_registers.size() == 1);
                    let reg = self.pushed_registers(0);
                    let src = self.readVariable(reg, self.current());
                    self.writeVariable(dest, self.current(), src);
                }
            }

            EnumLayout::Tagged => {
                let (classptr, size) = iface::getClassDataForEnumVariant(enum_id, type_params, variant_id);

                let data = AllocationData(classptr, size);
                let info = InstExtraData::AllocationData(data);
                let objInst = graph::createNewObjectInst(info);
                objInst.set_bytecode_position(self.offset);
                self.current().appendInst(objInst);

                self.writeVariable(dest, self.current(), objInst);
                let mut field_id = 0i32;

                let variant_id_inst = graph::createInt32Const(variant_id);
                self.current().appendInst(variant_id_inst);
                self.storeField(BytecodeType::Int32, objInst, iface::OBJECT_HEADER_LENGTH, variant_id_inst, true);

                for reg in self.pushed_registers {
                    let arg = self.readVariable(reg, self.current());
                    let argTy = self.reg(reg);

                    let offset = iface::getFieldOffsetForEnumVariant(enum_id, type_params, variant_id, field_id);
                    self.storeField(argTy, objInst, offset, arg, true);
                    field_id = field_id + 1i32;
                }
            }
        }

        self.pushed_registers.clear();
    }

    fn emitNewTuple(dest: BytecodeRegister, idx: ConstPoolId) {
        let subtypes = match self.bc.constPool(idx) {
            ConstPoolEntry::Tuple(subtypes) => subtypes,
            _ => unreachable[Array[BytecodeType]](),
        };
        let layout = self.computeTupleLayout(subtypes);

        let allocInst = graph::createAllocateStackInst(layout);
        self.current().appendInst(allocInst);

        self.writeVariable(dest, self.current(), allocInst);
        let mut fieldId = 0i32;

        assert(layout.fields.size() == self.pushed_registers.size());

        for idx in std::range(0, layout.fields.size()) {
            let reg = self.pushed_registers(idx);
            let field = layout.fields(idx);
            let arg = self.readVariable(reg, self.current());

            self.storeField(field.ty, allocInst, field.offset, arg, false);

            fieldId = fieldId + 1i32;
        }

        self.pushed_registers.clear();
    }

    fn emitNewArray(dest: BytecodeRegister, idx: ConstPoolId, length: BytecodeRegister) {
        let (class_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Class(class_id, type_params) => (class_id, type_params),
            _ => unreachable[(ClassId, Array[BytecodeType])](),
        };

        let type_params = type_params.specialize(self.typeParams);
        let element_size = iface::getElementSize(class_id, type_params);
        let element_size_inst = graph::createInt64Const(element_size.toInt64());
        self.current().appendInst(element_size_inst);

        let length = self.readVariable(length, self.current());

        let tmp = graph::createBinaryInst(Op::CheckedMul, Type::Int64, length, element_size_inst);
        tmp.set_bytecode_position(self.offset);
        self.current().appendInst(tmp);

        let header_size = graph::createInt64Const(iface::ARRAY_HEADER_LENGTH.toInt64());
        self.current().appendInst(header_size);

        let unaligned_size = graph::createBinaryInst(Op::CheckedAdd, Type::Int64, tmp, header_size);
        unaligned_size.set_bytecode_position(self.offset);
        self.current().appendInst(unaligned_size);

        let size = if element_size % iface::PTR_SIZE != 0i32 {
            let ptr_size_minus_1 = (iface::PTR_SIZE - 1i32).toInt64();
            let tmp = graph::createInt64Const(ptr_size_minus_1);
            self.current().appendInst(tmp);

            let tmp = graph::createBinaryInst(Op::CheckedAdd, Type::Int64, unaligned_size, tmp);
            tmp.set_bytecode_position(self.offset);
            self.current().appendInst(tmp);

            let mask = graph::createInt64Const(!ptr_size_minus_1);
            self.current().appendInst(mask);

            let tmp = graph::createBinaryInst(Op::And, Type::Int64, tmp, mask);
            self.current().appendInst(tmp);

            tmp
        } else {
            unaligned_size
        };

        let classptr = iface::getClassPointer(class_id, type_params);
        let info = ClassInfo(class_id, type_params);

        let objInst = graph::createNewArrayInst(info, size, length);
        objInst.set_bytecode_position(self.offset);
        self.current().appendInst(objInst);

        self.writeVariable(dest, self.current(), objInst);
    }

    fn loadGlobal(ty: BytecodeType, id: GlobalId): Inst {
        if ty.isStruct() {
            let (struct_id, type_params) = match ty {
                BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
                _ => unreachable[(StructId, Array[BytecodeType])](),
            };
            let layout = self.computeStructLayout(struct_id, type_params);

            let dest = graph::createAllocateStackInst(layout);
            self.current().appendInst(dest);
            let src = graph::createGetGlobalAddressInst(id);
            self.current().appendInst(src);
            self.copyStruct(ty, dest, 0i32, src, 0i32, false);
            dest

        } else if ty.isTuple() {
            let subtypes = match ty {
                BytecodeType::Tuple(subtypes) => subtypes,
                _ => unreachable[Array[BytecodeType]](),
            };
            let layout = self.computeTupleLayout(subtypes);

            let dest = graph::createAllocateStackInst(layout);
            self.current().appendInst(dest);
            let src = graph::createGetGlobalAddressInst(id);
            self.current().appendInst(src);
            self.copyTuple(ty, dest, 0i32, src, 0i32, false);

            dest

        } else {
            let ty = self.graphTy(ty);
            let inst = graph::createLoadGlobalInst(ty, id);
            self.current().appendInst(inst);
            inst
        }
    }

    fn storeGlobal(ty: BytecodeType, id: GlobalId, value: Inst) {
        if ty.isStruct() {
            let (struct_id, type_params) = match ty {
                BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
                _ => unreachable[(StructId, Array[BytecodeType])](),
            };
            let layout = self.computeStructLayout(struct_id, type_params);

            let dest = graph::createGetGlobalAddressInst(id);
            self.current().appendInst(dest);
            self.copyStruct(ty, dest, 0i32, value, 0i32, false);

        } else if ty.isTuple() {
            let subtypes = match ty {
                BytecodeType::Tuple(subtypes) => subtypes,
                _ => unreachable[Array[BytecodeType]](),
            };
            let layout = self.computeTupleLayout(subtypes);

            let dest = graph::createGetGlobalAddressInst(id);
            self.current().appendInst(dest);
            self.copyTuple(ty, dest, 0i32, value, 0i32, false);

        } else {
            let ty = self.graphTy(ty);
            let inst = graph::createStoreGlobalInst(ty, id, value);
            self.current().appendInst(inst);
        }
    }

    fn loadField(ty: BytecodeType, src: Inst, srcOffset: Int32): Inst {
        if ty.isStruct() {
            let (struct_id, type_params) = match ty {
                BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
                _ => unreachable[(StructId, Array[BytecodeType])](),
            };
            let layout = self.computeStructLayout(struct_id, type_params);

            let dest = graph::createAllocateStackInst(layout);
            self.current().appendInst(dest);
            self.copyStruct(ty, dest, 0i32, src, srcOffset, false);
            dest

        } else if ty.isTuple() {
            let subtypes = match ty {
                BytecodeType::Tuple(subtypes) => subtypes,
                _ => unreachable[Array[BytecodeType]](),
            };
            let layout = self.computeTupleLayout(subtypes);

            let dest = graph::createAllocateStackInst(layout);
            self.current().appendInst(dest);
            self.copyTuple(ty, dest, 0i32, src, srcOffset, false);
            dest
        } else {
            let ty = self.graphTy(ty);
            assert(!ty.isUnit());
            let inst = graph::createLoadInst(src, srcOffset, ty);
            inst.set_bytecode_position(self.offset);
            self.current().appendInst(inst);
            inst
        }
    }

    fn loadArray(ty: BytecodeType, arr: Inst, index: Inst): Inst {
        if ty.isStruct() {
            let (struct_id, type_params) = match ty {
                BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
                _ => unreachable[(StructId, Array[BytecodeType])](),
            };
            let layout = self.computeStructLayout(struct_id, type_params);

            let dest = graph::createAllocateStackInst(layout);
            self.current().appendInst(dest);

            let src = graph::createGetElementPtrInst(arr, index, layout.size.toInt64());
            self.current().appendInst(src);

            self.copyStruct(ty, dest, 0i32, src, 0i32, false);
            dest

        } else if ty.isTuple() {
            let subtypes = match ty {
                BytecodeType::Tuple(subtypes) => subtypes,
                _ => unreachable[Array[BytecodeType]](),
            };

            let layout = self.computeTupleLayout(subtypes);
            let dest = graph::createAllocateStackInst(layout);
            self.current().appendInst(dest);

            let src = graph::createGetElementPtrInst(arr, index, layout.size.toInt64());
            self.current().appendInst(src);

            self.copyTuple(ty, dest, 0i32, src, 0i32, false);
            dest

        } else {
            let ty = self.graphTy(ty);
            assert(!ty.isUnit());
            let inst = graph::createLoadArrayInst(arr, index, ty);
            self.current().appendInst(inst);
            inst
        }
    }

    fn storeArray(ty: BytecodeType, arr: Inst, index: Inst, value: Inst) {
        if ty.isStruct() {
            let (struct_id, type_params) = match ty {
                BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
                _ => unreachable[(StructId, Array[BytecodeType])](),
            };

            let structLayout = self.computeStructLayout(struct_id, type_params);
            let dest = graph::createGetElementPtrInst(arr, index, structLayout.size.toInt64());
            self.current().appendInst(dest);

            self.storeArrayNested(ty, arr, dest, 0i32, value, 0i32);
        } else if ty.isTuple() {
            let subtypes = match ty {
                BytecodeType::Tuple(subtypes) => subtypes,
                _ => unreachable[Array[BytecodeType]](),
            };

            let tupleLayout = self.computeTupleLayout(subtypes);
            let dest = graph::createGetElementPtrInst(arr, index, tupleLayout.size.toInt64());
            self.current().appendInst(dest);

            self.storeArrayNested(ty, arr, dest, 0i32, value, 0i32);
        } else if config.needsWriteBarrier && isReference(ty) {
            let inst = graph::createStoreArrayWbInst(arr, index, value, Type::Ptr);
            self.current().appendInst(inst);
        } else {
            let ty = self.graphTy(ty);
            assert(!ty.isUnit());
            let inst = graph::createStoreArrayInst(arr, index, value, ty);
            self.current().appendInst(inst);
        }
    }

    fn storeArrayNested(ty: BytecodeType, array: Inst, dest: Inst, destOffset: Int32, src: Inst, srcOffset: Int32) {
        if ty.isStruct() {
            let (struct_id, type_params) = match ty {
                BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
                _ => unreachable[(StructId, Array[BytecodeType])](),
            };

            let structLayout = self.computeStructLayout(struct_id, type_params);

            for field in structLayout.fields {
                self.storeArrayNested(field.ty, array, dest, destOffset + field.offset, src, srcOffset + field.offset);
            }
        } else if ty.isTuple() {
            let subtypes = match ty {
                BytecodeType::Tuple(subtypes) => subtypes,
                _ => unreachable[Array[BytecodeType]](),
            };

            let tupleLayout = self.computeTupleLayout(subtypes);

            for field in tupleLayout.fields {
                self.storeArrayNested(field.ty, array, dest, destOffset + field.offset, src, srcOffset + field.offset);
            }
        } else if config.needsWriteBarrier && isReference(ty) {
            let ty = Type::Ptr;
            let value = graph::createLoadInst(src, srcOffset, ty);
            self.current().appendInst(value);

            let inst = graph::createStoreArrayAddressWbInst(array, dest, destOffset, value, ty);
            self.current().appendInst(inst);
        } else {
            let ty = self.graphTy(ty);

            if !ty.isUnit() {
                let value = graph::createLoadInst(src, srcOffset, ty);
                self.current().appendInst(value);

                let inst = graph::createStoreInst(dest, destOffset, value, ty);
                self.current().appendInst(inst);
            }
        }
    }

    fn storeField(ty: BytecodeType, dest: Inst, destOffset: Int32, value: Inst, heap: Bool) {
        if ty.isStruct() {
            self.copyStruct(ty, dest, destOffset, value, 0i32, heap);
        } else if ty.isTuple() {
            self.copyTuple(ty, dest, destOffset, value, 0i32, heap);
        } else if heap && config.needsWriteBarrier && isReference(ty) {
            let inst = graph::createStoreWbInst(dest, destOffset, value, Type::Ptr);
            self.current().appendInst(inst);
        } else {
            let ty = self.graphTy(ty);
            assert(!ty.isUnit());
            let inst = graph::createStoreInst(dest, destOffset, value, ty);
            self.current().appendInst(inst);
        }
    }

    fn copyField(ty: BytecodeType, dest: Inst, destOffset: Int32, src: Inst, srcOffset: Int32, heap: Bool) {
        if ty.isStruct() {
            self.copyStruct(ty, dest, destOffset, src, srcOffset, heap);
        } else if ty.isTuple() {
            self.copyTuple(ty, dest, destOffset, src, srcOffset, heap);
        } else if heap && config.needsWriteBarrier && isReference(ty) {
            let ty = Type::Ptr;
            let value = graph::createLoadInst(src, srcOffset, ty);
            self.current().appendInst(value);
            let inst = graph::createStoreWbInst(dest, destOffset, value, ty);
            self.current().appendInst(inst);
        } else {
            let ty = self.graphTy(ty);

            if !ty.isUnit() {
                let value = graph::createLoadInst(src, srcOffset, ty);
                self.current().appendInst(value);
                let inst = graph::createStoreInst(dest, destOffset, value, ty);
                self.current().appendInst(inst);
            }
        }
    }

    fn copyTuple(ty: BytecodeType, dest: Inst, destOffset: Int32, src: Inst, srcOffset: Int32, heap: Bool) {
        let subtypes = match ty {
            BytecodeType::Tuple(subtypes) => subtypes,
            _ => unreachable[Array[BytecodeType]](),
        };

        let tupleLayout = self.computeTupleLayout(subtypes);

        for field in tupleLayout.fields {
            self.copyField(field.ty, dest, destOffset + field.offset, src, srcOffset + field.offset, heap);
        }
    }

    fn copyStruct(ty: BytecodeType, dest: Inst, destOffset: Int32, src: Inst, srcOffset: Int32, heap: Bool) {
        let (struct_id, type_params) = match ty {
            BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
            _ => unreachable[(StructId, Array[BytecodeType])](),
        };

        let structLayout = self.computeStructLayout(struct_id, type_params);

        for field in structLayout.fields {
            self.copyField(field.ty, dest, destOffset + field.offset, src, srcOffset + field.offset, heap);
        }
    }

    fn computeStructLayout(struct_id: StructId, type_params: Array[BytecodeType]): RecordLayout {
        let type_params = type_params.specialize(self.typeParams);
        regalloc::computeStructLayout(struct_id, type_params)
    }

    fn computeTupleLayout(subtypes: Array[BytecodeType]): RecordLayout {
        let subtypes = subtypes.specialize(self.typeParams);
        regalloc::computeTupleLayout(subtypes)
    }

    fn computeEnumLayout(enum_id: EnumId, type_params: Array[BytecodeType]): EnumLayout {
        let type_params = type_params.specialize(self.typeParams);
        regalloc::computeEnumLayout(enum_id, type_params)
    }

    fn regGraphTy(id: BytecodeRegister): Type {
        let ty = self.reg(id);
        self.graphTy(ty)
    }

    fn graphTy(ty: BytecodeType): Type {
        match ty {
            BytecodeType::Unit => Type::Unit,
            BytecodeType::Bool => Type::Bool,
            BytecodeType::UInt8 => Type::UInt8,
            BytecodeType::Float32 => Type::Float32,
            BytecodeType::Int32 => Type::Int32,
            BytecodeType::Char => Type::Int32,
            BytecodeType::Int64 => Type::Int64,
            BytecodeType::Float64 => Type::Float64,
            BytecodeType::Class(_, _)
            | BytecodeType::Ptr
            | BytecodeType::Lambda(_, _)
            | BytecodeType::Trait(_, _) => Type::Ptr,
            BytecodeType::Enum(enum_id, type_params) => {
                let layout = self.computeEnumLayout(enum_id, type_params);
                match layout {
                    EnumLayout::Int32 => Type::Int32,
                    EnumLayout::PtrOrNull(_) | EnumLayout::Tagged => Type::Ptr,
                }
            }
            BytecodeType::Struct(_, _)
            | BytecodeType::Tuple(_) => Type::Address,
            BytecodeType::This
            | BytecodeType::TypeParam(_) => unreachable[Type](),
            BytecodeType::TypeAlias(_) => unreachable[Type](),
        }
    }

    fn reg(id: BytecodeRegister): BytecodeType {
        self.regId(id.value)
    }

    fn regId(id: Int32): BytecodeType {
        let ty = self.bc.registers(id.toInt64());
        ty.specialize(self.typeParams)
    }
}

fn createBlocks(graph: Graph, bc: BytecodeFunction): BlockMap {
    let blockMap = BlockMap::new(bc);

    // The first pass creates blocks
    BlockCreator::new(graph, bc, blockMap).run();

    blockMap
}

fn createEdges(graph: Graph, bc: BytecodeFunction, blockMap: BlockMap) {
    // The second pass creates edges between blocks
    EdgeCreator::new(graph, bc, blockMap).run();
}

class BlockMap {
    bc: BytecodeFunction,
    blocks: HashMap[Int64, Block],
}

impl BlockMap {
    static fn new(bc: BytecodeFunction): BlockMap {
        BlockMap(bc, HashMap[Int64, Block]::new())
    }

    fn insert(offset: Int64, block: Block) {
        self.blocks.insert(offset, block);
    }

    fn blockAt(offset: Int64): Option[Block] {
        self.blocks(offset)
    }

    fn nextBlockAt(offset: Int64): Option[Block] {
        let mut offset = offset;

        while offset < self.bc.code.size() {
            let result = self.blockAt(offset);
            if result.isSome() { return result; }
            offset = offset + 1i64;
        }

        None
    }
}

class EdgeCreator {
    graph: Graph,
    bc: BytecodeFunction,
    blockMap: BlockMap,
    currentBlock: Option[Block],
    blockTerminated: Bool,
}

impl EdgeCreator {
    static fn new(graph: Graph, bc: BytecodeFunction, blockMap: BlockMap): EdgeCreator {
        EdgeCreator(graph, bc, blockMap, None[Block], false)
    }

    fn run() {
        for inst in BytecodeIterator::new(self.bc.code) {
            self.instructionStart(inst.start);
            self.processInstruction(inst.start, inst.size, inst.op);
        }
    }

    fn processInstruction(offset: Int64, size: Int64, inst: BytecodeInstruction) {
        match inst {
            BytecodeInstruction::JumpLoop(distance) => {
                let targetBlock = self.blockMap.blockAt(offset - distance.toInt64()).getOrPanic();
                self.currentBlock.getOrPanic().addSuccessor(targetBlock);
                self.markBlockTerminated();
            }
            BytecodeInstruction::JumpIfFalse(_opnd, distance) => {
                let targetBlock = self.blockMap.blockAt(offset + distance.toInt64()).getOrPanic();
                self.currentBlock.getOrPanic().addSuccessor(targetBlock);
            }
            BytecodeInstruction::JumpIfTrue(_opnd, distance) => {
                let targetBlock = self.blockMap.blockAt(offset + distance.toInt64()).getOrPanic();
                self.currentBlock.getOrPanic().addSuccessor(targetBlock);
            }
            BytecodeInstruction::Jump(distance) => {
                let targetBlock = self.blockMap.blockAt(offset + distance.toInt64()).getOrPanic();
                self.currentBlock.getOrPanic().addSuccessor(targetBlock);
                self.markBlockTerminated();
            }
            BytecodeInstruction::Ret(_) => {
                self.markBlockTerminated();
            }

            _ => {
                // Non-terminator instruction
            }
        }
    }

    fn instructionStart(offset: Int64) {
        let result = self.blockMap.blocks(offset);

        if result.isSome() {
            let nextBlock = result.getOrPanic();

            if self.currentBlock.isSome() {
                if !self.blockTerminated {
                    self.currentBlock.getOrPanic().addSuccessor(nextBlock);
                }
            }

            self.currentBlock = Some(nextBlock);
        }

        self.blockTerminated = false;
    }

    fn markBlockTerminated() {
        self.blockTerminated = true;
    }
}

class BlockCreator {
    graph: Graph,
    bc: BytecodeFunction,
    blockMap: BlockMap,
    blockStarts: BitSet,
}

impl BlockCreator {
    static fn new(graph: Graph, bc: BytecodeFunction, blockMap: BlockMap): BlockCreator {
        BlockCreator(
            graph,
            bc,
            blockMap,
            BitSet::new(bc.code.size())
        )
    }

    fn run() {
        // create block for first instruction
        let entryBlock = self.ensureBlock(0).getOrPanic();
        self.graph.setEntryBlock(entryBlock);

        for instInfo in BytecodeIterator::new(self.bc.code) {
            let start = instInfo.start;

            if self.blockStarts.contains(start) {
                self.ensureBlock(start);
            }

            self.processInstruction(start, instInfo.size, instInfo.op);
        }
    }

    fn processInstruction(start: Int64, size: Int64, inst: BytecodeInstruction) {
        match inst {
            BytecodeInstruction::Ret(_) => {
                self.ensureBlock(start + size);
            },
            BytecodeInstruction::LoopStart => {
                self.ensureBlock(start);
            },
            BytecodeInstruction::JumpLoop(distance) => {
                let target = start - distance.toInt64();
                assert(self.blockMap.blockAt(target).isSome());
            },
            BytecodeInstruction::JumpIfFalse(_opnd, distance) => {
                self.ensureBlockLazy(start + distance.toInt64());
                self.ensureBlock(start + size);
            },
            BytecodeInstruction::JumpIfTrue(_opnd, distance) => {
                self.ensureBlockLazy(start + distance.toInt64());
                self.ensureBlock(start + size);
            },
            BytecodeInstruction::Jump(distance) => {
                self.ensureBlockLazy(start + distance.toInt64());
                self.ensureBlock(start + size);
            },

            _ => {
                // Non-terminator instruction
            },
        }
    }

    fn ensureBlock(offset: Int64): Option[Block] {
        assert(offset <= self.bc.code.size());
        if offset == self.bc.code.size() {
            return None;
        }

        let result = self.blockMap.blockAt(offset);
        if result.isSome() { return result; }

        let block = Block::new();
        self.graph.addBlock(block);
        self.blockMap.insert(offset, block);
        Some(block)
    }

    fn ensureBlockLazy(offset: Int64) {
        self.blockStarts.insert(offset);
    }
}
