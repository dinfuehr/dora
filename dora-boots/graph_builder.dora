use std::HashMap;
use std::BitSet;

use package::bytecode::opcode as opc;
use package::graph;
use package::graph::{Block, Graph, Inst, InstExtraData, Op};
use package::graph::ty::Type;
use package::graph::{ClassInfo, FunctionInfo, CallKind, VirtualFunctionInfo, LambdaFunctionInfo, StructInfo, TupleInfo, TraitObjectInfo};
use package::bytecode::data::{BytecodeFunction, BytecodeRegister, BytecodeType, ClassId, FctId, FieldId};
use package::bytecode::data::{ConstPoolId, ConstPoolEntry, GlobalId, StructId, StructFieldId, TraitId};
use package::bytecode::instruction::BytecodeInstruction;
use package::bytecode::reader::BytecodeIterator;
use package::interface as iface;
use package::interface::{config, CompilationInfo};
use package::regalloc::{computeStructLayout, computeTupleLayout};

pub fn createGraph(ci: CompilationInfo): Graph {
    let graph = Graph::new();

    // Create basic blocks for the bytecode.
    let blockMap = createBlocks(graph, ci.bc);

    // Create edges for the blocks.
    createEdges(graph, ci.bc, blockMap);

    // Fill basic blocks with instructions.
    let ssagen = SsaGen::new(ci, graph, ci.bc, blockMap, ci.typeParams);
    ssagen.run();

    graph
}

class SsaGen {
    ci: CompilationInfo,
    graph: Graph,
    bc: BytecodeFunction,
    typeParams: Array[BytecodeType],
    blockMap: BlockMap,
    currentBlock: Option[Block],
    offset: Int32,
    currentDef: Array[HashMap[Block, Inst]],
    blockTerminated: Bool,

    returnValueArgInst: Option[Inst],

    pushed_registers: Vec[BytecodeRegister],

    // a block is considered filled when all instructions are inserted
    filledBlocks: BitSet,

    // block is considered sealed when the set of predecessors is final
    sealedBlocks: BitSet,

    // tracks all incomplete phi instructions inserted into unsealed blocks
    incompletePhis: HashMap[Block, HashMap[BytecodeRegister, Inst]],
}

impl SsaGen {
    static fn new(ci: CompilationInfo, graph: Graph, bc: BytecodeFunction, blockMap: BlockMap, typeParams: Array[BytecodeType]): SsaGen {
        SsaGen(
            ci,
            graph,
            bc,
            typeParams,
            blockMap,
            None[Block],
            0i32,
            Array[HashMap[Block, Inst]]::new(),
            false,
            None[Inst],
            Vec[BytecodeRegister]::new(),
            BitSet::new(0),
            BitSet::new(0),
            HashMap[Block, HashMap[BytecodeRegister, Inst]]::new(),
        )
    }

    fn run() {
        self.prepare();
        self.setupArguments();

        for inst in BytecodeIterator::new(self.bc.code) {
            self.instructionStart(inst.start.toInt32());
            self.processInstruction(inst.op);
        }

        assert(self.blockTerminated);
    }

    fn prepare() {
        self.currentBlock = None;
        let blockCount = self.graph.blockCount();

        self.filledBlocks = BitSet::new(blockCount.toInt64());
        self.sealedBlocks = BitSet::new(blockCount.toInt64());

        let data = Vec[HashMap[Block, Inst]]::new();

        for i in std::range(0, self.bc.registers.size()) {
            data.push(HashMap[Block, Inst]::new());
        }

        self.currentDef = data.toArray();
    }

    fn setupArguments() {
        let mut argIdx = 0i32;
        let entryBlock = self.graph.getEntryBlock();
        let retTy = self.ci.returnType;

        if retTy.isStruct() || retTy.isTuple() {
            let argInst = graph::createArgInst(argIdx, Type::Address);
            argIdx = argIdx + 1i32;
            entryBlock.appendInst(argInst);
            self.returnValueArgInst = Some[Inst](argInst);
        }

        let mut regIdx = 0i32;
        while regIdx < self.bc.arguments {
            let ty = self.regId(regIdx);
            let ty = if ty.isStruct() || ty.isTuple() {
                Type::Address
            } else {
                Type::fromBytecodeType(ty)
            };
            let argInst = graph::createArgInst(argIdx, ty);
            entryBlock.appendInst(argInst);
            self.writeVariable(BytecodeRegister(regIdx), entryBlock, argInst);
            argIdx = argIdx + 1i32;
            regIdx = regIdx + 1i32;
        }
    }

    fn current(): Block {
        self.currentBlock.getOrPanic()
    }

    fn writeVariable(register: BytecodeRegister, block: Block, value: Inst) {
        self.currentDef(register.value.toInt64()).insert(block, value);
    }

    fn readVariable(register: BytecodeRegister, block: Block): Inst {
        if self.currentDef(register.value.toInt64()).contains(block) {
            self.currentDef(register.value.toInt64())(block).getOrPanic()
        } else {
            self.readVariableRecursive(register, block)
        }
    }

    fn readVariableRecursive(register: BytecodeRegister, block: Block): Inst {
        let ty = self.regGraphTy(register);

        let value: Inst = if !self.sealedBlocks.contains(block.id().toInt64()) {
            // While all blocks are created with predecessors and successors before
            // this pass in the BlockBuilder already, we still need to handle unsealed blocks.
            // E.g. Register is accessed in while header and updated in the while body.
            // In this case the while header is filled before the while body. If we wouldn't
            // handle unsealed blocks we wouldn't create a Phi instruction, since the
            // while body predecessor is still empty.
            let incomplete = graph::createPhiInst(ty);
            block.appendPhi(incomplete);

            if self.incompletePhis.contains(block) {
                self.incompletePhis(block).getOrPanic().insert(register, incomplete);
            } else {
                let map = HashMap[BytecodeRegister, Inst]::new();
                map.insert(register, incomplete);
                self.incompletePhis.insert(block, map);
            }

            incomplete
        } else if block.predecessors.size() == 1i64 {
            self.readVariable(register, block.predecessors.first().getOrPanic().source)
        } else {
            let phi = graph::createPhiInst(ty);
            block.appendPhi(phi);
            self.writeVariable(register, block, phi);
            self.addPhiOperands(register, phi)
        };

        self.writeVariable(register, block, value);
        value
    }

    fn addPhiOperands(register: BytecodeRegister, phi: Inst): Inst {
        for pred in phi.getBlock().predecessors {
            let inst = self.readVariable(register, pred.source);
            phi.addInput(inst);
        }
        phi.registerUses();
        self.tryRemoveTrivialPhi(phi)
    }

    fn tryRemoveTrivialPhi(phi: Inst): Inst {
        let mut same = None[Inst];

        for inp in phi.getInputs() {
            let op = inp.getValue();

            if (same.isSome() && same.getOrPanic() === op) || op === phi {
                continue;
            }

            if same.isSome() {
                return phi;
            }

            same = Some(op);
        }

        if same.isNone() {
            same = Some(graph::createUndefInst());
        }

        let users = phi.users();

        phi.replaceWith(same.getOrPanic());
        phi.remove();

        for i in std::range(0, users.size()) {
            let user = users(i);

            if user === phi {
                continue;
            }

            if user.isPhi() {
                self.tryRemoveTrivialPhi(user);
            }
        }

        same.getOrPanic()
    }

    fn markBlockTerminated() {
        self.blockTerminated = true;
    }

    fn instructionStart(offset: Int32) {
        self.offset = offset;

        let block = self.blockMap.blockAt(offset.toInt64());

        if block.isSome() {
            if self.currentBlock.isSome() {
                self.blockEndReached(block);
            } else {
                self.currentBlock = block;
            }
        }

        self.blockTerminated = false;
    }

    fn processInstruction(inst: BytecodeInstruction) {
       match inst {
            BytecodeInstruction::Add(dest, lhs, rhs) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitBin(dest, lhs, rhs, Op::Add);
                } else {
                    self.emitBin(dest, lhs, rhs, Op::CheckedAdd);
                }
            },
            BytecodeInstruction::Sub(dest, lhs, rhs) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitBin(dest, lhs, rhs, Op::Sub);
                } else {
                    self.emitBin(dest, lhs, rhs, Op::CheckedSub);
                }
            },
            BytecodeInstruction::Neg(dest, src) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitUn(dest, src, Op::Neg);
                } else {
                    self.emitUn(dest, src, Op::CheckedNeg);
                }
            },
            BytecodeInstruction::Mul(dest, lhs, rhs) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitBin(dest, lhs, rhs, Op::Mul);
                } else {
                    self.emitBin(dest, lhs, rhs, Op::CheckedMul);
                }
            },
            BytecodeInstruction::Div(dest, lhs, rhs) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitDivMod(dest, lhs, rhs, Op::Div);
                } else {
                    self.emitDivMod(dest, lhs, rhs, Op::CheckedDiv);
                }
            },
            BytecodeInstruction::Mod(dest, lhs, rhs) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitDivMod(dest, lhs, rhs, Op::Mod);
                } else {
                    self.emitDivMod(dest, lhs, rhs, Op::CheckedMod);
                }
            },
            BytecodeInstruction::And(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::And);
            },
            BytecodeInstruction::Or(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Or);
            },
            BytecodeInstruction::Xor(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Xor);
            },
            BytecodeInstruction::Not(dest, src) => {
                self.emitUn(dest, src, Op::Not);
            },
            BytecodeInstruction::Shl(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Shl);
            },
            BytecodeInstruction::Shr(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Shr);
            },
            BytecodeInstruction::Sar(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Sar);
            },

            BytecodeInstruction::Mov(dest, src) => {
                self.emitMov(dest, src);
            },

            BytecodeInstruction::LoadTupleElement(dest, src, idx)  => {
                self.emitLoadTupleElement(dest, src, idx);
            },
            BytecodeInstruction::LoadEnumElement(dest, src, idx) => {                
                unimplemented();

            },
            BytecodeInstruction::LoadEnumVariant(dest, src, idx) => {
                unimplemented();
            },
            BytecodeInstruction::LoadStructField(dest, src, idx) => {
                self.emitLoadStructField(dest, src, idx);
            },

            BytecodeInstruction::LoadField(dest, obj, idx) => {
                self.emitLoadField(dest, obj, idx);
            },
            BytecodeInstruction::StoreField(src, obj, idx) => {
                self.emitStoreField(src, obj, idx);
            },

            BytecodeInstruction::LoadGlobal(dest, global_id) => {
                self.emitLoadGlobal(dest, global_id);
            },
            BytecodeInstruction::StoreGlobal(src, global_id) => {
                self.emitStoreGlobal(src, global_id);
            },

            BytecodeInstruction::PushRegister(src) => {
                self.pushed_registers.push(src);
            },
            
            BytecodeInstruction::ConstTrue(dest) => {
                let inst = graph::createBoolConst(true);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            },
            BytecodeInstruction::ConstFalse(dest) => {
                let inst = graph::createBoolConst(false);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            },
            BytecodeInstruction::ConstUInt8(dest, value) => {
                let inst = graph::createUInt8Const(value);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            },
            BytecodeInstruction::ConstChar(dest, idx) => {
                let value = self.bc.constPool(idx).toChar().getOrPanic();
                let inst = graph::createInt32Const(value.toInt32());
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            },
            BytecodeInstruction::ConstInt32(dest, idx) => {
                let value = self.bc.constPool(idx).toInt32().getOrPanic();
                let inst = graph::createInt32Const(value);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            },
            BytecodeInstruction::ConstInt64(dest, idx) => {
                let value = self.bc.constPool(idx).toInt64().getOrPanic();
                let inst = graph::createInt64Const(value);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            },
            BytecodeInstruction::ConstFloat32(dest, idx) => {
                let value = self.bc.constPool(idx).toFloat32().getOrPanic();
                let inst = graph::createFloat32Const(value);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);

            },
            BytecodeInstruction::ConstFloat64(dest, idx) => {
                let value = self.bc.constPool(idx).toFloat64().getOrPanic();
                let inst = graph::createFloat64Const(value);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            },
            BytecodeInstruction::ConstString(dest, idx) => {
                let value = self.bc.constPool(idx).toString().getOrPanic();
                let inst = graph::createStringConst(value);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            },

            BytecodeInstruction::TestIdentity(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::TestIdentity);
            },
            BytecodeInstruction::TestEq(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Equal);
            },
            BytecodeInstruction::TestNe(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::NotEqual);
            },
            BytecodeInstruction::TestGt(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Greater);
            },
            BytecodeInstruction::TestGe(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::GreaterOrEqual);
            },
            BytecodeInstruction::TestLt(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Less);
            },
            BytecodeInstruction::TestLe(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::LessOrEqual);
            },

            BytecodeInstruction::JumpLoop(distance) => {
                let targetBlock = self.blockMap.blockAt((self.offset - distance).toInt64()).getOrPanic();
                let gotoInst = graph::createGotoInst(targetBlock);
                self.current().appendInst(gotoInst);
                self.markBlockTerminated();
            },
            BytecodeInstruction::LoopStart => {
                // nothing to do
            },
            BytecodeInstruction::Jump(distance) => {
                self.emitJump(distance);
            },
            BytecodeInstruction::JumpIfFalse(opnd, distance) => {
                self.emitConditionalJump(opnd, distance, false);
            },
            BytecodeInstruction::JumpIfTrue(opnd, distance) => {
                self.emitConditionalJump(opnd, distance, true);
            },

            BytecodeInstruction::InvokeDirect(dest, idx) => {
                self.emitInvokeDirect(dest, idx);
            },
            BytecodeInstruction::InvokeVirtual(dest, idx) => {
                self.emitVirtualCall(dest, idx);
            },
            BytecodeInstruction::InvokeStatic(dest, idx) => {
                self.emitInvokeStatic(dest, idx);
            },
            BytecodeInstruction::InvokeLambda(dest, idx) => {
                self.emitLambdaCall(dest, idx);
            },
            BytecodeInstruction::InvokeGenericStatic(dest, idx) => {
                self.emitInvokeGeneric(dest, idx, CallKind::Static);
            },
            BytecodeInstruction::InvokeGenericDirect(dest, idx) => {
                self.emitInvokeGeneric(dest, idx, CallKind::Direct);
            },

            BytecodeInstruction::NewObject(dest, idx) => {
                self.emitNewObject(dest, idx);
            },
            BytecodeInstruction::NewObjectInitialized(dest, idx) => {
                self.emitNewObject(dest, idx);
            },
            BytecodeInstruction::NewArray(dest, idx, length) => {
                self.emitNewArray(dest, idx, length);
            },
            BytecodeInstruction::NewTuple(dest, idx) => {
                self.emitNewTuple(dest, idx);
            },
            BytecodeInstruction::NewEnum(dest, idx) => {
                unimplemented();
            },
            BytecodeInstruction::NewStruct(dest, idx) => {
                self.emitNewStruct(dest, idx);
            },
            BytecodeInstruction::NewTraitObject(dest, idx, obj) => {
                self.emitNewTraitObject(dest, idx, obj);
            },
            BytecodeInstruction::NewLambda(dest, idx) => {
                self.emitNewLambda(dest, idx);
            },
            BytecodeInstruction::ArrayLength(dest, src) => {
                let srcInst = self.readVariable(src, self.current());
                let destInst = graph::createArrayLength(srcInst);
                destInst.set_bytecode_position(self.offset);
                self.current().appendInst(destInst);
                self.writeVariable(dest, self.current(), destInst);
            },
            BytecodeInstruction::LoadArray(dest, arr, idx) => {
                self.emitLoadArray(dest, arr, idx);
            },
            BytecodeInstruction::StoreArray(src, arr, idx) => {
                self.emitStoreArray(src, arr, idx);
            },
            BytecodeInstruction::LoadTraitObjectValue(dest, src) => {
                self.emitLoadTraitObjectValue(dest, src);
            },
            BytecodeInstruction::Ret(opnd) => {
                self.emitRet(opnd);
            },
        }
    }

    fn blockEndReached(next: Option[Block]) {
        let block = self.current();

        if !self.blockTerminated {
            let gotoInst = graph::createGotoInst(next.getOrPanic());
            block.appendInst(gotoInst);
        }

        // We change the current block, that means all instructions
        // are inserted. The block is now filled.
        self.fillBlock(block);

        // We don't really know when to seal a block from the bytecode
        // Try to seal this block if all predecessors are filled.
        self.trySealBlock(block);

        // This block might have a back edge to a loop header. Since this
        // block is now filled, we might be able to seal another block.
        for succ in block.successors {
            self.trySealBlock(succ.target);
        }

        self.currentBlock = next;
    }

    fn fillBlock(block: Block) {
        assert(!self.filledBlocks.contains(block.id().toInt64()));
        self.filledBlocks.insert(block.id().toInt64());
    }

    fn trySealBlock(block: Block) {
        if self.sealedBlocks.contains(block.id().toInt64()) {
            return;
        }

        // all predecessors need to be filled
        for edge in block.predecessors {
            if !self.filledBlocks.contains(edge.source.id().toInt64()) {
                return;
            }
        }

        self.sealBlock(block);
    }

    fn sealBlock(block: Block) {
        assert(!self.sealedBlocks.contains(block.id().toInt64()));
        self.sealedBlocks.insert(block.id().toInt64());

        let map = self.incompletePhis(block);
        if map.isNone() { return; }

        for variableAndPhi in map.getOrPanic() {
            self.addPhiOperands(variableAndPhi.0, variableAndPhi.1);
        }
    }

    fn emitLoadField(dest: BytecodeRegister, obj: BytecodeRegister, idx: ConstPoolId) {
        let objInst = self.readVariable(obj, self.current());
        let destType = self.reg(dest);

        let (cls_id, type_params, field_id) = match self.bc.constPool(idx) {
            ConstPoolEntry::Field(cls_id, type_params, field_id) => (cls_id, type_params, field_id),
            _ => unreachable[(ClassId, Array[BytecodeType], FieldId)](),
        };

        let offset = iface::getFieldOffset(cls_id, type_params, field_id);
        let value = self.loadField(destType, objInst, offset);
        self.writeVariable(dest, self.current(), value);
    }

    fn emitStoreField(src: BytecodeRegister, obj: BytecodeRegister, idx: ConstPoolId) {
        let srcInst = self.readVariable(src, self.current());
        let srcType = self.reg(src);

        let objInst = self.readVariable(obj, self.current());

        let (cls_id, type_params, field_id) = match self.bc.constPool(idx) {
            ConstPoolEntry::Field(cls_id, type_params, field_id) => (cls_id, type_params, field_id),
            _ => unreachable[(ClassId, Array[BytecodeType], FieldId)](),
        };

        let offset = iface::getFieldOffset(cls_id, type_params, field_id);
        self.storeField(srcType, objInst, offset, srcInst, true);
    }

    fn emitLoadStructField(dest: BytecodeRegister, src: BytecodeRegister, idx: ConstPoolId) {
        let srcInst = self.readVariable(src, self.current());
        let destType = self.regGraphTy(dest);

        let (struct_id, type_params, field_id) = match self.bc.constPool(idx) {
            ConstPoolEntry::StructField(struct_id, type_params, field_id) => (struct_id, type_params, field_id),
            _ => unreachable[(StructId, Array[BytecodeType], StructFieldId)](),
        };

        let structLayout = computeStructLayout(struct_id, type_params);
        let field = structLayout.fields(field_id.value.toInt64());
        let destInst = graph::createLoadInst(srcInst, field.offset, destType);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitLoadTupleElement(dest: BytecodeRegister, src: BytecodeRegister, idx: ConstPoolId) {
        let srcInst = self.readVariable(src, self.current());
        let destType = self.regGraphTy(dest);

        let (tuple_ty, subtype_idx) = match self.bc.constPool(idx) {
            ConstPoolEntry::TupleElement(tuple_ty, subtype_idx) => (tuple_ty, subtype_idx),
            _ => unreachable[(BytecodeType, Int32)](),
        };

        assert(tuple_ty.isTuple());

        let subtypes = match tuple_ty {
            BytecodeType::Tuple(subtypes) => subtypes,
            _ => unreachable[Array[BytecodeType]](),
        };

        let tupleLayout = computeTupleLayout(subtypes);
        let offset = tupleLayout.fields(subtype_idx.toInt64()).offset;
        let destInst = graph::createLoadInst(srcInst, offset, destType);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitBin(dest: BytecodeRegister, lhs: BytecodeRegister, rhs: BytecodeRegister, op: Op) {
        let registerType = self.reg(lhs);

        let ty = match registerType {
            BytecodeType::Int32 => Type::int32(),
            BytecodeType::Int64 => Type::int64(),
            BytecodeType::Float32 => Type::float32(),
            BytecodeType::Float64 => Type::float64(),
            _ => unreachable[Type](),
        };

        let mut lhsInst = self.readVariable(lhs, self.current());
        let mut rhsInst = self.readVariable(rhs, self.current());

        if op.isCommutative() && lhsInst.isConst() {
            let tmp = lhsInst;
            lhsInst = rhsInst;
            rhsInst = tmp;
        }

        let destInst = graph::createBinaryInst(op, ty, lhsInst, rhsInst);
        destInst.set_bytecode_position(self.offset);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitUn(dest: BytecodeRegister, src: BytecodeRegister, op: Op) {
        let registerType = self.reg(dest);

        let ty = match registerType {
            BytecodeType::Int32 => Type::int32(),
            BytecodeType::Int64 => Type::int64(),
            BytecodeType::Float32 => Type::float32(),
            BytecodeType::Float64 => Type::float64(),
            _ => unreachable[Type](),
        };

        let srcInst = self.readVariable(src, self.current());
        let destInst = graph::createUnaryInst(op, ty, srcInst);
        destInst.set_bytecode_position(self.offset);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitDivMod(dest: BytecodeRegister, lhs: BytecodeRegister, rhs: BytecodeRegister, op: Op) {
        let registerType = self.reg(dest);

        let ty = match registerType {
            BytecodeType::Int32 => Type::int32(),
            BytecodeType::Int64 => Type::int64(),
            BytecodeType::Float32 => Type::float32(),
            BytecodeType::Float64 => Type::float64(),
            _ => unreachable[Type](),
        };
        
        let lhsInst = self.readVariable(lhs, self.current());
        let rhsInst = self.readVariable(rhs, self.current());

        if !registerType.isAnyFloat() {
            let divZeroCheck = graph::createDivZeroCheck(rhsInst);
            divZeroCheck.set_bytecode_position(self.offset);
            self.current().appendInst(divZeroCheck);
        }

        let destInst = graph::createBinaryInst(op, ty, lhsInst, rhsInst);
        destInst.set_bytecode_position(self.offset);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitMov(dest: BytecodeRegister, src: BytecodeRegister) {
        let srcInst = self.readVariable(src, self.current());
        self.writeVariable(dest, self.current(), srcInst);
    }

    fn emitLoadGlobal(dest: BytecodeRegister, glob: GlobalId) {
        let destType = self.regGraphTy(dest);

        if iface::hasGlobalInitialValue(glob) {
            let inst = graph::createEnsureGlobalInitializedInst(glob, destType);
            self.current().appendInst(inst);
            inst.set_bytecode_position(self.offset);
        }

        let globInst = graph::createLoadGlobalInst(destType, glob);
        self.current().appendInst(globInst);
        self.writeVariable(dest, self.current(), globInst);
    }

    fn emitStoreGlobal(src: BytecodeRegister, glob: GlobalId) {
        let srcType = self.regGraphTy(src);

        let srcInst = self.readVariable(src, self.current());
        let globInst = graph::createStoreGlobalInst(srcType, glob, srcInst);
        self.current().appendInst(globInst);

        if iface::hasGlobalInitialValue(glob) {
            let inst = graph::createMarkGlobalInitialized(glob);
            self.current().appendInst(inst);
        }
    }

    fn emitTest(dest: BytecodeRegister, lhs: BytecodeRegister, rhs: BytecodeRegister, op: Op) {
        let registerType = self.reg(lhs);

        let ty = match registerType {
            BytecodeType::Int32 => Type::int32(),
            BytecodeType::Int64 => Type::int64(),
            BytecodeType::Float32 => Type::float32(),
            BytecodeType::Float64 => Type::float64(),
            _ => unreachable[Type](),
        };

        let lhsInst = self.readVariable(lhs, self.current());
        let rhsInst = self.readVariable(rhs, self.current());
        let destInst = graph::createTestInst(op, ty, lhsInst, rhsInst);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitJump(offset: Int32) {
        let targetBlock = self.blockMap.blockAt((self.offset + offset).toInt64()).getOrPanic();
        let gotoInst = graph::createGotoInst(targetBlock);
        self.current().appendInst(gotoInst);
        self.markBlockTerminated();
    }

    fn emitConditionalJump(opnd: BytecodeRegister, distance: Int32, value: Bool) {
        let opndInst = self.readVariable(opnd, self.current());
        let targetBlock = self.blockMap.blockAt((self.offset + distance).toInt64()).getOrPanic();
        let fallthroughBlock = self.blockMap.nextBlockAt((self.offset+1i32).toInt64()).getOrPanic();

        let cond = if value {
            graph::createIfInst(opndInst, targetBlock, fallthroughBlock)
        } else {
            graph::createIfInst(opndInst, fallthroughBlock, targetBlock)
        };

        self.current().appendInst(cond);
        self.markBlockTerminated();
    }

    fn emitLoadArray(dest: BytecodeRegister, arr: BytecodeRegister, idx: BytecodeRegister) {
        let arrInst = self.readVariable(arr, self.current());
        let idxInst = self.readVariable(idx, self.current());

        let arrayLengthInst = graph::createArrayLength(arrInst);
        self.current().appendInst(arrayLengthInst);

        let boundsCheckInst = graph::createBoundsCheck(idxInst, arrayLengthInst);
        boundsCheckInst.set_bytecode_position(self.offset);
        self.current().appendInst(boundsCheckInst);

        let destType = self.regGraphTy(dest);
        let arrayLoadInst = graph::createLoadArray(arrInst, idxInst, destType);
        self.current().appendInst(arrayLoadInst);

        self.writeVariable(dest, self.current(), arrayLoadInst);
    }

    fn emitStoreArray(src: BytecodeRegister, arr: BytecodeRegister, idx: BytecodeRegister) {
        let srcInst = self.readVariable(src, self.current());
        let arrInst = self.readVariable(arr, self.current());
        let idxInst = self.readVariable(idx, self.current());
        let srcType = self.reg(src);

        let arrayLengthInst = graph::createArrayLength(arrInst);
        self.current().appendInst(arrayLengthInst);

        let boundsCheckInst = graph::createBoundsCheck(idxInst, arrayLengthInst);
        boundsCheckInst.set_bytecode_position(self.offset);
        self.current().appendInst(boundsCheckInst);

        if srcType.isPtr() && config.needsWriteBarrier {
            let inst = graph::createStoreArrayWbInst(arrInst, idxInst, srcInst, srcInst.getValueType());
            self.current().appendInst(inst);
        } else {
            let inst = graph::createStoreArrayInst(arrInst, idxInst, srcInst, srcInst.getValueType());
            self.current().appendInst(inst);
        }
    }

    fn emitLoadTraitObjectValue(dest: BytecodeRegister, src: BytecodeRegister) {
        let srcInst = self.readVariable(src, self.current());
        let destType = self.regGraphTy(dest);

        let offset = iface::OBJECT_HEADER_LENGTH;
        let value = self.loadField(BytecodeType::Ptr, srcInst, offset);

        self.writeVariable(dest, self.current(), value);
    }

    fn emitRet(opnd: BytecodeRegister) {
        let ty = self.reg(opnd);

        if ty.isUnit() {
            let inst = graph::createReturnVoidInst();
            self.current().appendInst(inst);
        } else if ty.isStruct() {
            let src = self.readVariable(opnd, self.current());
            let dest = self.returnValueArgInst.getOrPanic();
            self.copyStruct(ty, dest, 0i32, src, 0i32, false);
            let inst = graph::createReturnVoidInst();
            self.current().appendInst(inst);
        } else if ty.isTuple() {
            let src = self.readVariable(opnd, self.current());
            let dest = self.returnValueArgInst.getOrPanic();
            self.copyTuple(ty, dest, 0i32, src, 0i32, false);
            let inst = graph::createReturnVoidInst();
            self.current().appendInst(inst);
        } else {
            let value = self.readVariable(opnd, self.current());
            let ty = Type::fromBytecodeType(ty);
            let inst = graph::createReturnInst(value, ty);
            self.current().appendInst(inst);
        }

        self.markBlockTerminated();
    }

    fn emitInvokeDirect(dest: BytecodeRegister, idx: ConstPoolId) {
        let (fct_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Fct(fct_id, type_params) => (fct_id, type_params),
            _ => unreachable[(FctId, Array[BytecodeType])](),
        };

        let intrinsic = iface::getIntrinsicForFunction(fct_id);

        if intrinsic >= 0i32 {
            let args = self.pushed_registers.toArray();
            self.pushed_registers.clear();
            self.emitIntrinsic(intrinsic, dest, args);
        } else {
            self.emitCall(dest, CallKind::Direct, fct_id, type_params);
        }
    }

    fn emitInvokeStatic(dest: BytecodeRegister, idx: ConstPoolId) {
        let (fct_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Fct(fct_id, type_params) => (fct_id, type_params),
            _ => unreachable[(FctId, Array[BytecodeType])](),
        };

        let intrinsic = iface::getIntrinsicForFunction(fct_id);

        if intrinsic >= 0i32 {
            let args = self.pushed_registers.toArray();
            self.pushed_registers.clear();
            self.emitIntrinsic(intrinsic, dest, args);
        } else {
            self.emitCall(dest, CallKind::Static, fct_id, type_params);
        }
    }

    fn emitCall(dest: BytecodeRegister, kind: CallKind, fct_id: FctId, type_params: Array[BytecodeType]) {
        let args = Vec[Inst]::new();
        args.reserve(self.pushed_registers.size());
        let destTy = self.reg(dest);

        let valueTy = if destTy.isStruct() || destTy.isTuple() {
            let info = match destTy {
                BytecodeType::Struct(struct_id, type_params) => {
                    let info = StructInfo(struct_id, type_params);
                    InstExtraData::StructInfo(info)
                },

                BytecodeType::Tuple(subtypes) => {
                    let info = TupleInfo(subtypes);
                    InstExtraData::TupleInfo(info)
                },

                _ => unreachable[InstExtraData](),
            };

            let allocInst = graph::createAllocateStack(info);
            self.current().appendInst(allocInst);
            args.push(allocInst);

            self.writeVariable(dest, self.current(), allocInst);

            Type::Unit
        } else {
            Type::fromBytecodeType(destTy)
        };

        for reg in self.pushed_registers {
            let arg = self.readVariable(reg, self.current());
            args.push(arg);
        }

        self.pushed_registers.clear();

        let info = FunctionInfo(fct_id, type_params);

        let inst = graph::createCall(info, kind, args, valueTy);
        inst.set_bytecode_position(self.offset);
        self.current().appendInst(inst);

        if !valueTy.isUnit() {
            self.writeVariable(dest, self.current(), inst);
        }
    }

    fn emitVirtualCall(dest: BytecodeRegister, idx: ConstPoolId) {
        let args = Vec[Inst]::new();

        for reg in self.pushed_registers {
            let arg = self.readVariable(reg, self.current());
            args.push(arg);
        }

        self.pushed_registers.clear();

        let ty = self.regGraphTy(dest);

        let (trait_object_ty, fct_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::TraitObjectMethod(trait_object_ty, fct_id, type_params) => (trait_object_ty, fct_id, type_params),
            _ => unreachable[(BytecodeType, FctId, Array[BytecodeType])](),
        };

        let info = VirtualFunctionInfo(trait_object_ty, fct_id, type_params);

        let inst = graph::createVirtualCall(info, args, ty);
        inst.set_bytecode_position(self.offset);
        self.current().appendInst(inst);

        if !ty.isUnit() {
            self.writeVariable(dest, self.current(), inst);
        }
    }

    fn emitLambdaCall(dest: BytecodeRegister, idx: ConstPoolId) {
        let args = Vec[Inst]::new();

        for reg in self.pushed_registers {
            let arg = self.readVariable(reg, self.current());
            args.push(arg);
        }

        self.pushed_registers.clear();

        let ty = self.regGraphTy(dest);

        let (params, return_type) = match self.bc.constPool(idx) {
            ConstPoolEntry::Lambda(params, return_type) => (params, return_type),
            _ => unreachable[(Array[BytecodeType], BytecodeType)](),
        };

        let info = LambdaFunctionInfo(params, return_type);

        let inst = graph::createLambdaCall(info, args, ty);
        inst.set_bytecode_position(self.offset);
        self.current().appendInst(inst);

        if !ty.isUnit() {
            self.writeVariable(dest, self.current(), inst);
        }
    }

    fn emitInvokeGeneric(dest: BytecodeRegister, idx: ConstPoolId, kind: CallKind) {
        let ty = self.regGraphTy(dest);

        let (type_param_idx, trait_fct_id, trait_type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Generic(type_param_idx, trait_fct_id, trait_type_params) => (type_param_idx, trait_fct_id, trait_type_params),
            _ => unreachable[(Int32, FctId, Array[BytecodeType])](),
        };

        let object_ty = self.typeParams(type_param_idx.toInt64());
        let callee_id = iface::findTraitImpl(trait_fct_id, trait_type_params, object_ty);

        let intrinsic = iface::getIntrinsicForFunction(callee_id);

        if intrinsic >= 0i32 {
            let args = self.pushed_registers.toArray();
            self.pushed_registers.clear();
            self.emitIntrinsic(intrinsic, dest, args);
        } else {
            self.emitCall(dest, kind, callee_id, Array[BytecodeType]::new());
        }
    }

    fn emitIntrinsic(intrinsic: Int32, dest: BytecodeRegister, args: Array[BytecodeRegister]) {
        if intrinsic == opc::INTRINSIC_INT64_ADD || intrinsic == opc::INTRINSIC_INT32_ADD {
            self.emitBin(dest, args(0), args(1), Op::CheckedAdd);
        } else if intrinsic == opc::INTRINSIC_INT64_ADD_UNCHECKED || intrinsic == opc::INTRINSIC_INT32_ADD_UNCHECKED {
            self.emitBin(dest, args(0), args(1), Op::Add);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_ADD || intrinsic == opc::INTRINSIC_FLOAT32_ADD {
            self.emitBin(dest, args(0), args(1), Op::Add);
        } else if intrinsic == opc::INTRINSIC_THREAD_CURRENT {
            self.emitIntrinsicThreadCurrent(dest, args);
        } else {
            let name = opc::intrinsicName(intrinsic);
            std::fatalError("unknown intrinsic ${name}");
        }
    }

    fn emitIntrinsicThreadCurrent(dest: BytecodeRegister, args: Array[BytecodeRegister]) {
        assert(args.isEmpty());
        let inst = graph::createThreadCurrentInst();
        self.current().appendInst(inst);
        self.writeVariable(dest, self.current(), inst);
    }

    fn emitNewObject(dest: BytecodeRegister, idx: ConstPoolId) {
        let (class_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Class(class_id, type_params) => (class_id, type_params),
            _ => unreachable[(ClassId, Array[BytecodeType])](),
        };

        let info = ClassInfo(class_id, type_params);
        let info = InstExtraData::ClassInfo(info);
        let objInst = graph::createNewObject(info);
        objInst.set_bytecode_position(self.offset);
        self.current().appendInst(objInst);

        self.writeVariable(dest, self.current(), objInst);
        let mut field_id = 0i32;

        for reg in self.pushed_registers {
            let arg = self.readVariable(reg, self.current());
            let argTy = self.reg(reg);

            let offset = iface::getFieldOffset(class_id, type_params, FieldId(field_id));
            self.storeField(argTy, objInst, offset, arg, true);
            field_id = field_id + 1i32;
        }

        self.pushed_registers.clear();
    }

    fn emitNewLambda(dest: BytecodeRegister, idx: ConstPoolId) {
        let (fct_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Fct(fct_id, type_params) => (fct_id, type_params),
            _ => unreachable[(FctId, Array[BytecodeType])](),
        };

        let info = FunctionInfo(fct_id, type_params);
        let info = InstExtraData::FunctionInfo(info);
        let objInst = graph::createNewObject(info);
        objInst.set_bytecode_position(self.offset);
        self.current().appendInst(objInst);

        self.writeVariable(dest, self.current(), objInst);

        if !self.pushed_registers.isEmpty() {
            assert(self.pushed_registers.size() == 1);
            let reg = self.pushed_registers.first().getOrPanic();
            let arg = self.readVariable(reg, self.current());
            let argTy = self.reg(reg);

            let offset = iface::OBJECT_HEADER_LENGTH;
            self.storeField(argTy, objInst, offset, arg, true);
        }

        self.pushed_registers.clear();
    }

    fn emitNewTraitObject(dest: BytecodeRegister, idx: ConstPoolId, obj: BytecodeRegister) {
        let (trait_id, type_params, object_ty) = match self.bc.constPool(idx) {
            ConstPoolEntry::Trait(trait_id, type_params, object_ty) => (trait_id, type_params, object_ty),
            _ => unreachable[(TraitId, Array[BytecodeType], BytecodeType)](),
        };

        let info = TraitObjectInfo(trait_id, type_params, object_ty);
        let info = InstExtraData::TraitObjectInfo(info);
        let objInst = graph::createNewObject(info);
        objInst.set_bytecode_position(self.offset);
        self.current().appendInst(objInst);

        self.writeVariable(dest, self.current(), objInst);

        let arg = self.readVariable(obj, self.current());
        let argTy = self.reg(obj);

        let offset = iface::OBJECT_HEADER_LENGTH;
        self.storeField(argTy, objInst, offset, arg, true);
    }

    fn emitNewStruct(dest: BytecodeRegister, idx: ConstPoolId) {
        let (struct_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Struct(struct_id, type_params) => (struct_id, type_params),
            _ => unreachable[(StructId, Array[BytecodeType])](),
        };

        let info = StructInfo(struct_id, type_params);
        let info = InstExtraData::StructInfo(info);
        let allocInst = graph::createAllocateStack(info);
        self.current().appendInst(allocInst);

        self.writeVariable(dest, self.current(), allocInst);
        let mut fieldId = 0i32;

        let structLayout = computeStructLayout(struct_id, type_params);
        assert(structLayout.fields.size() == self.pushed_registers.size());

        for idx in std::range(0, structLayout.fields.size()) {
            let reg = self.pushed_registers(idx);
            let field = structLayout.fields(idx);
            let arg = self.readVariable(reg, self.current());

            self.storeField(field.ty, allocInst, field.offset, arg, false);

            fieldId = fieldId + 1i32;
        }

        self.pushed_registers.clear();
    }

    fn emitNewTuple(dest: BytecodeRegister, idx: ConstPoolId) {
        let subtypes = match self.bc.constPool(idx) {
            ConstPoolEntry::Tuple(subtypes) => subtypes,
            _ => unreachable[Array[BytecodeType]](),
        };

        let info = TupleInfo(subtypes);
        let info = InstExtraData::TupleInfo(info);
        let allocInst = graph::createAllocateStack(info);
        self.current().appendInst(allocInst);

        self.writeVariable(dest, self.current(), allocInst);
        let mut fieldId = 0i32;

        let tupleLayout = computeTupleLayout(subtypes);
        assert(tupleLayout.fields.size() == self.pushed_registers.size());

        for idx in std::range(0, tupleLayout.fields.size()) {
            let reg = self.pushed_registers(idx);
            let field = tupleLayout.fields(idx);
            let arg = self.readVariable(reg, self.current());

            self.storeField(field.ty, allocInst, field.offset, arg, false);

            fieldId = fieldId + 1i32;
        }

        self.pushed_registers.clear();
    }

    fn emitNewArray(dest: BytecodeRegister, idx: ConstPoolId, length: BytecodeRegister) {
        let (class_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Class(class_id, type_params) => (class_id, type_params),
            _ => unreachable[(ClassId, Array[BytecodeType])](),
        };

        let info = ClassInfo(class_id, type_params);
        let lengthInst = self.readVariable(length, self.current());

        let objInst = graph::createNewArray(info, lengthInst);
        objInst.set_bytecode_position(self.offset);
        self.current().appendInst(objInst);

        self.writeVariable(dest, self.current(), objInst);
    }

    fn loadField(ty: BytecodeType, src: Inst, srcOffset: Int32): Inst {
        if ty.isStruct() {
            let (struct_id, type_params) = match ty {
                BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
                _ => unreachable[(StructId, Array[BytecodeType])](),
            };

            let info = StructInfo(struct_id, type_params);
            let info = InstExtraData::StructInfo(info);
            let dest = graph::createAllocateStack(info);
            self.current().appendInst(dest);
            self.copyStruct(ty, dest, 0i32, src, srcOffset, false);
            dest

        } else if ty.isTuple() {
            let subtypes = match ty {
                BytecodeType::Tuple(subtypes) => subtypes,
                _ => unreachable[Array[BytecodeType]](),
            };

            let info = TupleInfo(subtypes);
            let info = InstExtraData::TupleInfo(info);
            let dest = graph::createAllocateStack(info);
            self.current().appendInst(dest);
            self.copyTuple(ty, dest, 0i32, src, srcOffset, false);
            dest

        } else {
            let ty = Type::fromBytecodeType(ty);
            let inst = graph::createLoadInst(src, srcOffset, ty);
            inst.set_bytecode_position(self.offset);
            self.current().appendInst(inst);
            inst
        }
    }

    fn storeField(ty: BytecodeType, dest: Inst, destOffset: Int32, value: Inst, heap: Bool) {
        if ty.isStruct() {
            self.copyStruct(ty, dest, destOffset, value, 0i32, heap);
        } else if ty.isTuple() {
            self.copyTuple(ty, dest, destOffset, value, 0i32, heap);
        } else if heap && config.needsWriteBarrier && ty.isPtr() {
            let ty = Type::fromBytecodeType(ty);
            let inst = graph::createStoreWbInst(dest, destOffset, value, ty);
            self.current().appendInst(inst);
        } else {
            let ty = Type::fromBytecodeType(ty);
            let inst = graph::createStoreInst(dest, destOffset, value, ty);
            self.current().appendInst(inst);
        }
    }

    fn copyField(ty: BytecodeType, dest: Inst, destOffset: Int32, src: Inst, srcOffset: Int32, heap: Bool) {
        if ty.isStruct() {
            self.copyStruct(ty, dest, destOffset, src, srcOffset, heap);
        } else if ty.isTuple() {
            self.copyTuple(ty, dest, destOffset, src, srcOffset, heap);
        } else {
            let ty = Type::fromBytecodeType(ty);
            let inst = graph::createLoadInst(src, srcOffset, ty);
            self.current().appendInst(inst);
            let inst = graph::createStoreInst(dest, destOffset, inst, ty);
            self.current().appendInst(inst);
            assert(!heap);
        }
    }

    fn copyTuple(ty: BytecodeType, dest: Inst, destOffset: Int32, src: Inst, srcOffset: Int32, heap: Bool) {
        let subtypes = match ty {
            BytecodeType::Tuple(subtypes) => subtypes,
            _ => unreachable[Array[BytecodeType]](),
        };

        let tupleLayout = computeTupleLayout(subtypes);

        for field in tupleLayout.fields {
            self.copyField(field.ty, dest, destOffset + field.offset, src, srcOffset + field.offset, heap);
        }
    }

    fn copyStruct(ty: BytecodeType, dest: Inst, destOffset: Int32, src: Inst, srcOffset: Int32, heap: Bool) {
        let (struct_id, type_params) = match ty {
            BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
            _ => unreachable[(StructId, Array[BytecodeType])](),
        };

        let structLayout = computeStructLayout(struct_id, type_params);

        for field in structLayout.fields {
            self.copyField(field.ty, dest, destOffset + field.offset, src, srcOffset + field.offset, heap);
        }
    }

    fn regGraphTy(id: BytecodeRegister): Type {
        Type::fromBytecodeType(self.reg(id))
    }

    fn reg(id: BytecodeRegister): BytecodeType {
        self.regId(id.value)
    }

    fn regId(id: Int32): BytecodeType {
        let ty = self.bc.registers(id.toInt64());
        ty.specialize(self.typeParams)
    }
}

fn createBlocks(graph: Graph, bc: BytecodeFunction): BlockMap {
    let blockMap = BlockMap::new(bc);

    // The first pass creates blocks
    BlockCreator::new(graph, bc, blockMap).run();

    blockMap
}

fn createEdges(graph: Graph, bc: BytecodeFunction, blockMap: BlockMap) {
    // The second pass creates edges between blocks
    EdgeCreator::new(graph, bc, blockMap).run();
}

class BlockMap {
    bc: BytecodeFunction,
    blocks: HashMap[Int64, Block],
}

impl BlockMap {
    static fn new(bc: BytecodeFunction): BlockMap {
        BlockMap(bc, HashMap[Int64, Block]::new())
    }

    fn insert(offset: Int64, block: Block) {
        self.blocks.insert(offset, block);
    }

    fn blockAt(offset: Int64): Option[Block] {
        self.blocks(offset)
    }

    fn nextBlockAt(offset: Int64): Option[Block] {
        let mut offset = offset;

        while offset < self.bc.code.size() {
            let result = self.blockAt(offset);
            if result.isSome() { return result; }
            offset = offset + 1i64;
        }

        None
    }
}

class EdgeCreator {
    graph: Graph,
    bc: BytecodeFunction,
    blockMap: BlockMap,
    currentBlock: Option[Block],
    blockTerminated: Bool,
}

impl EdgeCreator {
    static fn new(graph: Graph, bc: BytecodeFunction, blockMap: BlockMap): EdgeCreator {
        EdgeCreator(graph, bc, blockMap, None[Block], false)
    }

    fn run() {
        for inst in BytecodeIterator::new(self.bc.code) {
            self.instructionStart(inst.start);
            self.processInstruction(inst.start, inst.size, inst.op);
        }
    }

    fn processInstruction(offset: Int64, size: Int64, inst: BytecodeInstruction) {
        match inst {
            BytecodeInstruction::JumpLoop(distance) => {
                let targetBlock = self.blockMap.blockAt(offset - distance.toInt64()).getOrPanic();
                self.currentBlock.getOrPanic().addSuccessor(targetBlock);
                self.markBlockTerminated();
            },
            BytecodeInstruction::JumpIfFalse(_opnd, distance) => {
                let targetBlock = self.blockMap.blockAt(offset + distance.toInt64()).getOrPanic();
                self.currentBlock.getOrPanic().addSuccessor(targetBlock);
            },
            BytecodeInstruction::JumpIfTrue(_opnd, distance) => {
                let targetBlock = self.blockMap.blockAt(offset + distance.toInt64()).getOrPanic();
                self.currentBlock.getOrPanic().addSuccessor(targetBlock);
            },
            BytecodeInstruction::Jump(distance) => {
                let targetBlock = self.blockMap.blockAt(offset + distance.toInt64()).getOrPanic();
                self.currentBlock.getOrPanic().addSuccessor(targetBlock);
                self.markBlockTerminated();
            },
            BytecodeInstruction::Ret(_) => {
                self.markBlockTerminated();
            },

            _ => {
                // Non-terminator instruction
            },
        }
    }

    fn instructionStart(offset: Int64) {
        let result = self.blockMap.blocks(offset);

        if result.isSome() {
            let nextBlock = result.getOrPanic();

            if self.currentBlock.isSome() {
                if !self.blockTerminated {
                    self.currentBlock.getOrPanic().addSuccessor(nextBlock);
                }
            }

            self.currentBlock = Some(nextBlock);
        }

        self.blockTerminated = false;
    }

    fn markBlockTerminated() {
        self.blockTerminated = true;
    }
}

class BlockCreator {
    graph: Graph,
    bc: BytecodeFunction,
    blockMap: BlockMap,
    blockStarts: BitSet,
}

impl BlockCreator {
    static fn new(graph: Graph, bc: BytecodeFunction, blockMap: BlockMap): BlockCreator {
        BlockCreator(
            graph,
            bc,
            blockMap,
            BitSet::new(bc.code.size())
        )
    }

    fn run() {
        // create block for first instruction
        let entryBlock = self.ensureBlock(0).getOrPanic();
        self.graph.setEntryBlock(entryBlock);

        for instInfo in BytecodeIterator::new(self.bc.code) {
            let start = instInfo.start;

            if self.blockStarts.contains(start) {
                self.ensureBlock(start);
            }

            self.processInstruction(start, instInfo.size, instInfo.op);
        }
    }

    fn processInstruction(start: Int64, size: Int64, inst: BytecodeInstruction) {
        match inst {
            BytecodeInstruction::Ret(_) => {
                self.ensureBlock(start + size);
            },
            BytecodeInstruction::LoopStart => {
                self.ensureBlock(start);
            },
            BytecodeInstruction::JumpLoop(distance) => {
                let target = start - distance.toInt64();
                assert(self.blockMap.blockAt(target).isSome());
            },
            BytecodeInstruction::JumpIfFalse(_opnd, distance) => {
                self.ensureBlockLazy(start + distance.toInt64());
                self.ensureBlock(start + size);
            },
            BytecodeInstruction::JumpIfTrue(_opnd, distance) => {
                self.ensureBlockLazy(start + distance.toInt64());
                self.ensureBlock(start + size);
            },
            BytecodeInstruction::Jump(distance) => {
                self.ensureBlockLazy(start + distance.toInt64());
                self.ensureBlock(start + size);
            },

            _ => {
                // Non-terminator instruction
            },
        }
    }

    fn ensureBlock(offset: Int64): Option[Block] {
        assert(offset <= self.bc.code.size());
        if offset == self.bc.code.size() {
            return None;
        }

        let result = self.blockMap.blockAt(offset);
        if result.isSome() { return result; }

        let block = Block::new();
        self.graph.addBlock(block);
        self.blockMap.insert(offset, block);
        Some(block)
    }

    fn ensureBlockLazy(offset: Int64) {
        self.blockStarts.insert(offset);
    }
}
