use std::HashMap;
use std::BitSet;

use package::bytecode::opcode as opc;
use package::compilation::CompilationInfo;
use package::graph;
use package::graph::{AllocationData, Block, Graph, Inst, InstExtraData, Op};
use package::graph::ty::Type;
use package::graph::{ClassInfo, FunctionInfo, CallKind, VirtualFunctionInfo, LambdaFunctionInfo, TraitObjectInfo};
use package::bytecode::data::{BytecodeFunction, BytecodeRegister, BytecodeType, ClassId, EnumId, FctId, FieldId};
use package::bytecode::data::{ConstPoolId, ConstPoolEntry, GlobalId, StructId, StructFieldId, TraitId};
use package::bytecode::instruction::BytecodeInstruction;
use package::bytecode::reader::BytecodeIterator;
use package::interface as iface;
use package::interface::config;
use package::regalloc::{EnumLayout, isReference, RecordLayout};
use package::regalloc;

pub fn createGraph(ci: CompilationInfo): Graph {
    let graph = Graph::new();

    // Create basic blocks for the bytecode.
    let analysis = analyzeBytecode(graph, ci.bc);

    let predecessors = analysis.predecessors;

    for blockPos in analysis.starts {
        if predecessors(blockPos) == 0i32 {
            let successors = findSuccessorsForBlock(ci.bc, blockPos, analysis.starts);

            for succ in successors {
                predecessors(succ) = predecessors(succ) - 1i32;
            }
        }
    }

    if ci.emitGraph {
        dumpEdgeOverview(ci, graph);
    }

    // Fill basic blocks with instructions.
    let ssagen = SsaGen::new(ci, graph, analysis);
    ssagen.run();

    graph
}

fn dumpEdgeOverview(ci: CompilationInfo, graph: Graph) {
    println("blocks in ${ci.getDisplayName()}:");
    for block in graph.insertionOrderIterator() {
        print("  block ${block}");
        if block.getName().isSome() {
            print(" (${block.getName().getOrPanic()})");
        }
        if block.hasBytecodePosition() {
            print(" (at bc ${block.getBytecodePosition()})");
        }
        print(":");
        if !block.predecessors.isEmpty() {
            print(" preds:");
            for pred in block.predecessors {
                print(" ${pred.source}");
            }
        }

        if !block.successors.isEmpty() {
            print(" succs:");
            for succ in block.successors {
                print(" ${succ.target}");
            }
        }

        println("");
    }
    println("");
}

class SsaGen {
    ci: CompilationInfo,
    graph: Graph,
    bc: BytecodeFunction,
    typeParams: Array[BytecodeType],
    analysis: BytecodeAnalysisInfo,
    currentBlock: Option[Block],
    offset: Int32,
    currentDef: Array[HashMap[Block, Inst]],
    blockTerminated: Bool,
    isInstructionReachable: Bool,

    returnValueArgInst: Option[Inst],

    pushed_registers: Vec[BytecodeRegister],

    // A block is considered filled when all instructions
    // are inserted.
    filledBlocks: BitSet,

    // A block is considered sealed when the set of predecessors
    // is final and all predecessors are filled.
    sealedBlocks: BitSet,

    // Tracks all incomplete phi instructions inserted into unsealed blocks.
    incompletePhis: HashMap[Block, HashMap[BytecodeRegister, Inst]],

    // Current intended replacements for phis. See tryRemoveTrivialPhi().
    currentReplacements: Vec[Inst],
}

impl SsaGen {
    static fn new(ci: CompilationInfo, graph: Graph, analysis: BytecodeAnalysisInfo): SsaGen {
        SsaGen(
            ci,
            graph,
            ci.bc,
            ci.typeParams,
            analysis,
            None[Block],
            0i32,
            Array[HashMap[Block, Inst]]::new(),
            false,
            false,
            None[Inst],
            Vec[BytecodeRegister]::new(),
            BitSet::new(0),
            BitSet::new(0),
            HashMap[Block, HashMap[BytecodeRegister, Inst]]::new(),
            Vec[Inst]::new(),
        )
    }

    fn run() {
        self.prepare();
        self.fillEntryBlock();

        for inst in BytecodeIterator::new(self.bc.code) {
            self.instructionStart(inst.start.toInt32());

            if self.isInstructionReachable {
                self.processInstruction(inst.start, inst.size, inst.op);
            } else {
                self.processDeadInstruction(inst.op);
            }
        }

        assert(self.blockTerminated);
        self.blockEndReached(None[Block]);

        for block in self.analysis.blocks {
            if block.isNone() {
                continue;
            }

            let block = block.getOrPanic();
            assert(self.filledBlocks.contains(block.id().toInt64()));
            assert(self.sealedBlocks.contains(block.id().toInt64()));
        }
    }

    fn prepare() {
        self.currentBlock = None;
        let blockCount = self.graph.blockCount();

        self.filledBlocks = BitSet::new(blockCount.toInt64());
        self.sealedBlocks = BitSet::new(blockCount.toInt64());

        let data = Vec[HashMap[Block, Inst]]::new();

        for i in std::range(0, self.bc.registers.size()) {
            data.push(HashMap[Block, Inst]::new());
        }

        self.currentDef = data.toArray();
    }

    fn fillEntryBlock() {
        let entry = self.graph.getEntryBlock();
        self.setupArguments(entry);
        self.emitSafepoint(entry);

        let first = self.analysis.blockAt(0).getOrPanic();
        let inst = graph::createGotoInst(first);
        entry.appendInst(inst);
        entry.addSuccessor(first);

        self.fillBlock(entry);
        assert(self.trySealBlock(entry));
    }

    fn setupArguments(block: Block) {
        let mut argIdx = 0i32;
        let retTy = self.ci.returnType.specialize(self.typeParams);

        if retTy.isStruct() || retTy.isTuple() {
            let argInst = graph::createArgInst(argIdx, Type::Address);
            argIdx = argIdx + 1i32;
            block.appendInst(argInst);
            self.returnValueArgInst = Some[Inst](argInst);
        }

        let mut regIdx = 0i32;
        while regIdx < self.bc.arguments {
            let ty = self.regId(regIdx);
            let ty = self.graphTy(ty);

            if !ty.isUnit() {
                let argInst = graph::createArgInst(argIdx, ty);
                block.appendInst(argInst);
                self.writeVariable(BytecodeRegister(regIdx), block, argInst);
                argIdx = argIdx + 1i32;
            }

            regIdx = regIdx + 1i32;
        }
    }

    fn current(): Block {
        self.currentBlock.getOrPanic()
    }

    fn writeVariable(register: BytecodeRegister, block: Block, value: Inst) {
        assert(!self.reg(register).isUnit());
        assert(value.hasId());
        self.currentDef(register.value.toInt64()).insert(block, value);
    }

    fn readVariable(register: BytecodeRegister, block: Block): Inst {
        assert(!self.reg(register).isUnit());

        if self.currentDef(register.value.toInt64()).contains(block) {
            let inst = self.currentDef(register.value.toInt64())(block).getOrPanic();

            if inst.hasId() {
                return inst;
            }
        }

        self.readVariableRecursive(register, block)
    }

    fn readVariableRecursive(register: BytecodeRegister, block: Block): Inst {
        let ty = self.regGraphTy(register);

        let value: Inst = if !self.sealedBlocks.contains(block.id().toInt64()) {
            // While all blocks are created with predecessors and successors before
            // this pass in the BlockBuilder already, we still need to handle unsealed blocks.
            // E.g. Register is accessed in while header and updated in the while body.
            // In this case the while header is filled before the while body. If we wouldn't
            // handle unsealed blocks we wouldn't create a Phi instruction, since the
            // while body predecessor is still empty.
            let incomplete = graph::createPhiInst(ty);
            block.appendPhi(incomplete);

            if self.incompletePhis.contains(block) {
                self.incompletePhis(block).getOrPanic().insert(register, incomplete);
            } else {
                let map = HashMap[BytecodeRegister, Inst]::new();
                map.insert(register, incomplete);
                self.incompletePhis.insert(block, map);
            }

            incomplete
        } else if block.predecessors.size() == 1i64 {
            let predecessor = block.predecessors.first().getOrPanic().source;
            self.readVariable(register, predecessor)
        } else {
            let phi = graph::createPhiInst(ty);
            block.appendPhi(phi);
            self.writeVariable(register, block, phi);
            self.addPhiOperands(register, phi)
        };

        self.writeVariable(register, block, value);
        value
    }

    fn addPhiOperands(register: BytecodeRegister, phi: Inst): Inst {
        for pred in phi.getBlock().predecessors {
            let inst = self.readVariable(register, pred.source);
            phi.addInput(inst);
        }
        phi.registerUses();
        self.tryRemoveTrivialPhi(phi)
    }

    fn tryRemoveTrivialPhi(phi: Inst): Inst {
        let mut same = None[Inst];

        for inp in phi.getInputs() {
            let op = inp.getValue();

            if same.isSome() && same.getOrPanic() === op {
                continue;
            }

            if op === phi {
                continue;
            }

            if same.isSome() {
                return phi;
            }

            same = Some(op);
        }

        let replacement = if same.isNone() {
            graph::createUndefInst()
        } else {
            same.getOrPanic()
        };

        let users = phi.users();

        phi.replaceWith(replacement);
        phi.remove();

        for map in self.currentDef {
            let update = Vec[Block]::new();

            for (block, value) in map {
                if value === phi {
                    update.push(block);
                }
            }

            for block in update {
                assert(map.insert(block, replacement).isSome());
            }
        }

        for idx in std::range(0, self.currentReplacements.size()) {
            if self.currentReplacements(idx) === phi {
                self.currentReplacements(idx) = replacement;
            }
        }

        self.currentReplacements.push(replacement);

        for i in std::range(0, users.size()) {
            let user = users(i);

            if user === phi {
                continue;
            }

            if user.isPhi() {
                self.tryRemoveTrivialPhi(user);
            }
        }

        let replacement = self.currentReplacements.pop().getOrPanic();
        assert(replacement.hasId());
        replacement
    }

    fn markBlockTerminated() {
        self.blockTerminated = true;
    }

    fn instructionStart(offset: Int32) {
        self.offset = offset;

        let block = self.analysis.blockAt(offset.toInt64());

        if block.isSome() {
            if self.currentBlock.isSome() {
                self.blockEndReached(block);
            } else {
                self.currentBlock = block;
            }

            let block = block.getOrPanic();
            let isBlockReachable = block.isEntryBlock() || !block.predecessors.isEmpty();
            self.isInstructionReachable = isBlockReachable;
        }

        self.blockTerminated = false;
    }

    fn processInstruction(start: Int64, size: Int64, inst: BytecodeInstruction) {
       match inst {
            BytecodeInstruction::Add(dest, lhs, rhs) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitBin(dest, lhs, rhs, Op::Add);
                } else {
                    self.emitBin(dest, lhs, rhs, Op::CheckedAdd);
                }
            }
            BytecodeInstruction::Sub(dest, lhs, rhs) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitBin(dest, lhs, rhs, Op::Sub);
                } else {
                    self.emitBin(dest, lhs, rhs, Op::CheckedSub);
                }
            }
            BytecodeInstruction::Neg(dest, src) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitUn(dest, src, Op::Neg);
                } else {
                    self.emitUn(dest, src, Op::CheckedNeg);
                }
            }
            BytecodeInstruction::Mul(dest, lhs, rhs) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitBin(dest, lhs, rhs, Op::Mul);
                } else {
                    self.emitBin(dest, lhs, rhs, Op::CheckedMul);
                }
            }
            BytecodeInstruction::Div(dest, lhs, rhs) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitDivMod(dest, lhs, rhs, Op::Div);
                } else {
                    self.emitDivMod(dest, lhs, rhs, Op::CheckedDiv);
                }
            }
            BytecodeInstruction::Mod(dest, lhs, rhs) => {
                if self.reg(dest).isAnyFloat() {
                    self.emitDivMod(dest, lhs, rhs, Op::Mod);
                } else {
                    self.emitDivMod(dest, lhs, rhs, Op::CheckedMod);
                }
            }
            BytecodeInstruction::And(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::And);
            }
            BytecodeInstruction::Or(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Or);
            }
            BytecodeInstruction::Xor(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Xor);
            }
            BytecodeInstruction::Not(dest, src) => {
                self.emitUn(dest, src, Op::Not);
            }
            BytecodeInstruction::Shl(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Shl);
            }
            BytecodeInstruction::Shr(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Shr);
            }
            BytecodeInstruction::Sar(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Sar);
            }
            BytecodeInstruction::Mov(dest, src) => {
                self.emitMov(dest, src);
            }
            BytecodeInstruction::LoadTupleElement(dest, src, idx)  => {
                self.emitLoadTupleElement(dest, src, idx);
            }
            BytecodeInstruction::LoadEnumElement(dest, src, idx) => {                
                self.emitLoadEnumElement(dest, src, idx);
            }
            BytecodeInstruction::LoadEnumVariant(dest, src, idx) => {
                self.emitLoadEnumVariant(dest, src, idx);
            }
            BytecodeInstruction::LoadStructField(dest, src, idx) => {
                self.emitLoadStructField(dest, src, idx);
            }
            BytecodeInstruction::LoadField(dest, obj, idx) => {
                self.emitLoadField(dest, obj, idx);
            }
            BytecodeInstruction::StoreField(src, obj, idx) => {
                self.emitStoreField(src, obj, idx);
            }
            BytecodeInstruction::LoadGlobal(dest, global_id) => {
                self.emitLoadGlobal(dest, global_id);
            }
            BytecodeInstruction::StoreGlobal(src, global_id) => {
                self.emitStoreGlobal(src, global_id);
            }
            BytecodeInstruction::PushRegister(src) => {
                self.pushed_registers.push(src);
            }
            BytecodeInstruction::ConstTrue(dest) => {
                let inst = self.graph.ensureConstTrueInst();
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstFalse(dest) => {
                let inst = self.graph.ensureConstFalseInst();
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstUInt8(dest, value) => {
                let inst = self.graph.ensureConstUInt8Inst(value);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstChar(dest, idx) => {
                let value = self.bc.constPool(idx).toChar().getOrPanic().toInt32();
                let inst = self.graph.ensureConstInt32Inst(value);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstInt32(dest, idx) => {
                let value = self.bc.constPool(idx).toInt32().getOrPanic();
                let inst = self.graph.ensureConstInt32Inst(value);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstInt64(dest, idx) => {
                let value = self.bc.constPool(idx).toInt64().getOrPanic();
                let inst = self.graph.ensureConstInt64Inst(value);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstFloat32(dest, idx) => {
                let value = self.bc.constPool(idx).toFloat32().getOrPanic();
                let inst = graph::createFloat32Const(value);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstFloat64(dest, idx) => {
                let value = self.bc.constPool(idx).toFloat64().getOrPanic();
                let inst = graph::createFloat64Const(value);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::ConstString(dest, idx) => {
                let value = self.bc.constPool(idx).toString().getOrPanic();
                let inst = graph::createStringConst(value);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            }
            BytecodeInstruction::TestIdentity(dest, lhs, rhs) => {
                self.emitTestIdentity(dest, lhs, rhs);
            }
            BytecodeInstruction::TestEq(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Equal);
            }
            BytecodeInstruction::TestNe(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::NotEqual);
            }
            BytecodeInstruction::TestGt(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Greater);
            }
            BytecodeInstruction::TestGe(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::GreaterOrEqual);
            }
            BytecodeInstruction::TestLt(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::Less);
            }
            BytecodeInstruction::TestLe(dest, lhs, rhs) => {
                self.emitBin(dest, lhs, rhs, Op::LessOrEqual);
            }
            BytecodeInstruction::JumpLoop(distance) => {
                self.emitJumpLoop(distance);
            }
            BytecodeInstruction::LoopStart => {
                // nothing to do
            }
            BytecodeInstruction::Jump(distance) => {
                self.emitJump(distance);
            }
            BytecodeInstruction::JumpIfFalse(opnd, distance) => {
                let target = start + distance.toInt64();
                let fallthrough = start + size;
                self.emitConditionalJump(opnd, false, target, fallthrough);
            }
            BytecodeInstruction::JumpIfTrue(opnd, distance) => {
                let target = start + distance.toInt64();
                let fallthrough = start + size;
                self.emitConditionalJump(opnd, true, target, fallthrough);
            }
            BytecodeInstruction::InvokeDirect(dest, idx) => {
                self.emitInvokeDirect(dest, idx);
            }
            BytecodeInstruction::InvokeVirtual(dest, idx) => {
                self.emitInvokeVirtual(dest, idx, CallKind::Virtual);
            }
            BytecodeInstruction::InvokeStatic(dest, idx) => {
                self.emitInvokeStatic(dest, idx);
            }
            BytecodeInstruction::InvokeLambda(dest, idx) => {
                self.emitInvokeVirtual(dest, idx, CallKind::Lambda);
            }
            BytecodeInstruction::InvokeGenericStatic(dest, idx) => {
                self.emitInvokeGeneric(dest, idx, CallKind::Static);
            }
            BytecodeInstruction::InvokeGenericDirect(dest, idx) => {
                self.emitInvokeGeneric(dest, idx, CallKind::Direct);
            }
            BytecodeInstruction::NewObject(dest, idx) => {
                self.emitNewObject(dest, idx);
            }
            BytecodeInstruction::NewObjectInitialized(dest, idx) => {
                self.emitNewObject(dest, idx);
            }
            BytecodeInstruction::NewArray(dest, idx, length) => {
                self.emitNewArray(dest, idx, length);
            }
            BytecodeInstruction::NewTuple(dest, idx) => {
                self.emitNewTuple(dest, idx);
            }
            BytecodeInstruction::NewEnum(dest, idx) => {
                self.emitNewEnum(dest, idx);
            }
            BytecodeInstruction::NewStruct(dest, idx) => {
                self.emitNewStruct(dest, idx);
            }
            BytecodeInstruction::NewTraitObject(dest, idx, obj) => {
                self.emitNewTraitObject(dest, idx, obj);
            }
            BytecodeInstruction::NewLambda(dest, idx) => {
                self.emitNewLambda(dest, idx);
            }
            BytecodeInstruction::ArrayLength(dest, src) => {
                self.emitArrayLength(dest, src);
            }
            BytecodeInstruction::LoadArray(dest, arr, idx) => {
                self.emitLoadArray(dest, arr, idx);
            }
            BytecodeInstruction::StoreArray(src, arr, idx) => {
                self.emitStoreArray(src, arr, idx);
            }
            BytecodeInstruction::LoadTraitObjectValue(dest, src) => {
                self.emitLoadTraitObjectValue(dest, src);
            }
            BytecodeInstruction::Ret(opnd) => {
                self.emitRet(opnd);
            }
        }
    }

    fn processDeadInstruction(inst: BytecodeInstruction) {
        match inst {
            BytecodeInstruction::JumpLoop(_)
            | BytecodeInstruction::Jump(_)
            | BytecodeInstruction::Ret(_) => {
                self.markBlockTerminated();
            }

            _ => {
                // Non-terminator instruction
            }
        }
    }

    fn blockEndReached(next: Option[Block]) {
        let block = self.current();

        if !self.blockTerminated {
            let next = next.getOrPanic();
            let gotoInst = graph::createGotoInst(next);
            block.appendInst(gotoInst);
            block.addSuccessor(next);
        }

        // We change the current block, that means all instructions
        // are inserted. The block is now filled.
        self.fillBlock(block);

        // We don't really know when to seal a block from the bytecode
        // Try to seal this block if all predecessors are filled.
        self.trySealBlock(block);

        // This block might have a back edge to a loop header. Since this
        // block is now filled, we might be able to seal another block.
        for succ in block.successors {
            self.trySealBlock(succ.target);
        }

        self.currentBlock = next;
    }

    fn fillBlock(block: Block) {
        assert(!self.filledBlocks.contains(block.id().toInt64()));
        self.filledBlocks.insert(block.id().toInt64());
    }

    fn trySealBlock(block: Block): Bool {
        if self.sealedBlocks.contains(block.id().toInt64()) {
            return true;
        }

        let expected_predecessors = if block.hasBytecodePosition() {
            let pos = block.getBytecodePosition().toInt64();
            self.analysis.predecessors(pos).toInt64()
        } else {
            0
        };

        // Check whether all predecessors were already added.
        if block.predecessors.size() < expected_predecessors {
            return false;
        }

        assert(block.predecessors.size() == expected_predecessors);

        // all predecessors need to be filled
        for edge in block.predecessors {
            if !self.filledBlocks.contains(edge.source.id().toInt64()) {
                return false;
            }
        }

        self.sealBlock(block);
        true
    }

    fn sealBlock(block: Block) {
        assert(!self.sealedBlocks.contains(block.id().toInt64()));
        self.sealedBlocks.insert(block.id().toInt64());

        let map = self.incompletePhis(block);
        if map.isNone() { return; }

        for (register, phi) in map.getOrPanic() {
            assert(phi.hasId());
            let value = self.addPhiOperands(register, phi);
            self.writeVariable(register, block, value);
        }
    }

    fn emitArrayLength(dest: BytecodeRegister, src: BytecodeRegister) {
        let srcInst = self.readVariable(src, self.current());
        let destInst = graph::createArrayLengthInst(srcInst);
        destInst.setBytecodePosition(self.offset);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitJumpLoop(distance: Int32) {
        self.emitSafepoint(self.current());
        let targetBlock = self.analysis.blockAt((self.offset - distance).toInt64()).getOrPanic();
        let gotoInst = graph::createGotoInst(targetBlock);
        self.current().appendInst(gotoInst);
        self.current().addSuccessor(targetBlock);

        self.markBlockTerminated();
    }

    fn emitSafepoint(block: Block) {
        let inst = graph::createSafepointInst();
        block.appendInst(inst);
    }

    fn emitLoadField(dest: BytecodeRegister, obj: BytecodeRegister, idx: ConstPoolId) {
        let destType = self.reg(dest);

        let (cls_id, type_params, field_id) = match self.bc.constPool(idx) {
            ConstPoolEntry::Field(cls_id, type_params, field_id) => (cls_id, type_params, field_id),
            _ => unreachable[(ClassId, Array[BytecodeType], FieldId)](),
        };

        if !destType.isUnit() {
            let objInst = self.readVariable(obj, self.current());
            let type_params = type_params.specialize(self.typeParams);
            let offset = iface::getFieldOffset(cls_id, type_params, field_id);
            let value = self.loadField(destType, objInst, offset);
            self.writeVariable(dest, self.current(), value);
        }
    }

    fn emitStoreField(src: BytecodeRegister, obj: BytecodeRegister, idx: ConstPoolId) {
        let srcType = self.reg(src);

        let (cls_id, type_params, field_id) = match self.bc.constPool(idx) {
            ConstPoolEntry::Field(cls_id, type_params, field_id) => (cls_id, type_params, field_id),
            _ => unreachable[(ClassId, Array[BytecodeType], FieldId)](),
        };
        let type_params = type_params.specialize(self.typeParams);

        if !srcType.isUnit() {
            let srcInst = self.readVariable(src, self.current());
            let objInst = self.readVariable(obj, self.current());

            let offset = iface::getFieldOffset(cls_id, type_params, field_id);
            self.storeField(srcType, objInst, offset, srcInst, true);
        }
    }

    fn emitLoadStructField(dest: BytecodeRegister, src: BytecodeRegister, idx: ConstPoolId) {
        let destTy = self.reg(dest);

        let (struct_id, type_params, field_id) = match self.bc.constPool(idx) {
            ConstPoolEntry::StructField(struct_id, type_params, field_id) => (struct_id, type_params, field_id),
            _ => unreachable[(StructId, Array[BytecodeType], StructFieldId)](),
        };

        if !destTy.isUnit() {
            let srcInst = self.readVariable(src, self.current());
            let structLayout = self.computeStructLayout(struct_id, type_params);
            let field = structLayout.fields(field_id.value.toInt64());

            let inst = self.loadField(destTy, srcInst, field.offset);
            self.writeVariable(dest, self.current(), inst);
        }
    }

    fn emitLoadTupleElement(dest: BytecodeRegister, src: BytecodeRegister, idx: ConstPoolId) {
        let srcInst = self.readVariable(src, self.current());
        let destTy = self.reg(dest);

        let (tuple_ty, subtype_idx) = match self.bc.constPool(idx) {
            ConstPoolEntry::TupleElement(tuple_ty, subtype_idx) => (tuple_ty, subtype_idx),
            _ => unreachable[(BytecodeType, Int32)](),
        };

        assert(tuple_ty.isTuple());

        let subtypes = match tuple_ty {
            BytecodeType::Tuple(subtypes) => subtypes,
            _ => unreachable[Array[BytecodeType]](),
        };

        let tupleLayout = self.computeTupleLayout(subtypes);
        let offset = tupleLayout.fields(subtype_idx.toInt64()).offset;

        if !destTy.isUnit() {
            let inst = self.loadField(destTy, srcInst, offset);
            self.writeVariable(dest, self.current(), inst);
        }
    }

    fn emitLoadEnumVariant(dest: BytecodeRegister, src: BytecodeRegister, idx: ConstPoolId) {
        let (enum_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Enum(enum_id, type_params) => (enum_id, type_params),
            _ => unreachable[(EnumId, Array[BytecodeType])](),
        };

        let layout = self.computeEnumLayout(enum_id, type_params);

        match layout {
            EnumLayout::Int32 => {
                let inst = self.readVariable(src, self.current());
                self.writeVariable(dest, self.current(), inst);
            }

            EnumLayout::PtrOrNull(layout) => {
                let src = self.readVariable(src, self.current());

                let zero = graph::createNullConst();
                self.current().appendInst(zero);

                let op = if layout.null_is_first {
                    Op::NotEqual
                } else {
                    Op::Equal
                };

                let bool_result = graph::createBinaryInst(op, Type::Ptr, src, zero);
                self.current().appendInst(bool_result);

                let result = graph::createConvertInst(Type::Int32, bool_result);
                self.current().appendInst(result);

                self.writeVariable(dest, self.current(), result);
            }

            EnumLayout::Tagged => {
                let src = self.readVariable(src, self.current());

                let inst = graph::createLoadInst(src, iface::OBJECT_HEADER_LENGTH, Type::Int32);
                inst.setBytecodePosition(self.offset);
                self.current().appendInst(inst);

                self.writeVariable(dest, self.current(), inst);
            }
        }
    }

    fn emitLoadEnumElement(dest: BytecodeRegister, src: BytecodeRegister, idx: ConstPoolId) {
        let (enum_id, type_params, variant_id, field_id) = match self.bc.constPool(idx) {
            ConstPoolEntry::EnumElement(enum_id, type_params, variant_id, field_id) => (enum_id, type_params, variant_id, field_id),
            _ => unreachable[(EnumId, Array[BytecodeType], Int32, Int32)](),
        };
        let layout = self.computeEnumLayout(enum_id, type_params);
        let type_params = type_params.specialize(self.typeParams);

        match layout {
            EnumLayout::Int32 => {
                unreachable[()]();
            }

            EnumLayout::PtrOrNull(layout) => {
                let src = self.readVariable(src, self.current());

                let null_idx = if layout.null_is_first { 0i32 } else { 1i32 };
                assert(variant_id != null_idx);
                assert(field_id == 0i32);

                self.writeVariable(dest, self.current(), src);
            }

            EnumLayout::Tagged => {
                let destTy = self.reg(dest);
                let src = self.readVariable(src, self.current());

                let offset = iface::getFieldOffsetForEnumVariant(enum_id, type_params, variant_id, field_id);
                let result = self.loadField(destTy, src, offset);

                self.writeVariable(dest, self.current(), result);
            }
        }
    }

    fn emitTestIdentity(dest: BytecodeRegister, lhs: BytecodeRegister, rhs: BytecodeRegister) {
        let ty = self.regGraphTy(lhs);
        assert(ty == Type::Ptr);

        let mut lhsInst = self.readVariable(lhs, self.current());
        let mut rhsInst = self.readVariable(rhs, self.current());

        let destInst = graph::createBinaryInst(Op::Equal, ty, lhsInst, rhsInst);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitBin(dest: BytecodeRegister, lhs: BytecodeRegister, rhs: BytecodeRegister, op: Op) {
        let ty = self.regGraphTy(lhs);

        let mut lhsInst = self.readVariable(lhs, self.current());
        let mut rhsInst = self.readVariable(rhs, self.current());

        if op.isCommutative() && lhsInst.isConst() {
            let tmp = lhsInst;
            lhsInst = rhsInst;
            rhsInst = tmp;
        }

        let destInst = graph::createBinaryInst(op, ty, lhsInst, rhsInst);
        destInst.setBytecodePosition(self.offset);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitUn(dest: BytecodeRegister, src: BytecodeRegister, op: Op) {
        let registerType = self.reg(dest);

        let ty = match registerType {
            BytecodeType::Bool => Type::Bool,
            BytecodeType::Int32 => Type::Int32,
            BytecodeType::Int64 => Type::Int64,
            BytecodeType::Float32 => Type::Float32,
            BytecodeType::Float64 => Type::Float64,
            _ => unreachable[Type](),
        };

        let srcInst = self.readVariable(src, self.current());
        let destInst = graph::createUnaryInst(op, ty, srcInst);
        destInst.setBytecodePosition(self.offset);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitDivMod(dest: BytecodeRegister, lhs: BytecodeRegister, rhs: BytecodeRegister, op: Op) {
        let registerType = self.reg(dest);

        let ty = match registerType {
            BytecodeType::Int32 => Type::Int32,
            BytecodeType::Int64 => Type::Int64,
            BytecodeType::Float32 => Type::Float32,
            BytecodeType::Float64 => Type::Float64,
            _ => unreachable[Type](),
        };
        
        let lhsInst = self.readVariable(lhs, self.current());
        let rhsInst = self.readVariable(rhs, self.current());

        if !registerType.isAnyFloat() {
            let divZeroCheck = graph::createDivZeroCheckInst(rhsInst);
            divZeroCheck.setBytecodePosition(self.offset);
            self.current().appendInst(divZeroCheck);
        }

        let destInst = graph::createBinaryInst(op, ty, lhsInst, rhsInst);
        destInst.setBytecodePosition(self.offset);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitMov(dest: BytecodeRegister, src: BytecodeRegister) {
        let ty = self.reg(src);

        if !ty.isUnit() {
            let srcInst = self.readVariable(src, self.current());
            self.writeVariable(dest, self.current(), srcInst);
        }
    }

    fn emitLoadGlobal(dest: BytecodeRegister, glob: GlobalId) {
        let destType = self.reg(dest);

        if iface::hasGlobalInitialValue(glob) {
            let ty = self.graphTy(destType);
            let inst = graph::createEnsureGlobalInitializedInst(glob, ty);
            self.current().appendInst(inst);
            inst.setBytecodePosition(self.offset);
        }

        let value = self.loadGlobal(destType, glob);
        self.writeVariable(dest, self.current(), value);
    }

    fn emitStoreGlobal(src: BytecodeRegister, glob: GlobalId) {
        let srcType = self.reg(src);

        let srcInst = self.readVariable(src, self.current());
        self.storeGlobal(srcType, glob, srcInst);

        if iface::hasGlobalInitialValue(glob) {
            let inst = graph::createMarkGlobalInitializedInst(glob);
            self.current().appendInst(inst);
        }
    }

    fn emitTest(dest: BytecodeRegister, lhs: BytecodeRegister, rhs: BytecodeRegister, op: Op) {
        let registerType = self.reg(lhs);

        let ty = match registerType {
            BytecodeType::Int32 => Type::Int32,
            BytecodeType::Int64 => Type::Int64,
            BytecodeType::Float32 => Type::Float32,
            BytecodeType::Float64 => Type::Float64,
            _ => unreachable[Type](),
        };

        let lhsInst = self.readVariable(lhs, self.current());
        let rhsInst = self.readVariable(rhs, self.current());
        let destInst = graph::createTestInst(op, ty, lhsInst, rhsInst);
        self.current().appendInst(destInst);
        self.writeVariable(dest, self.current(), destInst);
    }

    fn emitJump(offset: Int32) {
        let targetBlock = self.analysis.blockAt((self.offset + offset).toInt64()).getOrPanic();
        let gotoInst = graph::createGotoInst(targetBlock);
        self.current().appendInst(gotoInst);
        self.current().addSuccessor(targetBlock);
        self.markBlockTerminated();
    }

    fn emitConditionalJump(opnd: BytecodeRegister, value: Bool, target: Int64, fallthrough: Int64) {
        let opndInst = self.readVariable(opnd, self.current());
        let targetBlock = self.analysis.blockAt(target).getOrPanic();
        let fallthroughBlock = self.analysis.blockAt(fallthrough).getOrPanic();

        let cond = if value {
            graph::createIfInst(opndInst, targetBlock, fallthroughBlock)
        } else {
            graph::createIfInst(opndInst, fallthroughBlock, targetBlock)
        };

        self.current().appendInst(cond);

        self.current().addSuccessor(targetBlock);
        self.current().addSuccessor(fallthroughBlock);
        self.markBlockTerminated();
    }

    fn emitLoadArray(dest: BytecodeRegister, arr: BytecodeRegister, idx: BytecodeRegister) {
        let arrInst = self.readVariable(arr, self.current());
        let idxInst = self.readVariable(idx, self.current());

        let arrayLengthInst = graph::createArrayLengthInst(arrInst);
        self.current().appendInst(arrayLengthInst);

        let boundsCheckInst = graph::createBoundsCheckInst(idxInst, arrayLengthInst);
        boundsCheckInst.setBytecodePosition(self.offset);
        self.current().appendInst(boundsCheckInst);

        let destTy = self.reg(dest);

        if !destTy.isUnit() {
            let result = self.loadArray(destTy, arrInst, idxInst);
            self.writeVariable(dest, self.current(), result);
        }
    }

    fn emitStoreArray(src: BytecodeRegister, arr: BytecodeRegister, idx: BytecodeRegister) {
        let arrInst = self.readVariable(arr, self.current());
        let idxInst = self.readVariable(idx, self.current());

        let arrayLengthInst = graph::createArrayLengthInst(arrInst);
        self.current().appendInst(arrayLengthInst);

        let boundsCheckInst = graph::createBoundsCheckInst(idxInst, arrayLengthInst);
        boundsCheckInst.setBytecodePosition(self.offset);
        self.current().appendInst(boundsCheckInst);

        let srcType = self.reg(src);
        if !srcType.isUnit() {
            let srcInst = self.readVariable(src, self.current());
            self.storeArray(srcType, arrInst, idxInst, srcInst);
        }
    }

    fn emitLoadTraitObjectValue(dest: BytecodeRegister, src: BytecodeRegister) {
        let srcInst = self.readVariable(src, self.current());
        let ty = self.reg(dest);

        if !ty.isUnit() {
            let offset = iface::OBJECT_HEADER_LENGTH;
            let value = self.loadField(ty, srcInst, offset);
            self.writeVariable(dest, self.current(), value);
        }
    }

    fn emitRet(opnd: BytecodeRegister) {
        let ty = self.reg(opnd);

        if ty.isUnit() {
            let inst = graph::createReturnVoidInst();
            self.current().appendInst(inst);
        } else if ty.isStruct() {
            let src = self.readVariable(opnd, self.current());
            let dest = self.returnValueArgInst.getOrPanic();
            self.copyStruct(ty, dest, 0i32, src, 0i32, false);
            let inst = graph::createReturnVoidInst();
            self.current().appendInst(inst);
        } else if ty.isTuple() {
            let src = self.readVariable(opnd, self.current());
            let dest = self.returnValueArgInst.getOrPanic();
            self.copyTuple(ty, dest, 0i32, src, 0i32, false);
            let inst = graph::createReturnVoidInst();
            self.current().appendInst(inst);
        } else {
            let value = self.readVariable(opnd, self.current());
            let ty = self.graphTy(ty);
            let inst = graph::createReturnInst(value, ty);
            self.current().appendInst(inst);
        }

        self.markBlockTerminated();
    }

    fn emitInvokeDirect(dest: BytecodeRegister, idx: ConstPoolId) {
        let (fct_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Fct(fct_id, type_params) => (fct_id, type_params),
            _ => unreachable[(FctId, Array[BytecodeType])](),
        };

        let intrinsic = iface::getIntrinsicForFunction(fct_id);

        if self.isIntrinsic(intrinsic) {
            let args = self.pushed_registers.toArray();
            self.pushed_registers.clear();
            self.emitIntrinsic(intrinsic, dest, args, idx, type_params);
        } else {
            self.emitCall(dest, CallKind::Direct, fct_id, type_params);
        }
    }

    fn emitInvokeStatic(dest: BytecodeRegister, idx: ConstPoolId) {
        let (fct_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Fct(fct_id, type_params) => (fct_id, type_params),
            _ => unreachable[(FctId, Array[BytecodeType])](),
        };

        let intrinsic = iface::getIntrinsicForFunction(fct_id);

        if self.isIntrinsic(intrinsic) {
            let args = self.pushed_registers.toArray();
            self.pushed_registers.clear();
            self.emitIntrinsic(intrinsic, dest, args, idx, type_params);
        } else {
            self.emitCall(dest, CallKind::Static, fct_id, type_params);
        }
    }

    fn emitCall(dest: BytecodeRegister, kind: CallKind, fct_id: FctId, type_params: Array[BytecodeType]) {
        let args = Vec[Inst]::new();
        args.reserve(self.pushed_registers.size());
        let destTy = self.reg(dest);

        let returnTy = self.prepareReturnValue(dest, destTy, args);

        for reg in self.pushed_registers {
            let ty = self.regGraphTy(reg);

            if !ty.isUnit() {
                let arg = self.readVariable(reg, self.current());
                args.push(arg);
            }
        }

        self.pushed_registers.clear();

        let type_params = type_params.specialize(self.typeParams);
        let info = FunctionInfo(fct_id, type_params);

        let inst = graph::createCallInst(info, kind, args, returnTy);
        inst.setBytecodePosition(self.offset);
        self.current().appendInst(inst);

        if !returnTy.isUnit() {
            self.writeVariable(dest, self.current(), inst);
        }
    }

    fn emitInvokeVirtual(dest: BytecodeRegister, idx: ConstPoolId, kind: CallKind) {
        let args = Vec[Inst]::new();
        args.reserve(self.pushed_registers.size());
        let destTy = self.reg(dest);

        let returnTy = self.prepareReturnValue(dest, destTy, args);
        let receiver_is_first = args.size() == 0;

        for reg in self.pushed_registers {
            let arg = self.readVariable(reg, self.current());
            args.push(arg);
        }

        self.pushed_registers.clear();

        let inst = match kind {
            CallKind::Virtual => {
                let (trait_object_ty, fct_id, type_params) = match self.bc.constPool(idx) {
                    ConstPoolEntry::TraitObjectMethod(trait_object_ty, fct_id, type_params) => (trait_object_ty, fct_id, type_params),
                    _ => unreachable[(BytecodeType, FctId, Array[BytecodeType])](),
                };

                let info = VirtualFunctionInfo(trait_object_ty, fct_id, type_params, receiver_is_first);
                graph::createVirtualCallInst(info, args, returnTy)
            }

            CallKind::Lambda => {
                let (params, return_type) = match self.bc.constPool(idx) {
                    ConstPoolEntry::Lambda(params, return_type) => (params, return_type),
                    _ => unreachable[(Array[BytecodeType], BytecodeType)](),
                };
                let params = params.specialize(self.typeParams);
                let return_type = return_type.specialize(self.typeParams);
                let info = LambdaFunctionInfo(params, return_type, receiver_is_first);
                graph::createLambdaCallInst(info, args, returnTy)
            }

            _ => unreachable[Inst](),
        };

        inst.setBytecodePosition(self.offset);
        self.current().appendInst(inst);

        if !returnTy.isUnit() {
            self.writeVariable(dest, self.current(), inst);
        }
    }

    fn prepareReturnValue(dest: BytecodeRegister, destTy: BytecodeType, args: Vec[Inst]): Type {
        if destTy.isStruct() || destTy.isTuple() {
            let layout = match destTy {
                BytecodeType::Struct(struct_id, type_params) => {
                    self.computeStructLayout(struct_id, type_params)
                },

                BytecodeType::Tuple(subtypes) => {
                    self.computeTupleLayout(subtypes)
                },

                _ => unreachable[RecordLayout](),
            };

            let allocInst = graph::createAllocateStackInst(layout);
            self.current().appendInst(allocInst);
            args.push(allocInst);

            if layout.refs.size() > 0 {
                let zero = graph::createNullConst();
                self.current().appendInst(zero);

                for ref in layout.refs {
                    let inst = graph::createStoreInst(allocInst, ref, zero, Type::Ptr);
                    self.current().appendInst(inst);
                }
            }

            self.writeVariable(dest, self.current(), allocInst);

            Type::Unit
        } else {
            self.graphTy(destTy)
        }
    }

    fn emitInvokeGeneric(dest: BytecodeRegister, idx: ConstPoolId, kind: CallKind) {
        let ty = self.regGraphTy(dest);

        let (type_param_idx, trait_fct_id, trait_type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Generic(type_param_idx, trait_fct_id, trait_type_params) => (type_param_idx, trait_fct_id, trait_type_params),
            _ => unreachable[(Int32, FctId, Array[BytecodeType])](),
        };

        let object_ty = self.typeParams(type_param_idx.toInt64());
        let callee_id = iface::findTraitImpl(trait_fct_id, trait_type_params, object_ty);

        let intrinsic = iface::getIntrinsicForFunction(callee_id);

        if self.isIntrinsic(intrinsic) {
            let args = self.pushed_registers.toArray();
            self.pushed_registers.clear();
            self.emitIntrinsic(intrinsic, dest, args, idx, Array[BytecodeType]::new());
        } else {
            self.emitCall(dest, kind, callee_id, Array[BytecodeType]::new());
        }
    }

    fn isIntrinsic(intrinsic: Int32): Bool {
        intrinsic >= 0i32 &&
        intrinsic != opc::INTRINSIC_OPTION_GET_OR_PANIC
    }

    fn emitIntrinsic(intrinsic: Int32, dest: BytecodeRegister, args: Array[BytecodeRegister], idx: ConstPoolId, type_params: Array[BytecodeType]) {
        if intrinsic == opc::INTRINSIC_INT64_ADD_UNCHECKED || intrinsic == opc::INTRINSIC_INT32_ADD_UNCHECKED {
            self.emitIntrinsicBin(dest, args, Op::Add);
        } else if intrinsic == opc::INTRINSIC_INT64_SUB_UNCHECKED || intrinsic == opc::INTRINSIC_INT32_SUB_UNCHECKED {
            self.emitIntrinsicBin(dest, args, Op::Sub);
        } else if intrinsic == opc::INTRINSIC_INT64_MUL_UNCHECKED || intrinsic == opc::INTRINSIC_INT32_MUL_UNCHECKED {
            self.emitIntrinsicBin(dest, args, Op::Mul);
        } else if intrinsic == opc::INTRINSIC_INT32_NEG_UNCHECKED || intrinsic == opc::INTRINSIC_INT64_NEG_UNCHECKED {
            self.emitIntrinsicUn(dest, args, Op::Neg);
        } else if intrinsic == opc::INTRINSIC_THREAD_CURRENT {
            self.emitIntrinsicThreadCurrent(dest, args);
        } else if intrinsic == opc::INTRINSIC_UNREACHABLE {
            self.emitIntrinsicUnreachable(dest, args, idx);
        } else if intrinsic == opc::INTRINSIC_ASSERT {
            self.emitIntrinsicAssert(dest, args);
        } else if intrinsic == opc::INTRINSIC_OPTION_IS_SOME || intrinsic == opc::INTRINSIC_OPTION_IS_NONE {
            self.emitIntrinsicOptionIsSomeAndIsNone(intrinsic, dest, args);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_TO_INT32 {
            self.emitIntrinsicConvert(dest, args, Type::Int32, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_TO_INT64 {
            self.emitIntrinsicConvert(dest, args, Type::Int64, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_TO_INT32 {
            self.emitIntrinsicConvert(dest, args, Type::Int32, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_TO_INT64 {
            self.emitIntrinsicConvert(dest, args, Type::Int64, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_INT32_TO_FLOAT32 {
            self.emitIntrinsicConvert(dest, args, Type::Float32, Type::Int32);
        } else if intrinsic == opc::INTRINSIC_INT32_TO_FLOAT64 {
            self.emitIntrinsicConvert(dest, args, Type::Float64, Type::Int32);
        } else if intrinsic == opc::INTRINSIC_INT32_TO_INT64 || intrinsic == opc::INTRINSIC_CHAR_TO_INT64 {
            self.emitIntrinsicConvert(dest, args, Type::Int64, Type::Int32);
        } else if intrinsic == opc::INTRINSIC_INT64_TO_FLOAT32 {
            self.emitIntrinsicConvert(dest, args, Type::Float32, Type::Int64);
        } else if intrinsic == opc::INTRINSIC_INT64_TO_FLOAT64 {
            self.emitIntrinsicConvert(dest, args, Type::Float64, Type::Int64);
        } else if intrinsic == opc::INTRINSIC_INT64_TO_INT32 || intrinsic == opc::INTRINSIC_INT64_TO_CHAR {
            self.emitIntrinsicConvert(dest, args, Type::Int32, Type::Int64);
        } else if intrinsic == opc::INTRINSIC_INT64_TO_UINT8 {
            self.emitIntrinsicConvert(dest, args, Type::UInt8, Type::Int64);
        } else if intrinsic == opc::INTRINSIC_PROMOTE_FLOAT32_TO_FLOAT64 {
            self.emitIntrinsicConvert(dest, args, Type::Float64, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_DEMOTE_FLOAT64_TO_FLOAT32 {
            self.emitIntrinsicConvert(dest, args, Type::Float32, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_ADD || intrinsic == opc::INTRINSIC_FLOAT32_ADD {
            self.emitBin(dest, args(0), args(1), Op::Add);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_SUB || intrinsic == opc::INTRINSIC_FLOAT32_SUB {
            self.emitBin(dest, args(0), args(1), Op::Sub);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_MUL || intrinsic == opc::INTRINSIC_FLOAT32_MUL {
            self.emitIntrinsicBin(dest, args, Op::Mul);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_DIV || intrinsic == opc::INTRINSIC_FLOAT32_DIV {
            self.emitIntrinsicBin(dest, args, Op::Div);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_NEG || intrinsic == opc::INTRINSIC_FLOAT32_NEG {
            self.emitIntrinsicUn(dest, args, Op::Neg);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_EQ || intrinsic == opc::INTRINSIC_FLOAT32_EQ {
            self.emitIntrinsicBin(dest, args, Op::Equal);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_ABS {
            self.emitIntrinsicAbs(dest, args, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_ABS {
            self.emitIntrinsicAbs(dest, args, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_INT64_ADD || intrinsic == opc::INTRINSIC_INT32_ADD {
            self.emitBin(dest, args(0), args(1), Op::CheckedAdd);
        } else if intrinsic == opc::INTRINSIC_INT64_SUB || intrinsic == opc::INTRINSIC_INT32_SUB {
            self.emitBin(dest, args(0), args(1), Op::CheckedSub);
        } else if intrinsic == opc::INTRINSIC_INT64_MUL || intrinsic == opc::INTRINSIC_INT32_MUL {
            self.emitIntrinsicBin(dest, args, Op::CheckedMul);
        } else if intrinsic == opc::INTRINSIC_INT64_DIV || intrinsic == opc::INTRINSIC_INT32_DIV {
            self.emitIntrinsicBin(dest, args, Op::CheckedDiv);
        } else if intrinsic == opc::INTRINSIC_INT64_MOD || intrinsic == opc::INTRINSIC_INT32_MOD {
            self.emitIntrinsicBin(dest, args, Op::CheckedMod);
        } else if intrinsic == opc::INTRINSIC_INT64_AND || intrinsic == opc::INTRINSIC_INT32_AND {
            self.emitIntrinsicBin(dest, args, Op::And);
        } else if intrinsic == opc::INTRINSIC_INT64_OR || intrinsic == opc::INTRINSIC_INT32_OR {
            self.emitIntrinsicBin(dest, args, Op::Or);
        } else if intrinsic == opc::INTRINSIC_INT64_XOR || intrinsic == opc::INTRINSIC_INT32_XOR {
            self.emitIntrinsicBin(dest, args, Op::Xor);
        } else if intrinsic == opc::INTRINSIC_INT64_SAR || intrinsic == opc::INTRINSIC_INT32_SAR {
            self.emitIntrinsicBin(dest, args, Op::Sar);
        } else if intrinsic == opc::INTRINSIC_INT64_SHL || intrinsic == opc::INTRINSIC_INT32_SHL {
            self.emitIntrinsicBin(dest, args, Op::Shl);
        } else if intrinsic == opc::INTRINSIC_INT64_SHR || intrinsic == opc::INTRINSIC_INT32_SHR {
            self.emitIntrinsicBin(dest, args, Op::Shr);
        } else if intrinsic == opc::INTRINSIC_INT64_NEG || intrinsic == opc::INTRINSIC_INT32_NEG {
            self.emitIntrinsicUn(dest, args, Op::CheckedNeg);
        } else if intrinsic == opc::INTRINSIC_INT64_EQ || intrinsic == opc::INTRINSIC_INT32_EQ {
            self.emitIntrinsicBin(dest, args, Op::Equal);
        } else if intrinsic == opc::INTRINSIC_BOOL_NOT || intrinsic == opc::INTRINSIC_INT64_NOT || intrinsic == opc::INTRINSIC_INT32_NOT {
            self.emitIntrinsicUn(dest, args, Op::Not);
        } else if intrinsic == opc::INTRINSIC_BOOL_EQ || intrinsic == opc::INTRINSIC_U_INT8_EQ {
            self.emitIntrinsicBin(dest, args, Op::Equal);
        } else if intrinsic == opc::INTRINSIC_INT64_CMP {
            self.emitIntrinsicCompareOrdering(dest, args, Type::Int64);
        } else if intrinsic == opc::INTRINSIC_INT32_CMP || intrinsic == opc::INTRINSIC_CHAR_CMP {
            self.emitIntrinsicCompareOrdering(dest, args, Type::Int32);
        } else if intrinsic == opc::INTRINSIC_U_INT8_CMP {
            self.emitIntrinsicCompareOrdering(dest, args, Type::UInt8);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_CMP {
            self.emitIntrinsicCompareOrdering(dest, args, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_CMP {
            self.emitIntrinsicCompareOrdering(dest, args, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_REINTERPRET_FLOAT64_AS_INT64 {
            self.emitIntrinsicBitcast(dest, args, Type::Int64, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_REINTERPRET_INT64_AS_FLOAT64 {
            self.emitIntrinsicBitcast(dest, args, Type::Float64, Type::Int64);
        } else if intrinsic == opc::INTRINSIC_REINTERPRET_FLOAT32_AS_INT32 {
            self.emitIntrinsicBitcast(dest, args, Type::Int32, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_REINTERPRET_INT32_AS_FLOAT32 {
            self.emitIntrinsicBitcast(dest, args, Type::Float32, Type::Int32);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_ROUND_DOWN {
            self.emitIntrinsicRounding(dest, args, Op::RoundDown, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_ROUND_UP {
            self.emitIntrinsicRounding(dest, args, Op::RoundUp, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_ROUND_TO_ZERO {
            self.emitIntrinsicRounding(dest, args, Op::RoundToZero, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_ROUND_HALF_EVEN {
            self.emitIntrinsicRounding(dest, args, Op::RoundHalfEven, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_ROUND_DOWN {
            self.emitIntrinsicRounding(dest, args, Op::RoundDown, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_ROUND_UP {
            self.emitIntrinsicRounding(dest, args, Op::RoundUp, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_ROUND_TO_ZERO {
            self.emitIntrinsicRounding(dest, args, Op::RoundToZero, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_ROUND_HALF_EVEN {
            self.emitIntrinsicRounding(dest, args, Op::RoundHalfEven, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_FLOAT64_SQRT {
            self.emitIntrinsicRounding(dest, args, Op::Sqrt, Type::Float64);
        } else if intrinsic == opc::INTRINSIC_FLOAT32_SQRT {
            self.emitIntrinsicRounding(dest, args, Op::Sqrt, Type::Float32);
        } else if intrinsic == opc::INTRINSIC_INT32_TO_CHAR || intrinsic == opc::INTRINSIC_CHAR_TO_INT32 {
            self.emitIntrinsicCastBetweenInt32AndChar(dest, args);
        } else if intrinsic == opc::INTRINSIC_INT32_TO_UINT8 {
            self.emitIntrinsicConvert(dest, args, Type::UInt8, Type::Int32);
        } else if intrinsic == opc::INTRINSIC_U_INT8_TO_INT32 || intrinsic == opc::INTRINSIC_U_INT8_TO_CHAR {
            self.emitIntrinsicConvert(dest, args, Type::Int32, Type::UInt8);
        } else if intrinsic == opc::INTRINSIC_BOOL_TO_INT32 {
            self.emitIntrinsicConvert(dest, args, Type::Int32, Type::Bool);
        } else if intrinsic == opc::INTRINSIC_U_INT8_TO_INT64 {
            self.emitIntrinsicConvert(dest, args, Type::Int64, Type::UInt8);
        } else if intrinsic == opc::INTRINSIC_INT32_ROTATE_LEFT {
            self.emitIntrinsicRotate(dest, args, Type::Int32, true);
        } else if intrinsic == opc::INTRINSIC_INT32_ROTATE_RIGHT {
            self.emitIntrinsicRotate(dest, args, Type::Int32, false);
        } else if intrinsic == opc::INTRINSIC_INT64_ROTATE_LEFT {
            self.emitIntrinsicRotate(dest, args, Type::Int64, true);
        } else if intrinsic == opc::INTRINSIC_INT64_ROTATE_RIGHT {
            self.emitIntrinsicRotate(dest, args, Type::Int64, false);
        } else if intrinsic == opc::INTRINSIC_DEBUG {
            self.emitIntrinsicDebug(dest, args);
        } else if intrinsic == opc::INTRINSIC_ATOMIC_INT32_GET {
            self.emitIntrinsicAtomicLoad(dest, args, Type::Int32);
        } else if intrinsic == opc::INTRINSIC_ATOMIC_INT64_GET {
            self.emitIntrinsicAtomicLoad(dest, args, Type::Int64);
        } else if intrinsic == opc::INTRINSIC_ATOMIC_INT32_SET {
            self.emitIntrinsicAtomicStore(dest, args, Type::Int32);
        } else if intrinsic == opc::INTRINSIC_ATOMIC_INT64_SET {
            self.emitIntrinsicAtomicStore(dest, args, Type::Int64);
        } else if intrinsic == opc::INTRINSIC_ATOMIC_INT32_EXCHANGE {
            self.emitIntrinsicAtomicExchange(dest, args, Type::Int32);
        } else if intrinsic == opc::INTRINSIC_ATOMIC_INT64_EXCHANGE {
            self.emitIntrinsicAtomicExchange(dest, args, Type::Int64);
        } else if intrinsic == opc::INTRINSIC_ATOMIC_INT32_COMPARE_EXCHANGE {
            self.emitIntrinsicAtomicCompareExchange(dest, args, Type::Int32);
        } else if intrinsic == opc::INTRINSIC_ATOMIC_INT64_COMPARE_EXCHANGE {
            self.emitIntrinsicAtomicCompareExchange(dest, args, Type::Int64);
        } else if intrinsic == opc::INTRINSIC_ATOMIC_INT32_FETCH_ADD {
            self.emitIntrinsicAtomicFetchAdd(dest, args, Type::Int32);
        } else if intrinsic == opc::INTRINSIC_ATOMIC_INT64_FETCH_ADD {
            self.emitIntrinsicAtomicFetchAdd(dest, args, Type::Int64);
        } else if intrinsic == opc::INTRINSIC_INT32_COUNT_ZERO_BITS {
            self.emitIntrinsicCountBits(dest, args, Type::Int32, false);
        } else if intrinsic == opc::INTRINSIC_INT32_COUNT_ONE_BITS {
            self.emitIntrinsicCountBits(dest, args, Type::Int32, true);
        } else if intrinsic == opc::INTRINSIC_INT64_COUNT_ZERO_BITS {
            self.emitIntrinsicCountBits(dest, args, Type::Int64, false);
        } else if intrinsic == opc::INTRINSIC_INT64_COUNT_ONE_BITS {
            self.emitIntrinsicCountBits(dest, args, Type::Int64, true);
        } else if intrinsic == opc::INTRINSIC_INT64_COUNT_ZERO_BITS_LEADING {
            self.emitIntrinsicCountLeading(dest, args, Type::Int64, false);
        } else if intrinsic == opc::INTRINSIC_INT32_COUNT_ZERO_BITS_LEADING {
            self.emitIntrinsicCountLeading(dest, args, Type::Int32, false);
        } else if intrinsic == opc::INTRINSIC_INT64_COUNT_ONE_BITS_LEADING {
            self.emitIntrinsicCountLeading(dest, args, Type::Int64, true);
        } else if intrinsic == opc::INTRINSIC_INT32_COUNT_ONE_BITS_LEADING {
            self.emitIntrinsicCountLeading(dest, args, Type::Int32, true);
        } else if intrinsic == opc::INTRINSIC_INT64_COUNT_ONE_BITS_TRAILING {
            self.emitIntrinsicCountTrailing(dest, args, Type::Int64, true);
        } else if intrinsic == opc::INTRINSIC_INT32_COUNT_ONE_BITS_TRAILING {
            self.emitIntrinsicCountTrailing(dest, args, Type::Int32, true);
        } else if intrinsic == opc::INTRINSIC_INT64_COUNT_ZERO_BITS_TRAILING {
            self.emitIntrinsicCountTrailing(dest, args, Type::Int64, false);
        } else if intrinsic == opc::INTRINSIC_INT32_COUNT_ZERO_BITS_TRAILING {
            self.emitIntrinsicCountTrailing(dest, args, Type::Int32, false);
        } else if intrinsic == opc::INTRINSIC_UNSAFE_KILL_REFS {
            self.emitIntrinsicUnsafeKillRefs(dest, args, type_params);

        } else {
            let name = opc::intrinsicName(intrinsic);
            std::fatalError("unknown intrinsic ${name}");
        }
    }

    fn emitIntrinsicUnsafeKillRefs(dest: BytecodeRegister, args: Array[BytecodeRegister], type_params: Array[BytecodeType]) {
        assert(args.size() == 2);
        assert(self.regGraphTy(dest) == Type::Unit);
        assert(type_params.size() == 1);
        let array = self.readVariable(args(0), self.current());
        let index = self.readVariable(args(1), self.current());
        let ty = type_params(0).specialize(self.typeParams);

        match ty {
            BytecodeType::Unit |
            BytecodeType::Bool |
            BytecodeType::UInt8 |
            BytecodeType::Float32 |
            BytecodeType::Int32 |
            BytecodeType::Char |
            BytecodeType::Int64 |
            BytecodeType::Float64 => {
                // No need to clear any reference.
            }
            BytecodeType::Class(_, _)
            | BytecodeType::Ptr
            | BytecodeType::Lambda(_, _)
            | BytecodeType::Trait(_, _) => {
                self.clearArrayElementPointer(array, index);
            }
            BytecodeType::Enum(enum_id, type_params) => {
                let layout = self.computeEnumLayout(enum_id, type_params);

                match layout {
                    EnumLayout::Int32 => {
                        // No need to clear any reference.
                    }

                    EnumLayout::PtrOrNull(_)
                    | EnumLayout::Tagged => {
                        self.clearArrayElementPointer(array, index);
                    }
                }
            }
            BytecodeType::Struct(struct_id, typeParams) => {
                let layout = self.computeStructLayout(struct_id, typeParams);
                self.clearArrayElementLayout(array, index, layout);
            }
            BytecodeType::Tuple(subtypes) => {
                let layout = self.computeTupleLayout(subtypes);
                self.clearArrayElementLayout(array, index, layout);
            }
            BytecodeType::This
            | BytecodeType::TypeParam(_)
            | BytecodeType::TypeAlias(_) => unreachable[()](),
        }
    }

    fn clearArrayElementPointer(arr: Inst, index: Inst) {
        let null = graph::createNullConst();
        self.current().appendInst(null);

        let inst = graph::createStoreArrayInst(arr, index, null, Type::Ptr);
        self.current().appendInst(inst);
    }

    fn clearArrayElementLayout(arr: Inst, index: Inst, layout: RecordLayout) {
        if layout.refs.size() > 0 {
            let null = graph::createNullConst();
            self.current().appendInst(null);

            let array_element = graph::createGetElementPtrInst(arr, index, layout.size.toInt64());
            self.current().appendInst(array_element);

            for ref in layout.refs {
                let inst = graph::createStoreInst(array_element, ref, null, Type::Ptr);
                self.current().appendInst(inst);
            }
        }
    }

    fn emitIntrinsicAbs(dest: BytecodeRegister, args: Array[BytecodeRegister], ty: Type) {
        assert(args.size() == 1);
        assert(self.regGraphTy(dest) == ty);
        assert(self.regGraphTy(args(0)) == ty);
        let value = self.readVariable(args(0), self.current());
        let inst = graph::createAbsInst(ty, value);
        self.current().appendInst(inst);
        self.writeVariable(dest, self.current(), inst);
    }

    fn emitIntrinsicCountBits(dest: BytecodeRegister, args: Array[BytecodeRegister], ty: Type, count_set_bits: Bool) {
        assert(args.size() == 1);
        assert(self.regGraphTy(dest) == Type::Int32);
        assert(self.regGraphTy(args(0)) == ty);
        let value = self.readVariable(args(0), self.current());
        let value = if count_set_bits {
            value
        } else {
            let inst = graph::createUnaryInst(Op::Not, ty, value);
            self.current().appendInst(inst);
            inst
        };
        let inst = graph::createCountBitsInst(ty, value);
        self.current().appendInst(inst);
        self.writeVariable(dest, self.current(), inst);
    }

    fn emitIntrinsicCountLeading(dest: BytecodeRegister, args: Array[BytecodeRegister], ty: Type, count_set_bits: Bool) {
        assert(args.size() == 1);
        assert(self.regGraphTy(dest) == Type::Int32);
        assert(self.regGraphTy(args(0)) == ty);
        let value = self.readVariable(args(0), self.current());
        let value = if count_set_bits {
            let inst = graph::createUnaryInst(Op::Not, ty, value);
            self.current().appendInst(inst);
            inst
        } else {
            value
        };
        let inst = graph::createCountLeadingZerosInst(ty, value);
        self.current().appendInst(inst);
        self.writeVariable(dest, self.current(), inst);
    }

    fn emitIntrinsicCountTrailing(dest: BytecodeRegister, args: Array[BytecodeRegister], ty: Type, count_set_bits: Bool) {
        assert(args.size() == 1);
        assert(self.regGraphTy(dest) == Type::Int32);
        assert(self.regGraphTy(args(0)) == ty);
        let value = self.readVariable(args(0), self.current());
        let value = if count_set_bits {
            let inst = graph::createUnaryInst(Op::Not, ty, value);
            self.current().appendInst(inst);
            inst
        } else {
            value
        };
        let inst = graph::createCountTrailingZerosInst(ty, value);
        self.current().appendInst(inst);
        self.writeVariable(dest, self.current(), inst);
    }

    fn emitIntrinsicAtomicLoad(dest: BytecodeRegister, args: Array[BytecodeRegister], ty: Type) {
        assert(args.size() == 1);
        assert(self.regGraphTy(dest) == ty);
        assert(self.regGraphTy(args(0)) == Type::Ptr);
        let obj = self.readVariable(args(0), self.current());
        let inst = graph::createAtomicLoadInst(ty, obj);
        self.current().appendInst(inst);
        self.writeVariable(dest, self.current(), inst);
    }

    fn emitIntrinsicAtomicStore(dest: BytecodeRegister, args: Array[BytecodeRegister], ty: Type) {
        assert(args.size() == 2);
        assert(self.regGraphTy(dest) == Type::Unit);
        assert(self.regGraphTy(args(0)) == Type::Ptr);
        assert(self.regGraphTy(args(1)) == ty);
        let obj = self.readVariable(args(0), self.current());
        let value = self.readVariable(args(1), self.current());
        let inst = graph::createAtomicStoreInst(ty, obj, value);
        self.current().appendInst(inst);
    }

    fn emitIntrinsicAtomicExchange(dest: BytecodeRegister, args: Array[BytecodeRegister], ty: Type) {
        assert(args.size() == 2);
        assert(self.regGraphTy(dest) == ty);
        assert(self.regGraphTy(args(0)) == Type::Ptr);
        assert(self.regGraphTy(args(1)) == ty);
        let obj = self.readVariable(args(0), self.current());
        let new_value = self.readVariable(args(1), self.current());
        let inst = graph::createAtomicExchangeInst(ty, obj, new_value);
        self.current().appendInst(inst);
        self.writeVariable(dest, self.current(), inst);
    }

    fn emitIntrinsicAtomicCompareExchange(dest: BytecodeRegister, args: Array[BytecodeRegister], ty: Type) {
        assert(args.size() == 3);
        assert(self.regGraphTy(dest) == ty);
        assert(self.regGraphTy(args(0)) == Type::Ptr);
        assert(self.regGraphTy(args(1)) == ty);
        assert(self.regGraphTy(args(2)) == ty);
        let obj = self.readVariable(args(0), self.current());
        let expected = self.readVariable(args(1), self.current());
        let new_value = self.readVariable(args(2), self.current());
        let inst = graph::createAtomicCompareExchangeInst(ty, obj, expected, new_value);
        self.current().appendInst(inst);
        self.writeVariable(dest, self.current(), inst);
    }

    fn emitIntrinsicAtomicFetchAdd(dest: BytecodeRegister, args: Array[BytecodeRegister], ty: Type) {
        assert(args.size() == 2);
        assert(self.regGraphTy(dest) == ty);
        assert(self.regGraphTy(args(0)) == Type::Ptr);
        assert(self.regGraphTy(args(1)) == ty);
        let obj = self.readVariable(args(0), self.current());
        let increment = self.readVariable(args(1), self.current());
        let inst = graph::createAtomicFetchAddInst(ty, obj, increment);
        self.current().appendInst(inst);
        self.writeVariable(dest, self.current(), inst);
    }

    fn emitIntrinsicDebug(dest: BytecodeRegister, args: Array[BytecodeRegister]) {
        assert(args.isEmpty());
        let inst = graph::createDebugInst();
        self.current().appendInst(inst);
    }

    fn emitIntrinsicRotate(dest: BytecodeRegister, args: Array[BytecodeRegister], ty: Type, is_left: Bool) {
        assert(args.size() == 2);
        let src = args(0);
        let by = args(1);
        assert(self.regGraphTy(src) == ty);
        assert(self.regGraphTy(by) == Type::Int32);
        let src = self.readVariable(src, self.current());
        let by = self.readVariable(by, self.current());
        let inst = if is_left {
            graph::createRotateLeftInst(ty, src, by)
        } else {
            graph::createRotateRightInst(ty, src, by)
        };
        self.current().appendInst(inst);
        self.writeVariable(dest, self.current(), inst);
    }

    fn emitIntrinsicCastBetweenInt32AndChar(dest: BytecodeRegister, args: Array[BytecodeRegister]) {
        assert(args.size() == 1);
        let src = args(0);
        assert(self.regGraphTy(dest) == Type::Int32);
        assert(self.regGraphTy(src) == Type::Int32);
        let value = self.readVariable(src, self.current());
        self.writeVariable(dest, self.current(), value);
    }

    fn emitIntrinsicRounding(dest: BytecodeRegister, args: Array[BytecodeRegister], op: Op, ty: Type) {
        assert(args.size() == 1);
        let src = args(0);
        assert(ty == self.regGraphTy(dest));
        assert(ty == self.regGraphTy(src));
        let value = self.readVariable(src, self.current());
        let result = graph::createUnaryInst(op, ty, value);
        self.current().appendInst(result);

        self.writeVariable(dest, self.current(), result);
    }

    fn emitIntrinsicCompareOrdering(dest: BytecodeRegister, args: Array[BytecodeRegister], ty: Type) {
        assert(args.size() == 2);
        assert(ty == self.regGraphTy(args(0)));
        assert(ty == self.regGraphTy(args(1)));
        let lhs = self.readVariable(args(0), self.current());
        let rhs = self.readVariable(args(1), self.current());
        let result = graph::createCompareOrderingInst(ty, lhs, rhs);
        self.current().appendInst(result);
        self.writeVariable(dest, self.current(), result);
    }

    fn emitIntrinsicUn(dest: BytecodeRegister, args: Array[BytecodeRegister], op: Op) {
        assert(args.size() == 1);
        self.emitUn(dest, args(0), op);
    }

    fn emitIntrinsicBin(dest: BytecodeRegister, args: Array[BytecodeRegister], op: Op) {
        assert(args.size() == 2);
        self.emitBin(dest, args(0), args(1), op);
    }

    fn emitIntrinsicConvert(dest: BytecodeRegister, args: Array[BytecodeRegister], dest_ty: Type, src_ty: Type) {
        assert(args.size() == 1);
        let src = args(0);
        assert(dest_ty == self.regGraphTy(dest));
        assert(src_ty == self.regGraphTy(src));
        let value = self.readVariable(src, self.current());
        let result = graph::createConvertInst(dest_ty, value);
        self.current().appendInst(result);

        self.writeVariable(dest, self.current(), result);
    }

    fn emitIntrinsicBitcast(dest: BytecodeRegister, args: Array[BytecodeRegister], dest_ty: Type, src_ty: Type) {
        assert(args.size() == 1);
        let src = args(0);
        assert(dest_ty == self.regGraphTy(dest));
        assert(src_ty == self.regGraphTy(src));
        let value = self.readVariable(src, self.current());
        let result = graph::createBitcastInst(dest_ty, value);
        self.current().appendInst(result);

        self.writeVariable(dest, self.current(), result);
    }

    fn emitIntrinsicOptionIsSomeAndIsNone(intrinsic: Int32, dest: BytecodeRegister, args: Array[BytecodeRegister]) {
        assert(args.size() == 1);
        let ty = self.reg(args(0));
        let (enum_id, type_params) = match ty {
            BytecodeType::Enum(enum_id, type_params) => (enum_id, type_params),
            _ => unreachable[(EnumId, Array[BytecodeType])](),
        };
        let layout = self.computeEnumLayout(enum_id, type_params);
        let is_some = if intrinsic == opc::INTRINSIC_OPTION_IS_SOME {
            true
        } else {
            assert(intrinsic == opc::INTRINSIC_OPTION_IS_NONE);
            false
        };

        let enumData = iface::getEnumData(enum_id);
        let some_variant_id = if enumData.variants(0).fields.isEmpty() {
            1i32
        } else {
            0i32
        };

        let obj = self.readVariable(args(0), self.current());

        match layout {
            EnumLayout::Int32 => unreachable[()](),
            EnumLayout::PtrOrNull(_) => {
                let null = graph::createNullConst();
                self.current().appendInst(null);

                let op = if is_some {
                    Op::NotEqual
                } else {
                    Op::Equal
                };

                let result = graph::createBinaryInst(op, Type::Ptr, obj, null);
                self.current().appendInst(result);

                self.writeVariable(dest, self.current(), result);
            }
            EnumLayout::Tagged => {
                let variant_field = graph::createLoadInst(obj, iface::OBJECT_HEADER_LENGTH, Type::Int32);
                variant_field.setBytecodePosition(self.offset);
                self.current().appendInst(variant_field);

                let op = if is_some {
                    Op::Equal
                } else {
                    Op::NotEqual
                };

                let some_const = graph::createInt32Const(some_variant_id);
                self.current().appendInst(some_const);

                let result = graph::createBinaryInst(op, Type::Int32, variant_field, some_const);
                self.current().appendInst(result);

                self.writeVariable(dest, self.current(), result);
            }
        }
    }

    fn emitIntrinsicUnreachable(dest: BytecodeRegister, args: Array[BytecodeRegister], idx: ConstPoolId) {
        assert(args.isEmpty());
        let ty = self.regGraphTy(dest);

        let inst = graph::createUnreachableInst(ty);
        self.current().appendInst(inst);
        inst.setBytecodePosition(self.offset);

        if !ty.isUnit() {
            self.writeVariable(dest, self.current(), inst);
        }
    }

    fn emitIntrinsicAssert(dest: BytecodeRegister, args: Array[BytecodeRegister]) {
        assert(args.size() == 1);
        let condition = self.readVariable(args(0), self.current());

        let inst = graph::createAssertInst(condition);
        self.current().appendInst(inst);
        inst.setBytecodePosition(self.offset);
    }

    fn emitIntrinsicThreadCurrent(dest: BytecodeRegister, args: Array[BytecodeRegister]) {
        assert(args.isEmpty());
        let inst = graph::createThreadCurrentInst();
        self.current().appendInst(inst);
        self.writeVariable(dest, self.current(), inst);
    }

    fn emitNewObject(dest: BytecodeRegister, idx: ConstPoolId) {
        let (class_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Class(class_id, type_params) => (class_id, type_params),
            _ => unreachable[(ClassId, Array[BytecodeType])](),
        };

        let type_params = type_params.specialize(self.typeParams);
        let info = ClassInfo(class_id, type_params);
        let info = InstExtraData::ClassInfo(info);
        let objInst = graph::createNewObjectInst(info);
        objInst.setBytecodePosition(self.offset);
        self.current().appendInst(objInst);

        self.writeVariable(dest, self.current(), objInst);
        let mut field_id = 0i32;

        for reg in self.pushed_registers {
            let argTy = self.reg(reg);

            if !argTy.isUnit() {
                let arg = self.readVariable(reg, self.current());

                let offset = iface::getFieldOffset(class_id, type_params, FieldId(field_id));
                self.storeField(argTy, objInst, offset, arg, true);
            }

            field_id = field_id + 1i32;
        }

        self.pushed_registers.clear();
    }

    fn emitNewLambda(dest: BytecodeRegister, idx: ConstPoolId) {
        let (fct_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Fct(fct_id, type_params) => (fct_id, type_params),
            _ => unreachable[(FctId, Array[BytecodeType])](),
        };

        let type_params = type_params.specialize(self.typeParams);
        let info = FunctionInfo(fct_id, type_params);
        let info = InstExtraData::FunctionInfo(info);
        let objInst = graph::createNewObjectInst(info);
        objInst.setBytecodePosition(self.offset);
        self.current().appendInst(objInst);

        self.writeVariable(dest, self.current(), objInst);

        if !self.pushed_registers.isEmpty() {
            assert(self.pushed_registers.size() == 1);
            let reg = self.pushed_registers.first().getOrPanic();
            let arg = self.readVariable(reg, self.current());
            let argTy = self.reg(reg);

            let offset = iface::OBJECT_HEADER_LENGTH;
            self.storeField(argTy, objInst, offset, arg, true);
        }

        self.pushed_registers.clear();
    }

    fn emitNewTraitObject(dest: BytecodeRegister, idx: ConstPoolId, obj: BytecodeRegister) {
        let (trait_id, type_params, object_ty) = match self.bc.constPool(idx) {
            ConstPoolEntry::Trait(trait_id, type_params, object_ty) => (trait_id, type_params, object_ty),
            _ => unreachable[(TraitId, Array[BytecodeType], BytecodeType)](),
        };

        let info = TraitObjectInfo(trait_id, type_params, object_ty);
        let info = InstExtraData::TraitObjectInfo(info);
        let objInst = graph::createNewObjectInst(info);
        objInst.setBytecodePosition(self.offset);
        self.current().appendInst(objInst);

        self.writeVariable(dest, self.current(), objInst);

        let arg = self.readVariable(obj, self.current());
        let argTy = self.reg(obj);

        let offset = iface::OBJECT_HEADER_LENGTH;
        self.storeField(argTy, objInst, offset, arg, true);
    }

    fn emitNewStruct(dest: BytecodeRegister, idx: ConstPoolId) {
        let (struct_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Struct(struct_id, type_params) => (struct_id, type_params),
            _ => unreachable[(StructId, Array[BytecodeType])](),
        };
        let layout = self.computeStructLayout(struct_id, type_params);

        let allocInst = graph::createAllocateStackInst(layout);
        self.current().appendInst(allocInst);

        self.writeVariable(dest, self.current(), allocInst);
        let mut fieldId = 0i32;

        assert(layout.fields.size() == self.pushed_registers.size());

        for idx in std::range(0, layout.fields.size()) {
            let reg = self.pushed_registers(idx);
            let ty = self.reg(reg);

            if !ty.isUnit() {
                let field = layout.fields(idx);
                let arg = self.readVariable(reg, self.current());

                self.storeField(field.ty, allocInst, field.offset, arg, false);
            }

            fieldId = fieldId + 1i32;
        }

        self.pushed_registers.clear();
    }

    fn emitNewEnum(dest: BytecodeRegister, idx: ConstPoolId) {
        let (enum_id, type_params, variant_id) = match self.bc.constPool(idx) {
            ConstPoolEntry::EnumVariant(enum_id, type_params, variant_id) => (enum_id, type_params, variant_id),
            _ => unreachable[(EnumId, Array[BytecodeType], Int32)](),
        };
        let layout = self.computeEnumLayout(enum_id, type_params);
        let type_params = type_params.specialize(self.typeParams);

        match layout {
            EnumLayout::Int32 => {
                assert(self.pushed_registers.isEmpty());
                let inst = graph::createInt32Const(variant_id);
                self.current().appendInst(inst);
                self.writeVariable(dest, self.current(), inst);
            }

            EnumLayout::PtrOrNull(_) => {
                if self.pushed_registers.isEmpty() {
                    let inst = graph::createNullConst();
                    self.current().appendInst(inst);
                    self.writeVariable(dest, self.current(), inst);
                } else {
                    assert(self.pushed_registers.size() == 1);
                    let reg = self.pushed_registers(0);
                    let src = self.readVariable(reg, self.current());
                    self.writeVariable(dest, self.current(), src);
                }
            }

            EnumLayout::Tagged => {
                let (classptr, size) = iface::getClassDataForEnumVariant(enum_id, type_params, variant_id);

                let data = AllocationData(classptr, size);
                let info = InstExtraData::AllocationData(data);
                let objInst = graph::createNewObjectInst(info);
                objInst.setBytecodePosition(self.offset);
                self.current().appendInst(objInst);

                self.writeVariable(dest, self.current(), objInst);
                let mut field_id = 0i32;

                let variant_id_inst = graph::createInt32Const(variant_id);
                self.current().appendInst(variant_id_inst);
                self.storeField(BytecodeType::Int32, objInst, iface::OBJECT_HEADER_LENGTH, variant_id_inst, true);

                for reg in self.pushed_registers {
                    let argTy = self.reg(reg);

                    if !argTy.isUnit() {
                        let arg = self.readVariable(reg, self.current());
                        let offset = iface::getFieldOffsetForEnumVariant(enum_id, type_params, variant_id, field_id);
                        self.storeField(argTy, objInst, offset, arg, true);
                    }

                    field_id = field_id + 1i32;
                }
            }
        }

        self.pushed_registers.clear();
    }

    fn emitNewTuple(dest: BytecodeRegister, idx: ConstPoolId) {
        let subtypes = match self.bc.constPool(idx) {
            ConstPoolEntry::Tuple(subtypes) => subtypes,
            _ => unreachable[Array[BytecodeType]](),
        };
        let layout = self.computeTupleLayout(subtypes);

        let allocInst = graph::createAllocateStackInst(layout);
        self.current().appendInst(allocInst);

        self.writeVariable(dest, self.current(), allocInst);
        let mut regIdx = 0;

        for field in layout.fields {
            if !field.ty.isUnit() {
                let reg = self.pushed_registers(regIdx);
                let arg = self.readVariable(reg, self.current());
                self.storeField(field.ty, allocInst, field.offset, arg, false);
                regIdx = regIdx + 1;
            }
        }

        assert(regIdx == self.pushed_registers.size());
        assert(regIdx <= layout.fields.size());
        self.pushed_registers.clear();
    }

    fn emitNewArray(dest: BytecodeRegister, idx: ConstPoolId, length: BytecodeRegister) {
        let (class_id, type_params) = match self.bc.constPool(idx) {
            ConstPoolEntry::Class(class_id, type_params) => (class_id, type_params),
            _ => unreachable[(ClassId, Array[BytecodeType])](),
        };

        let type_params = type_params.specialize(self.typeParams);
        let element_size = iface::getElementSize(class_id, type_params);
        let element_size_inst = graph::createInt64Const(element_size.toInt64());
        self.current().appendInst(element_size_inst);

        let length = self.readVariable(length, self.current());

        let tmp = graph::createBinaryInst(Op::CheckedMul, Type::Int64, length, element_size_inst);
        tmp.setBytecodePosition(self.offset);
        self.current().appendInst(tmp);

        let header_size = graph::createInt64Const(iface::ARRAY_HEADER_LENGTH.toInt64());
        self.current().appendInst(header_size);

        let unaligned_size = graph::createBinaryInst(Op::CheckedAdd, Type::Int64, tmp, header_size);
        unaligned_size.setBytecodePosition(self.offset);
        self.current().appendInst(unaligned_size);

        let size = if element_size % iface::PTR_SIZE != 0i32 {
            let ptr_size_minus_1 = (iface::PTR_SIZE - 1i32).toInt64();
            let tmp = graph::createInt64Const(ptr_size_minus_1);
            self.current().appendInst(tmp);

            let tmp = graph::createBinaryInst(Op::CheckedAdd, Type::Int64, unaligned_size, tmp);
            tmp.setBytecodePosition(self.offset);
            self.current().appendInst(tmp);

            let mask = graph::createInt64Const(!ptr_size_minus_1);
            self.current().appendInst(mask);

            let tmp = graph::createBinaryInst(Op::And, Type::Int64, tmp, mask);
            self.current().appendInst(tmp);

            tmp
        } else {
            unaligned_size
        };

        let classptr = iface::getClassPointer(class_id, type_params);
        let info = ClassInfo(class_id, type_params);

        let objInst = graph::createNewArrayInst(info, size, length);
        objInst.setBytecodePosition(self.offset);
        self.current().appendInst(objInst);

        self.writeVariable(dest, self.current(), objInst);
    }

    fn loadGlobal(ty: BytecodeType, id: GlobalId): Inst {
        if ty.isStruct() {
            let (struct_id, type_params) = match ty {
                BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
                _ => unreachable[(StructId, Array[BytecodeType])](),
            };
            let layout = self.computeStructLayout(struct_id, type_params);

            let dest = graph::createAllocateStackInst(layout);
            self.current().appendInst(dest);
            let src = graph::createGetGlobalAddressInst(id);
            self.current().appendInst(src);
            self.copyStruct(ty, dest, 0i32, src, 0i32, false);
            dest

        } else if ty.isTuple() {
            let subtypes = match ty {
                BytecodeType::Tuple(subtypes) => subtypes,
                _ => unreachable[Array[BytecodeType]](),
            };
            let layout = self.computeTupleLayout(subtypes);

            let dest = graph::createAllocateStackInst(layout);
            self.current().appendInst(dest);
            let src = graph::createGetGlobalAddressInst(id);
            self.current().appendInst(src);
            self.copyTuple(ty, dest, 0i32, src, 0i32, false);

            dest

        } else {
            let ty = self.graphTy(ty);
            let inst = graph::createLoadGlobalInst(ty, id);
            self.current().appendInst(inst);
            inst
        }
    }

    fn storeGlobal(ty: BytecodeType, id: GlobalId, value: Inst) {
        if ty.isStruct() {
            let (struct_id, type_params) = match ty {
                BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
                _ => unreachable[(StructId, Array[BytecodeType])](),
            };
            let layout = self.computeStructLayout(struct_id, type_params);

            let dest = graph::createGetGlobalAddressInst(id);
            self.current().appendInst(dest);
            self.copyStruct(ty, dest, 0i32, value, 0i32, false);

        } else if ty.isTuple() {
            let subtypes = match ty {
                BytecodeType::Tuple(subtypes) => subtypes,
                _ => unreachable[Array[BytecodeType]](),
            };
            let layout = self.computeTupleLayout(subtypes);

            let dest = graph::createGetGlobalAddressInst(id);
            self.current().appendInst(dest);
            self.copyTuple(ty, dest, 0i32, value, 0i32, false);

        } else {
            let ty = self.graphTy(ty);
            let inst = graph::createStoreGlobalInst(ty, id, value);
            self.current().appendInst(inst);
        }
    }

    fn loadField(ty: BytecodeType, src: Inst, srcOffset: Int32): Inst {
        if ty.isStruct() {
            let (struct_id, type_params) = match ty {
                BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
                _ => unreachable[(StructId, Array[BytecodeType])](),
            };
            let layout = self.computeStructLayout(struct_id, type_params);

            let dest = graph::createAllocateStackInst(layout);
            self.current().appendInst(dest);
            self.copyStruct(ty, dest, 0i32, src, srcOffset, false);
            dest

        } else if ty.isTuple() {
            let subtypes = match ty {
                BytecodeType::Tuple(subtypes) => subtypes,
                _ => unreachable[Array[BytecodeType]](),
            };
            let layout = self.computeTupleLayout(subtypes);

            let dest = graph::createAllocateStackInst(layout);
            self.current().appendInst(dest);
            self.copyTuple(ty, dest, 0i32, src, srcOffset, false);
            dest
        } else {
            let ty = self.graphTy(ty);
            assert(!ty.isUnit());
            let inst = graph::createLoadInst(src, srcOffset, ty);
            inst.setBytecodePosition(self.offset);
            self.current().appendInst(inst);
            inst
        }
    }

    fn loadArray(ty: BytecodeType, arr: Inst, index: Inst): Inst {
        if ty.isStruct() {
            let (struct_id, type_params) = match ty {
                BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
                _ => unreachable[(StructId, Array[BytecodeType])](),
            };
            let layout = self.computeStructLayout(struct_id, type_params);

            let dest = graph::createAllocateStackInst(layout);
            self.current().appendInst(dest);

            let src = graph::createGetElementPtrInst(arr, index, layout.size.toInt64());
            self.current().appendInst(src);

            self.copyStruct(ty, dest, 0i32, src, 0i32, false);
            dest

        } else if ty.isTuple() {
            let subtypes = match ty {
                BytecodeType::Tuple(subtypes) => subtypes,
                _ => unreachable[Array[BytecodeType]](),
            };

            let layout = self.computeTupleLayout(subtypes);
            let dest = graph::createAllocateStackInst(layout);
            self.current().appendInst(dest);

            let src = graph::createGetElementPtrInst(arr, index, layout.size.toInt64());
            self.current().appendInst(src);

            self.copyTuple(ty, dest, 0i32, src, 0i32, false);
            dest

        } else {
            let ty = self.graphTy(ty);
            assert(!ty.isUnit());
            let inst = graph::createLoadArrayInst(arr, index, ty);
            self.current().appendInst(inst);
            inst
        }
    }

    fn storeArray(ty: BytecodeType, arr: Inst, index: Inst, value: Inst) {
        if ty.isStruct() {
            let (struct_id, type_params) = match ty {
                BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
                _ => unreachable[(StructId, Array[BytecodeType])](),
            };

            let structLayout = self.computeStructLayout(struct_id, type_params);
            let dest = graph::createGetElementPtrInst(arr, index, structLayout.size.toInt64());
            self.current().appendInst(dest);

            self.storeArrayNested(ty, arr, dest, 0i32, value, 0i32);
        } else if ty.isTuple() {
            let subtypes = match ty {
                BytecodeType::Tuple(subtypes) => subtypes,
                _ => unreachable[Array[BytecodeType]](),
            };

            let tupleLayout = self.computeTupleLayout(subtypes);
            let dest = graph::createGetElementPtrInst(arr, index, tupleLayout.size.toInt64());
            self.current().appendInst(dest);

            self.storeArrayNested(ty, arr, dest, 0i32, value, 0i32);
        } else if config.needsWriteBarrier && isReference(ty) {
            let inst = graph::createStoreArrayWbInst(arr, index, value, Type::Ptr);
            self.current().appendInst(inst);
        } else {
            let ty = self.graphTy(ty);
            assert(!ty.isUnit());
            let inst = graph::createStoreArrayInst(arr, index, value, ty);
            self.current().appendInst(inst);
        }
    }

    fn storeArrayNested(ty: BytecodeType, array: Inst, dest: Inst, destOffset: Int32, src: Inst, srcOffset: Int32) {
        if ty.isStruct() {
            let (struct_id, type_params) = match ty {
                BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
                _ => unreachable[(StructId, Array[BytecodeType])](),
            };

            let structLayout = self.computeStructLayout(struct_id, type_params);

            for field in structLayout.fields {
                self.storeArrayNested(field.ty, array, dest, destOffset + field.offset, src, srcOffset + field.offset);
            }
        } else if ty.isTuple() {
            let subtypes = match ty {
                BytecodeType::Tuple(subtypes) => subtypes,
                _ => unreachable[Array[BytecodeType]](),
            };

            let tupleLayout = self.computeTupleLayout(subtypes);

            for field in tupleLayout.fields {
                self.storeArrayNested(field.ty, array, dest, destOffset + field.offset, src, srcOffset + field.offset);
            }
        } else if config.needsWriteBarrier && isReference(ty) {
            let ty = Type::Ptr;
            let value = graph::createLoadInst(src, srcOffset, ty);
            self.current().appendInst(value);

            let inst = graph::createStoreArrayAddressWbInst(array, dest, destOffset, value, ty);
            self.current().appendInst(inst);
        } else {
            let ty = self.graphTy(ty);

            if !ty.isUnit() {
                let value = graph::createLoadInst(src, srcOffset, ty);
                self.current().appendInst(value);

                let inst = graph::createStoreInst(dest, destOffset, value, ty);
                self.current().appendInst(inst);
            }
        }
    }

    fn storeField(ty: BytecodeType, dest: Inst, destOffset: Int32, value: Inst, heap: Bool) {
        if ty.isStruct() {
            self.copyStruct(ty, dest, destOffset, value, 0i32, heap);
        } else if ty.isTuple() {
            self.copyTuple(ty, dest, destOffset, value, 0i32, heap);
        } else if heap && config.needsWriteBarrier && isReference(ty) {
            let inst = graph::createStoreWbInst(dest, destOffset, value, Type::Ptr);
            self.current().appendInst(inst);
        } else {
            let ty = self.graphTy(ty);
            assert(!ty.isUnit());
            let inst = graph::createStoreInst(dest, destOffset, value, ty);
            self.current().appendInst(inst);
        }
    }

    fn copyField(ty: BytecodeType, dest: Inst, destOffset: Int32, src: Inst, srcOffset: Int32, heap: Bool) {
        if ty.isStruct() {
            self.copyStruct(ty, dest, destOffset, src, srcOffset, heap);
        } else if ty.isTuple() {
            self.copyTuple(ty, dest, destOffset, src, srcOffset, heap);
        } else if heap && config.needsWriteBarrier && isReference(ty) {
            let ty = Type::Ptr;
            let value = graph::createLoadInst(src, srcOffset, ty);
            self.current().appendInst(value);
            let inst = graph::createStoreWbInst(dest, destOffset, value, ty);
            self.current().appendInst(inst);
        } else {
            let ty = self.graphTy(ty);

            if !ty.isUnit() {
                let value = graph::createLoadInst(src, srcOffset, ty);
                self.current().appendInst(value);
                let inst = graph::createStoreInst(dest, destOffset, value, ty);
                self.current().appendInst(inst);
            }
        }
    }

    fn copyTuple(ty: BytecodeType, dest: Inst, destOffset: Int32, src: Inst, srcOffset: Int32, heap: Bool) {
        let subtypes = match ty {
            BytecodeType::Tuple(subtypes) => subtypes,
            _ => unreachable[Array[BytecodeType]](),
        };

        let tupleLayout = self.computeTupleLayout(subtypes);

        for field in tupleLayout.fields {
            self.copyField(field.ty, dest, destOffset + field.offset, src, srcOffset + field.offset, heap);
        }
    }

    fn copyStruct(ty: BytecodeType, dest: Inst, destOffset: Int32, src: Inst, srcOffset: Int32, heap: Bool) {
        let (struct_id, type_params) = match ty {
            BytecodeType::Struct(struct_id, type_params) => (struct_id, type_params),
            _ => unreachable[(StructId, Array[BytecodeType])](),
        };

        let structLayout = self.computeStructLayout(struct_id, type_params);

        for field in structLayout.fields {
            self.copyField(field.ty, dest, destOffset + field.offset, src, srcOffset + field.offset, heap);
        }
    }

    fn computeStructLayout(struct_id: StructId, type_params: Array[BytecodeType]): RecordLayout {
        let type_params = type_params.specialize(self.typeParams);
        regalloc::computeStructLayout(struct_id, type_params)
    }

    fn computeTupleLayout(subtypes: Array[BytecodeType]): RecordLayout {
        let subtypes = subtypes.specialize(self.typeParams);
        regalloc::computeTupleLayout(subtypes)
    }

    fn computeEnumLayout(enum_id: EnumId, type_params: Array[BytecodeType]): EnumLayout {
        let type_params = type_params.specialize(self.typeParams);
        regalloc::computeEnumLayout(enum_id, type_params)
    }

    fn regGraphTy(id: BytecodeRegister): Type {
        let ty = self.reg(id);
        self.graphTy(ty)
    }

    fn graphTy(ty: BytecodeType): Type {
        match ty {
            BytecodeType::Unit => Type::Unit,
            BytecodeType::Bool => Type::Bool,
            BytecodeType::UInt8 => Type::UInt8,
            BytecodeType::Float32 => Type::Float32,
            BytecodeType::Int32 => Type::Int32,
            BytecodeType::Char => Type::Int32,
            BytecodeType::Int64 => Type::Int64,
            BytecodeType::Float64 => Type::Float64,
            BytecodeType::Class(_, _)
            | BytecodeType::Ptr
            | BytecodeType::Lambda(_, _)
            | BytecodeType::Trait(_, _) => Type::Ptr,
            BytecodeType::Enum(enum_id, type_params) => {
                let layout = self.computeEnumLayout(enum_id, type_params);
                match layout {
                    EnumLayout::Int32 => Type::Int32,
                    EnumLayout::PtrOrNull(_) | EnumLayout::Tagged => Type::Ptr,
                }
            }
            BytecodeType::Struct(_, _)
            | BytecodeType::Tuple(_) => Type::Address,
            BytecodeType::This
            | BytecodeType::TypeParam(_) => unreachable[Type](),
            BytecodeType::TypeAlias(_) => unreachable[Type](),
        }
    }

    fn reg(id: BytecodeRegister): BytecodeType {
        self.regId(id.value)
    }

    fn regId(id: Int32): BytecodeType {
        let ty = self.bc.registers(id.toInt64());
        ty.specialize(self.typeParams)
    }
}

fn analyzeBytecode(graph: Graph, bc: BytecodeFunction): BytecodeAnalysisInfo {
    let starts = findBlockStarts(bc);
    let successors = findSuccessors(bc, starts);
    let predecessors = countPredecessors(starts, successors);
    let blocks = createBlocks(graph, starts);

    // The first pass creates blocks
    let bc = BytecodeAnalysis::new(graph, bc, starts, blocks);
    bc.run();

    BytecodeAnalysisInfo(bc.starts, bc.predecessors, predecessors, bc.blocks)
}

class BytecodeAnalysisInfo {
    starts: BitSet,
    predecessors: Array[Int32],
    predecessors2: Array[Int32],
    blocks: Array[Option[Block]],
}

impl BytecodeAnalysisInfo {
    fn blockAt(offset: Int64): Option[Block] {
        self.blocks(offset)
    }
}

class BytecodeAnalysis {
    graph: Graph,
    bc: BytecodeFunction,
    starts: BitSet,
    predecessors: Array[Int32],
    blocks: Array[Option[Block]],
}

impl BytecodeAnalysis {
    static fn new(graph: Graph, bc: BytecodeFunction, starts: BitSet, blocks: Array[Option[Block]]): BytecodeAnalysis {
        let size = bc.code.size();

        BytecodeAnalysis(
            graph,
            bc,
            starts,
            Array[Int32]::zero(size),
            blocks,
        )
    }

    fn run() {
        let mut fallthrough = true;

        for instInfo in BytecodeIterator::new(self.bc.code) {
            let start = instInfo.start;

            if self.starts.contains(start) {
                self.ensureBlock(start);

                if fallthrough {
                    self.incrementPredecessors(start);
                }
            }

            fallthrough = self.processInstruction(start, instInfo.size, instInfo.op);
        }
    }

    fn processInstruction(start: Int64, size: Int64, inst: BytecodeInstruction): Bool {
        match inst {
            BytecodeInstruction::Ret(_) => {
                false
            },
            BytecodeInstruction::LoopStart => {
                true
            },
            BytecodeInstruction::JumpLoop(distance) => {
                let target = start - distance.toInt64();
                assert(self.blocks(target).isSome());

                self.incrementPredecessors(target);
                false
            },
            BytecodeInstruction::JumpIfFalse(_opnd, distance) => {
                self.incrementPredecessors(start + distance.toInt64());
                true
            },
            BytecodeInstruction::JumpIfTrue(_opnd, distance) => {
                self.incrementPredecessors(start + distance.toInt64());
                true
            },
            BytecodeInstruction::Jump(distance) => {
                self.incrementPredecessors(start + distance.toInt64());
                false
            },

            _ => {
                // Non-terminator instruction
                true
            },
        }
    }

    fn ensureBlock(offset: Int64) {
        assert(offset <= self.bc.code.size());
        assert(self.blocks(offset).isSome());
    }

    fn addBlock(): Block {
        let block = Block::new();
        self.graph.addBlock(block);
        block
    }

    fn incrementPredecessors(offset: Int64) {
        self.predecessors(offset) = self.predecessors(offset) + 1i32;
    }
}

fn findBlockStarts(bc: BytecodeFunction): BitSet {
    let starts = BitSet::new(bc.code.size());
    starts.insert(0);

    let markBlockStart = |offset: Int64| {
        if offset < bc.code.size() {
            starts.insert(offset);
        }
    };

    for inst in BytecodeIterator::new(bc.code) {
        let start = inst.start;
        let next = start + inst.size;

        match inst.op {
            BytecodeInstruction::Ret(_) => {
                markBlockStart(next);
            },
            BytecodeInstruction::LoopStart => {
                markBlockStart(start);
            },
            BytecodeInstruction::JumpLoop(distance) => {
                let target = start - distance.toInt64();
                assert(starts.contains(target));
                markBlockStart(next);
            },
            BytecodeInstruction::JumpIfFalse(_opnd, distance) => {
                let target = start + distance.toInt64();
                markBlockStart(next);
                markBlockStart(target);
            }
            BytecodeInstruction::JumpIfTrue(_opnd, distance) => {
                let target = start + distance.toInt64();
                markBlockStart(next);
                markBlockStart(target);
            },
            BytecodeInstruction::Jump(distance) => {
                let target = start + distance.toInt64();
                markBlockStart(next);
                markBlockStart(target);
            },
            _ => {
                // Non-terminator instruction
            },
        }
    }

    starts
}

fn findSuccessors(bc: BytecodeFunction, starts: BitSet): Array[Option[Array[Int64]]] {
    let successors = Array[Option[Array[Int64]]]::fill(bc.code.size(), None[Array[Int64]]);

    for block in starts {
        let result = findSuccessorsForBlock(bc, block, starts);
        successors(block) = Some[Array[Int64]](result);
    }

    successors
}

fn findSuccessorsForBlock(bc: BytecodeFunction, block: Int64, starts: BitSet): Array[Int64] {
    for inst in BytecodeIterator::newAtPos(bc.code, block) {
        if inst.start > block && starts.contains(inst.start) {
            return Array[Int64]::new(inst.start);
        }

        let next = inst.start + inst.size;

        match inst.op {
            BytecodeInstruction::JumpLoop(distance) => {
                let target = inst.start - distance.toInt64();
                return Array[Int64]::new(target);
            }
            BytecodeInstruction::JumpIfFalse(_opnd, distance) => {
                let target = inst.start + distance.toInt64();
                return Array[Int64]::new(target, next);
            }
            BytecodeInstruction::JumpIfTrue(_opnd, distance) => {
                let target = inst.start + distance.toInt64();
                return Array[Int64]::new(target, next);

            }
            BytecodeInstruction::Jump(distance) => {
                let target = inst.start + distance.toInt64();
                return Array[Int64]::new(target);
            }
            BytecodeInstruction::Ret(_) => {
                return Array[Int64]::new();
            }
            BytecodeInstruction::LoopStart => {
                return Array[Int64]::new(inst.start);
            }

            _ => {
                // Non-terminator instruction
            }
        }
    }

    unreachable[Array[Int64]]()
}

fn countPredecessors(starts: BitSet, successors: Array[Option[Array[Int64]]]): Array[Int32] {
    let predecessorCounts = Array[Int32]::zero(starts.size());
    let worklist = Vec[Int64]::new();

    let pushOnWorklist = |offset: Int64| {
        assert(starts.contains(offset));
        predecessorCounts(offset) = predecessorCounts(offset) + 1i32;
        if predecessorCounts(offset) == 1i32 {
            worklist.push(offset);
        }
    };

    worklist.push(0);

    while !worklist.isEmpty() {
        let block = worklist.pop().getOrPanic();
        assert(starts.contains(block));
        let blockSuccessors = successors(block).getOrPanic();

        for succ in blockSuccessors {
            pushOnWorklist(succ);
        }
    }

    predecessorCounts
}

fn createBlocks(graph: Graph, starts: BitSet): Array[Option[Block]] {
    let entryBlock = Block::new();
    graph.addBlock(entryBlock);
    entryBlock.setName("entry");
    graph.setEntryBlock(entryBlock);

    let blocks = Array[Option[Block]]::fill(starts.size(), None[Block]);

    for pos in starts {
        let block = Block::new();
        graph.addBlock(block);
        blocks(pos) = Some[Block](block);
        block.setBytecodePosition(pos.toInt32());
    }

    blocks
}
